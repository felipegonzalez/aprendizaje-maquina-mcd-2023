[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje Máquina",
    "section": "",
    "text": "Temario y referencias\nTodas las notas y material del curso estarán en este repositorio.\n\nIntroducción al aprendizaje máquina\nMétodos locales y estructurados. Ingeniería de variables de entrada.\nPrincipios de Regularización\nProblemas de clasificación, métricas y evaluación\nMétodos de remuestreo y validación cruzada\nRedes neuronales\nÁrboles, bosques aleatorios y boosting\nDiagnóstico y mejora en problemas de aprendizaje supervisado\nReducción de dimensionalidad: Embeddings, descomposición en valores singulares, componentes principales\nAnálisis de conglomerados\n\n\nEvaluación\n\nTareas semanales (20%) para discutir en clase, compartidas en el repositorio y en nuestro espacio de trabajo de Posit Cloud.\nExamen parcial (40% práctico), con una componente oral.\nUn examen final (40% práctico), con una componente oral.\n\n\n\nMaterial\nCada semestre las notas cambian, en algunas partes considerablemente. Las de este semestre están en este repositorio, incluyendo ejemplos, ejercicios y tareas.\n\n\nReferencias principales\n\nAn Introduction to Statistical Learning, James et al. (2014)\nDeep Learning, Goodfellow, Bengio, y Courville (2016)\nTidy Modeling with R, Kuhn y Silge (2022)\n\n\n\nOtras referencias\n\nPattern Recognition and Machine Learning, Bishop (2006)\nThe Elements of Statistical Learning, Hastie, Tibshirani, y Friedman (2017)\nPredicción conforme\n\n\n\nSoftware\nPara hacer las tareas y exámenes pueden usar cualquier lenguaje o flujo de trabajo que les convenga (R o Python, por ejemplo) - el único requisito esté basado en código y no point-and-click. En lo posible utilizamos librerías especializadas que se pueden utilizar desde varias plataformas (keras, por ejemplo).\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc.\n\n\nGoodfellow, Ian, Yoshua Bengio, y Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated. http://www-bcf.usc.edu/~gareth/ISL/.\n\n\nKuhn, M., y J. Silge. 2022. Tidy Modeling with R. O’Reilly Media. https://books.google.com.mx/books?id=9cJ6EAAAQBAJ."
  },
  {
    "objectID": "01-introduccion.html#qué-es-aprendizaje-de-máquina-machine-learning",
    "href": "01-introduccion.html#qué-es-aprendizaje-de-máquina-machine-learning",
    "title": "1  Introducción",
    "section": "1.1 ¿Qué es aprendizaje de máquina (machine learning)?",
    "text": "1.1 ¿Qué es aprendizaje de máquina (machine learning)?\nMétodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión.\nEn este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Existe también aprendizaje por refuerzo, en donde buscamos aprender a tomar decisiones en un entorno en donde la decisión afecta directa e inmediatamente al entorno.\n\nEjemplos de tareas de aprendizaje:\n\nPredecir si un cliente de tarjeta de crédito va a caer en impago en los próximos tres meses.\nReconocer palabras escritas a mano (OCR).\nDetectar llamados de ballenas en grabaciones de boyas.\nEstimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica.\nDividir a los clientes de Netflix según sus gustos.\nRecomendar artículos a clientes de un programa de lealtad o servicio online.\n\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\n\nQuisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Igual oír cada segundo de grabación de las boyas para saber si hay ballenas o no. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo.\nQuisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\nAl resolver estos problemas computacionalmente tenemos oportunidad de aprender más del problema que nos interesa: estas soluciones forman parte de un ciclo de análisis de datos donde podemos aprender de una forma más concentrada cuáles son características y patrones importantes de nuestros datos.\n\nEs posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc). Las razones para no tomar un enfoque de reglas construidas “a mano”:\n\nCuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.)\nReglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico), pues para problemas interesantes muchas veces se requieren grandes cantidades de reglas. Por ejemplo: ¿qué búsquedas www se enfocan en dar direcciones como resultados? ¿cómo filtrar comentarios no aceptables en foros?"
  },
  {
    "objectID": "01-introduccion.html#ejemplo-reglas-y-aprendizaje",
    "href": "01-introduccion.html#ejemplo-reglas-y-aprendizaje",
    "title": "1  Introducción",
    "section": "1.2 Ejemplo: reglas y aprendizaje",
    "text": "1.2 Ejemplo: reglas y aprendizaje\nLectura de un medidor mediante imágenes. Supongamos que en una infraestructura donde hay medidores análogos (de agua, electricidad, gas, etc.) que no se comunican. ¿Podríamos pensar en utilizar fotos tomadas automáticamente para medir el consumo?\nPor ejemplo, consideramos el siguiente problema (tomado de aquí, ver código y datos):\n\nlibrary(imager)\nlibrary(tidyverse)\nlibrary(gt)\nset.seed(437)\npath_img &lt;- \"../datos/medidor/\"\npath_full_imgs &lt;- list.files(path = path_img, full.names = TRUE)\nmedidor &lt;- load.image(sample(path_full_imgs, 1))\npar(mar = c(1, 1, 1, 1))\nplot(medidor, axes = FALSE)\n\n\n\n\nNótese que las imágenes y videos son matrices o arreglos de valores de pixeles, por ejemplo estas son las dimensiones para una imagen:\n\ndim(medidor)\n\n[1] 142 142   1   3\n\n\nEn este caso, la imagen es de 193 x 193 pixeles y tiene tres canales, o tres matrices de 193 x 193 donde la entrada de cada matriz es la intensidad del canal correspondiente. Buscámos hacer cálculos con estas matrices para extraer la información que queremos. En este caso, construiremos estos cálculos a mano.\nPrimero filtramos (extraemos canal rojo y azul, restamos, difuminamos y aplicamos un umbral):\n\nmedidor_rojo &lt;- medidor |&gt;  R() \nmedidor_azul &lt;- medidor |&gt; B()\nmedidor_1 &lt;- (medidor_rojo - medidor_azul) |&gt; isoblur(5)\naguja &lt;-  medidor_1 |&gt;  imager::threshold(\"90%\", approx = FALSE)\n\n\n\n\n\n\nLogramos extraer la aguja, aunque hay algo de ruido adicional. Una estrategia es extraer la componente conexa más grande (que debería corresponder a la aguja), y luego calcular su orientación. Una manera fácil es encontrar una recta que vaya del centro de la imagen hasta el punto más alejado del centro (aunque quizá puedes pensar maneras más robustas de hacer esto):\n\ncalcular_punta &lt;- function(pixset){\n  centro &lt;- floor(dim(pixset)[1:2] / 2)\n  # segmentar en componentes conexas\n  componentes &lt;- split_connected(pixset)\n  # calcular la más grande\n  num_pixeles &lt;- map_dbl(componentes, sum)\n  ind_maxima &lt;- which.max(num_pixeles)\n  pixset_tbl &lt;- as_tibble(componentes[[ind_maxima]]) |&gt; \n    mutate(dist = (x - centro[1])^2 + (y - centro[2])^2) |&gt; \n    top_n(1, dist)  |&gt; \n    mutate(x_1 = x - centro[1], y_1 = y - centro[2])\n  pixset_tbl[1, ] \n}\n\nY ahora podemos aplicar el proceso de arriba a todas la imágenes:\n\npath_imgs &lt;- list.files(path = path_img)\n\npath_full_imgs &lt;- list.files(path = path_img, full.names = TRUE)\n# en este caso los datos están etiquetados\ny_imagenes &lt;- path_imgs |&gt; str_sub(1, 3) |&gt; as.numeric()\n# procesar algunas imagenes\nset.seed(82)\nindice_imgs &lt;- sample(1:length(path_full_imgs), 500)\nangulos &lt;- path_full_imgs[indice_imgs] |&gt; \n    map( ~ load.image(.x)) |&gt;  \n    map(~ R(.x) - B(.x)) |&gt; \n    map( ~ isoblur(.x, 5)) |&gt; \n    map( ~ imager::threshold(.x, \"90%\")) |&gt; \n    map( ~ calcular_punta(.x)) |&gt; \n  bind_rows()\n\n\nangulos_tbl &lt;- angulos |&gt; \n  mutate(y_medidor = y_imagenes[indice_imgs])\nggplot(angulos_tbl, \n    aes(x = 180 * atan2(y_1, x_1) / pi + 90, y = y_medidor)) +\n  geom_point() + xlab(\"Ángulo\")\n\n\n\n\nEl desempeño no es muy malo pero tiene algunas fallas grandes. Quizá refinando nuestro pipeline de procesamiento podemos mejorarlo.\n\nPor el contrario, en el enfoque de aprendizaje, comenzamos con un conjunto de datos etiquetado (por una persona, por un método costoso, etc.), y utilizamos alguna estructura general para aprender a producir la respuesta a partir de las imágenes. Por ejemplo, en este caso podríamos una red convolucional sobre los valores de los pixeles de la imagen:\n\nlibrary(keras)\n# usamos los tres canales de la imagen\nimagenes &lt;- map(path_full_imgs, ~ image_load(.x, target_size = c(64, 64)))\nimgs_array &lt;-  imagenes |&gt;  map(~ image_to_array(.x)) \nimgs_array &lt;- map(imgs_array, ~ array_reshape(.x, c(1, 64, 64, 3)))\nx &lt;- abind::abind(imgs_array, along = 1)\nset.seed(25311)\nindices_entrena &lt;- sample(1:dim(x)[1], size = 4200)\n# generar lotes de datos de las imágenes originales\ngenerador_1 &lt;- image_data_generator(\n  rescale = 1/255,\n  rotation_range = 5,\n  zoom_range = 0.05,\n  horizontal_flip = FALSE,\n  vertical_flip = FALSE,\n  fill_mode = \"nearest\"\n)\ngenerador_entrena &lt;- flow_images_from_data(\n  x = x[indices_entrena,,,],\n  y = y_imagenes[indices_entrena] / 10,\n  generator = generador_1,\n  shuffle = TRUE,\n  batch_size = 64\n)\n\n\nmodelo_aguja &lt;- keras_model_sequential() |&gt;\n  layer_conv_2d(input_shape = c(64, 64, 3), \n    filters = 32, kernel_size = c(5, 5)) |&gt; \n  layer_max_pooling_2d(pool_size = c(2, 2)) |&gt;\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5)) |&gt; \n  layer_max_pooling_2d(pool_size = c(2, 2)) |&gt; \n  layer_conv_2d(filters = 16, kernel_size = c(3, 3)) |&gt; \n  layer_max_pooling_2d(pool_size = c(2, 2)) |&gt; \n  layer_flatten() |&gt; \n  layer_dropout(0.2) |&gt; \n  layer_dense(units = 100, activation = \"sigmoid\") |&gt;\n  layer_dropout(0.2) |&gt; \n  layer_dense(units = 100, activation = \"sigmoid\") |&gt;\n  layer_dropout(0.2) |&gt; \n  layer_dense(units = 1, activation = 'linear')\n\nAjustamos el modelo:\n\nmodelo_aguja |&gt; compile(\n  loss = \"mse\",\n  optimizer = optimizer_adam(learning_rate = 0.0005),\n  metrics = c('mae')\n)                                                                                                        \n# Entrenar\nmodelo_aguja |&gt; fit(\n  generador_entrena,\n  epochs = 200,\n  verbose = TRUE, \n  validation_data = list(x = x[-indices_entrena,,,], \n                         y = y_imagenes[-c(indices_entrena)] / 10)\n)\nsave_model_hdf5(modelo_aguja, \"cache/modelo-aguja.h5\")\n\n\nmodelo &lt;- load_model_hdf5(\"cache/modelo-aguja.h5\")\nmodelo\n\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_8 (Conv2D)                  (None, 60, 60, 32)              2432        \n max_pooling2d_8 (MaxPooling2D)     (None, 30, 30, 32)              0           \n conv2d_7 (Conv2D)                  (None, 26, 26, 32)              25632       \n max_pooling2d_7 (MaxPooling2D)     (None, 13, 13, 32)              0           \n conv2d_6 (Conv2D)                  (None, 11, 11, 16)              4624        \n max_pooling2d_6 (MaxPooling2D)     (None, 5, 5, 16)                0           \n flatten_2 (Flatten)                (None, 400)                     0           \n dropout_8 (Dropout)                (None, 400)                     0           \n dense_8 (Dense)                    (None, 100)                     40100       \n dropout_7 (Dropout)                (None, 100)                     0           \n dense_7 (Dense)                    (None, 100)                     10100       \n dropout_6 (Dropout)                (None, 100)                     0           \n dense_6 (Dense)                    (None, 1)                       101         \n================================================================================\nTotal params: 82,989\nTrainable params: 82,989\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nY observamos que obtenemos predicciones prometedoras:\n\npreds &lt;- predict(modelo, x[-indices_entrena,,,])\npreds_tbl &lt;- tibble(y = y_imagenes[-c(indices_entrena)] / 10, preds = preds)\nggplot(preds_tbl, aes(x = preds, y = y)) +\n  geom_jitter(alpha = 0.5) +\n  geom_abline(colour = 'red')\n\n\n\n\nDe forma que podemos resolver este problema con algoritmos generales, sin tener que aplicar métodos sofisticados de procesamiento de imágenes. El enfoque de aprendizaje es particularmente efectivo cuando hay cantidades grandes de datos poco ruidosos, y aunque en este ejemplo los dos enfoques dan resultados razonables, en procesamiento de imágenes es cada vez más común usar redes neuronales grandes para resolver este tipo de problemas."
  },
  {
    "objectID": "01-introduccion.html#medicioncostosa",
    "href": "01-introduccion.html#medicioncostosa",
    "title": "1  Introducción",
    "section": "1.3 Ejemplo: mediciones costosas",
    "text": "1.3 Ejemplo: mediciones costosas\nEn algunos casos, el estándar de la medición que nos interesa es uno que es costoso de cumplir: a veces se dice que etiquetar los datos es costoso. Un ejemplo es producir las estimaciones de ingreso trimestral de un hogar que se recolecta en la ENIGH (ver aquí). En este caso particular, se utiliza esta encuesta como datos etiquetados para poder estimar el ingreso de otros hogares que no están en la muestra del ENIGH, pero para los que se conocen características de las vivienda, características de los integrantes, y otras medidas que son más fácilmente recolectadas en encuestas de opinión.\nVeremos otro ejemplo: estimar el valor de mercado de las casas en venta de una región. Es posible que tengamos un inventario de casas con varias de sus características registradas, pero producir estimaciones correctas de su valor de mercado puede requerir de inspecciones costosas de expertos, o tomar aproximaciones imprecisas de esta cantidad (por ejemplo, cuál es el precio ofertado).\nUtilizaremos datos de casas que se vendieron en Ames, Iowa en cierto periodo. En este caso, conocemos el valor a la que se vendió una casa. Buscamos producir una estimación para otras casas para las cuales conocemos características como su localización, superficie en metros cuadrados, año de construcción, espacio de estacionamiento, y así sucesivamente. Estas medidas son más fáciles de recolectar, y quisiéramos producir una estimación de su precio de venta en términos de estas medidas.\nEn este ejemplo intentaremos una forma simple de predecir.\n\nlibrary(tidymodels)\nlibrary(patchwork)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(68821)\n# dividir muestra\ncasas_split &lt;- initial_split(casas, prop = 0.75)\n# obtener muestra de entrenamiento\ncasas_entrena &lt;- training(casas_split)\n# graficar\ng_1 &lt;- ggplot(casas_entrena, aes(x = precio_miles)) +\n  geom_histogram()\ng_2 &lt;- ggplot(casas_entrena, aes(x = area_hab_m2, \n                          y = precio_miles, \n                          colour = condicion_venta)) +\n  geom_point() \ng_1 + g_2\n\n\n\n\nLa variable de condición de venta no podemos utilizarla para predecir, pues sólo la conocemos una vez que la venta se hace. Podemos ver en lugar de eso solamente las de condición normal. Consideramos además del área habitable, por ejemplo, la calidad general de terminados:\n\nggplot(casas_entrena |&gt;  \n       filter(condicion_venta == \"Normal\") |&gt;  \n       mutate(calidad_grupo = \n        cut(calidad_gral, breaks = c(0, 5, 7, 8, 10))), \n  aes(x = area_hab_m2, \n      y = precio_miles,\n      colour = calidad_grupo)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\")\n\n\n\n\nPrecio vs área y calidad\n\n\n\n\nVemos que estas dos variables que hemos usado explican buena parte de la variación de los precios de las casas. Podemos examinar otras variables como la existencia y tamaño del garage:\n\nggplot(casas_entrena |&gt;  filter(condicion_venta == \"Normal\"),\n       aes(x = area_hab_m2, y = precio_miles, colour = area_garage_m2)) +\n  geom_point(alpha = 0.5) + facet_wrap(~ (area_garage_m2 == 0))\n\n\n\n\nY quizá podríamos proponer una fórmula simple de la forma:\n\\[Precio = a_{calidad} + b_{calidad}\\textrm{Area} + c \\textrm{AreaGarage} + d\\textrm{TieneGarage}\\]\ndonde los valores de \\(a_{calidad}, b_{calidad}, c, d\\) podríamos estimarlos de los datos. La pendiente de Area dependende de la calificación de la calidad de los terminados.\nNuestro proceso comenzaría entonces construir los datos para usar en el modelo:\n\nreceta_casas &lt;- \n  recipe(precio_miles ~ area_hab_m2 + calidad_gral + \n           area_garage_m2, \n         data = casas_entrena) |&gt;  \n  step_cut(calidad_gral, breaks = c(3, 5, 6, 7, 8)) |&gt;  \n  step_normalize(starts_with(\"area\")) |&gt; \n  step_mutate(tiene_garage = ifelse(area_garage_m2 &gt; 0, 1, 0)) |&gt; \n  step_dummy(calidad_gral) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) \n\nDefinimos el tipo de modelo que queremos ajustar, creamos un flujo y ajustamos\n\n# modelo\ncasas_modelo &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\n# flujo\nflujo_casas &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(casas_modelo)\n# ajustar flujo\najuste &lt;- fit(flujo_casas, casas_entrena)\najuste\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_cut()\n• step_normalize()\n• step_mutate()\n• step_dummy()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                       (Intercept)                         area_hab_m2  \n                           111.124                              22.505  \n                    area_garage_m2                        tiene_garage  \n                            12.014                               3.230  \n               calidad_gral_X.3.5.                 calidad_gral_X.5.6.  \n                            30.104                              54.623  \n               calidad_gral_X.6.7.                 calidad_gral_X.7.8.  \n                            79.565                             119.639  \n              calidad_gral_X.8.10.   area_hab_m2_x_calidad_gral_X.3.5.  \n                           217.099                              -7.942  \n area_hab_m2_x_calidad_gral_X.5.6.   area_hab_m2_x_calidad_gral_X.6.7.  \n                             2.839                              14.141  \n area_hab_m2_x_calidad_gral_X.7.8.  area_hab_m2_x_calidad_gral_X.8.10.  \n                            14.221                              -1.421  \n\n\nY ahora podemos hacer predicciones para nuevos datos no observados en el entrenamiento:\n\nset.seed(8)\ncasas_prueba &lt;- testing(casas_split) \nejemplos &lt;- casas_prueba|&gt; sample_n(5)\npredict(ajuste, ejemplos) |&gt; \n  bind_cols(ejemplos |&gt; select(precio_miles, area_hab_m2)) |&gt; \n  arrange(desc(precio_miles)) |&gt; gt() |&gt; \n  fmt_number(columns = everything(), decimals = 1)\n\n\n\n\n\n  \n    \n    \n      .pred\n      precio_miles\n      area_hab_m2\n    \n  \n  \n    242.3\n275.0\n152.9\n    177.3\n181.0\n155.6\n    169.3\n175.5\n132.1\n    123.1\n133.0\n117.8\n    115.6\n128.5\n90.2\n  \n  \n  \n\n\n\n\nY finalmente podemos evaluar nuestro modelo. En este casos mostramos diversas métricas como ejemplo:\n\nmetricas &lt;- metric_set(mape, mae, rmse)\nmetricas(casas_prueba |&gt; bind_cols(predict(ajuste, casas_prueba)), \n     truth = precio_miles, estimate = .pred) |&gt; gt() |&gt; \n  fmt_number(columns = where(is_double), decimals = 1)\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    mape\nstandard\n14.1\n    mae\nstandard\n23.4\n    rmse\nstandard\n33.3\n  \n  \n  \n\n\n\n\n\ncasas_prueba_f &lt;- filter(casas_prueba,\n  condicion_venta %in% c(\"Normal\", \"Partial\", \"Abnorml\"))\nggplot(casas_prueba_f |&gt;\n       bind_cols(predict(ajuste, casas_prueba_f)),\n       aes(x = .pred, y = precio_miles)) +\n  geom_point() +\n  geom_abline(colour = \"red\") + facet_wrap(~ condicion_venta)\n\n\n\n\nEste modelo tiene algunos defectos y todavía tiene error considerablemente grande. La mejora sin embargo podemos cuantificarla con un modelo base o benchmark. En este caso utilizamos el siguiente modelo simple, cuya predicción es el promedio de entrenamiento:\n\n# nearest neighbors es grande, así que la predicción\n# es el promedio de precio en entrenamiento\ncasas_promedio &lt;- nearest_neighbor(\n    neighbors = 1000, weight_func = \"rectangular\") |&gt;\n  set_mode(\"regression\") |&gt; \n  set_engine(\"kknn\")\nworkflow_base &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(casas_promedio)\najuste_base &lt;- fit(workflow_base, casas_entrena)\nmetricas(casas_prueba |&gt; bind_cols(predict(ajuste_base, casas_prueba)), \n     truth = precio_miles, estimate = .pred)|&gt; gt() |&gt; \n  fmt_number(columns = where(is_double), decimals = 1)\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    mape\nstandard\n33.4\n    mae\nstandard\n54.8\n    rmse\nstandard\n77.2"
  },
  {
    "objectID": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "href": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "title": "1  Introducción",
    "section": "1.4 Aprendizaje supervisado y no supervisado",
    "text": "1.4 Aprendizaje supervisado y no supervisado\nLas tareas de aprendizaje se dividen en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado.\nEn Aprendizaje supervisado buscamos construir un modelo o algoritmo para predecir o estimar un target o una respuesta a partir de ciertas variables de entrada.\nPredecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno.\nPor ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago.\nUsualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida:\n\nProblemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión\nProblemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación.\n\nEn contraste, en Aprendizaje no supervisado no hay target o variable respuesta. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos.\nLos problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles."
  },
  {
    "objectID": "02-principios-supervisado.html#población-y-pérdida",
    "href": "02-principios-supervisado.html#población-y-pérdida",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.1 Población y pérdida",
    "text": "2.1 Población y pérdida\nSupongamos que tenemos una población grande de observaciones potenciales de la forma\n\\[(x_1, x_2, \\ldots, x_p, y) \\]\nY para esa población nos interesa predecir una variable respuesta \\(y\\) numérica en términos de variables de entrada disponibles \\(x = (x_1,x_2,\\ldots, x_p)\\):\n\\[(x_1, x_2, \\ldots, x_p) \\to y\\]\nEl proceso que produce la salida \\(y\\) a partir de las entradas es típicamente muy complejo y dificíl de describir de forma mecanística (por ejemplo, el ingreso dadas características de los hogares).\n\nEjemplo\nPara ilustrar esta discusión teórica, consideraremos datos simulados. La población está dada por el siguiente proceso generador de datos:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(gt)\ngenera_datos &lt;- function(n = 500, tipo = NULL){\n  dat_tbl &lt;- tibble(nse = runif(n, 0, 100)) |&gt;\n    mutate(estudio_años = floor(rnorm(n, 1.5 * sqrt(nse), 1))) |&gt;\n    mutate(estudio_años = pmax(0, pmin(17, estudio_años))) |&gt; \n    mutate(habilidad = rnorm(n, 100 + 0.1 * nse, 1)) |&gt; \n    mutate(z = 100 + (habilidad/100) * ( 20 * nse + 5 * (estudio_años))) |&gt; \n    mutate(ingreso = pmax(0, 0.2*(z + rnorm(n, 0, 150))))\n  obs_tbl &lt;- dat_tbl |&gt; \n    mutate(tipo = tipo, id = 1:n)\n  obs_tbl |&gt; select(id, tipo, x = estudio_años, y = ingreso)\n}\n\nTenemos una sola entrada y una respuesta numérica, y una muestra se ve como sigue:\n\nset.seed(1234)\ndatos_tbl &lt;- genera_datos(n = 500, tipo = \"entrena\")\nggplot(datos_tbl, aes(x = x, y = y)) + geom_jitter(width = 0.3) +\n  xlab(\"Años de estudio\") + ylab(\"Ingreso anual (miles)\")\n\n\n\n\n\nBuscamos construir una función \\(f\\) tal que si observamos cualquier \\(x = (x_1, x_2, \\ldots, x_p) \\to y\\), entonces nuestra predicción es\n\\[\\hat{y} = f(x_1, x_2, \\ldots, x_p) = f(x).\\] Con esta regla o algoritmo \\(f\\) queremos predecir con buena precisión el valor de \\(y\\). Esta \\(f\\), como explicamos antes, puede ser producida de muy distintas maneras (experiencia, reglas a mano, datos, etc.)\nNuestra primera tarea es definir qué quiere decir predecir con buena precisión.\nPara hacer esto tenemos que introducir una medida del error, que llamamos en general función de pérdida.\n\n\n\n\n\n\nFunción de pérdida y error de predicción\n\n\n\nSi el verdadero valor observado es \\(y\\) y nuestra predicción es \\(f(x)\\), denotamos la pérdida asociada a esta observación como \\[L(y, f(x))\\] Para medir el desempeño general de la regla \\(f\\), consideramos su valor esperado, el error de predicción, que es el promedio sobre toda la población:\n\\[Err(f) = E[L(y, f(x))]\\]\nEste es el error que obtendríamos promediando las pérdidas sobre toda la población de interés.\n\n\nObservación: Para fijar ideas, podríamos usar por ejemplo la pérdida absoluta \\(L(y, f(x)) = |y - f(x)|\\) o la pérdida cuadrática \\(L(y, f(x)) = (y - f(x))^2\\).\nAl menos en teoría, podemos encontrar una \\(f\\) que minimiza esta pérdida:\n\n\n\n\n\n\nPredictor óptimo\n\n\n\nPara una población dada, el predictor óptimo (teórico) es\n\\[f^* = \\underset{f}{\\mathrm{argmin}} E[L(y, f(x))].\\]\nEs decir: el mínimo error posible que podemos obtener es \\(Err(f^*)\\). Para cualquier otro predictor \\(f\\) tenemos que \\(Err(f) \\geq Err(f^*).\\)\n\n\nPor ejemplo si usamos la pérdida cuadrática \\(L(y, f(x)) = (y - f(x))^2\\), entonces puede mostrarse que\n\\[f^*(x) = E(y | x)\\] de forma que \\(f^*\\) es la media condicional de la \\(y\\) dado que sabemos que las entradas son \\(x\\). Si usáramos la pérdida absoluta \\(L(y, f(x)) = |y - f(x)|\\) entonces \\[f^*(x) = \\textrm{mediana}(y|x).\\] Distintas funciones de pérdida dan distintas soluciones teóricas. Por ejemplo, si existen valores atípicos en \\(y\\) producidos por errores de registro o medición, usar la pérdida absoluta puede dar mejores resultados que la cuadrática, que tiende a dar mayor peso a errores grandes.\nObservaciones:\n\nPodemos ver nuestra tarea entonces como una de ajuste de curvas: queremos aproximar tan bien como sea posible la función \\(f^*(x)\\).\nNo es simple decidir qué función de pérdida debería utilizarse para un problema dado de predicción.\nGeneralmente es una combinación de costos/beneficios del problema que tratamos, conveniencia computacional, y cómo se comportan los errores de nuestros predictores bajo distintas pérdidas. Sin embargo, al principio del proceso de construcción de modelos es mejor escoger una métrica simple que capture a grandes rasgos el comportamiento que esperamos (pérdida cuadrática, absoluta o logarítmica por ejemplo).\nMuchas veces es mejor considerar el problema de selección de la pérdida desde dos ángulos: el primero es computacional y de propiedades de la predicción, y el segundo tiene que ver con costos y beneficios asociados al problema que queremos resolver. Para el primero, alguna de las pérdidas estándar (como las que vimos arriba, cuadrática y absoluta, ologarítmica) son usualmente suficiente. En el segundo enfoque, el análisis es generalmente involucra más aspectos particulares del problema y generalmente tiene que hacerse de manera ad-hoc.\n\n\n\nEjemplo\nSupongamos que nos interesa minimizar la pérdida cuadrática. Si tomamos una muestra muy grande (para este problema), podemos aproximar la predicción óptima directamente. Abajo graficamos nuestra muestra chica de datos junto con una buena aproximación del predictor óptimo:\n\npoblacion_tbl &lt;- genera_datos(n = 50000, tipo = \"poblacion\")\n# calcular óptimo\npreds_graf_tbl &lt;- poblacion_tbl |&gt; \n  group_by(x) |&gt; # condicionar a x\n  summarise(.pred = mean(y)) |&gt; # media en cada grupo\n  mutate(predictor = \"_óptimo\")\n# graficar con una muestra grande\nggplot(datos_tbl, aes(x = x)) +\n  geom_jitter(aes(y = y), colour = \"red\") + \n  geom_line(data = preds_graf_tbl, aes(y = .pred, colour = predictor), \n    linewidth = 1.1) +\n  xlab(\"Años de estudio\") + ylab(\"Ingreso anual (miles)\")"
  },
  {
    "objectID": "02-principios-supervisado.html#estimando-el-desempeño-y-datos-de-prueba",
    "href": "02-principios-supervisado.html#estimando-el-desempeño-y-datos-de-prueba",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.2 Estimando el desempeño y datos de prueba",
    "text": "2.2 Estimando el desempeño y datos de prueba\nPara obtener una estimación de la pérdida para una función \\(f\\) que usamos para hacer predicciones, podemos tomar una muestra de datos del proceso generador:\n\\[{\\mathcal T} = \\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)})\\},\\]\nCompararíamos entonces las respuestas observadas \\(\\mathbf{y^{(i)}}\\) con las predicciones \\(f(\\mathbf{x^{(i)}})\\). Ahora resumimos evaluando el error promedio sobre los datos de prueba. El error de prueba de \\(f\\) es\n\\[ \\widehat{Err}(f) = \\frac{1}{m} \\sum_{i=1}^m L(\\mathbf{y}^{(i)} , f(\\mathbf{x}^{(i)}))\\] Por ejemplo, si usamos la pérdida cuadrática,\n\\[ \\widehat{Err}(f) = \\frac{1}{m} \\sum_{i=1}^m (\\mathbf{y}^{(i)} - f(\\mathbf{x}^{(i)}))^2\\] Si \\(m\\) es grande, entonces tenemos por la ley de los grandes números que \\[Err(f) \\approx \\widehat{Err} (f)\\] Podemos también estimar el error de estimación de \\(\\widehat{Err}(f)\\) con técnicas estándar, por ejemplo bootstrap o aproximación normal.\nObervación: nótese que en estos cálculos no es necesario hacer ningún supuesto acerca de \\(f\\), que en este argumento está fija y no utiliza la muestra de prueba.\n\nEjemplo: óptimo\nSupongamos que \\(f\\) es el predictor óptimo que obtuvimos arriba (pero esto aplica para cualquier otra función \\(f\\) que usemos para hacer predicciones). Tomamos una muestra de prueba, y evaluamos usando la raíz de la pérdida cuadrática media:\n\nprueba_tbl &lt;- genera_datos(n = 2000, tipo = \"prueba\")\neval_tbl &lt;- prueba_tbl |&gt;  \n  left_join(preds_graf_tbl, by = \"x\") \nresumen_tbl &lt;- eval_tbl |&gt;  \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen &lt;- function(resumen_tbl){\n  resumen_tbl |&gt; \n    select(-.estimator) |&gt; \n    pivot_wider(names_from = tipo, values_from = .estimate) |&gt; \n    gt() |&gt; \n    fmt_number(where(is_double), decimals = 0)\n}\nfmt_resumen(resumen_tbl)\n\n\n\n\n\n  \n    \n    \n      predictor\n      .metric\n      prueba\n    \n  \n  \n    _óptimo\nrmse\n49\n  \n  \n  \n\n\n\n\nEste es nuestro error de prueba. Como la muestra de prueba no es muy grande, podríamos usar un método estándar para estimar su precisión, por ejemplo con bootstrap.\n\n\nEjemplo: regla\nAhora probemos con otro predictor, por ejemplo, supongamos que estamos usando la regla de “cada año de escolaridad aumenta ingresos potenciales en 20 unidades”, un predictor construido con reglas manuales que es\n\nf_regla &lt;- function(x){\n  20 * x\n}\n\nAbajo lo graficamos en comparación con el modelo óptimo:\n\naños_x &lt;- tibble(x = seq(0, 17, by = 0.5))\npreds_regla_tbl &lt;- años_x |&gt; \n  mutate(.pred = f_regla(x), predictor = \"regla\")\npreds_graf_tbl &lt;- bind_rows(preds_regla_tbl, preds_graf_tbl)\nggplot(datos_tbl, aes(x = x)) +\n  geom_point(aes(y = y), colour = \"red\", alpha = 0.2) +\n  geom_line(data = preds_graf_tbl, aes(y = .pred, colour = predictor), size = 1.1) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\neval_tbl &lt;- prueba_tbl |&gt;  \n  left_join(preds_graf_tbl, by = \"x\") \n\nWarning in left_join(prueba_tbl, preds_graf_tbl, by = \"x\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 21 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nresumen_tbl &lt;- eval_tbl |&gt;  \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(resumen_tbl)\n\n\n\n\n\n  \n    \n    \n      predictor\n      .metric\n      prueba\n    \n  \n  \n    _óptimo\nrmse\n49\n    regla\nrmse\n91\n  \n  \n  \n\n\n\n\nObserva que el error es considerablemente mayor que el error que obtuvimos con el predictor óptimo del ejemplo anterior. Quisiéramos buscar algoritmos que tengan mejor desempeño aprendiendo de datos anteriores."
  },
  {
    "objectID": "02-principios-supervisado.html#aprendizaje",
    "href": "02-principios-supervisado.html#aprendizaje",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.3 Aprendizaje supervisado",
    "text": "2.3 Aprendizaje supervisado\nEn aprendizaje supervisado, buscamos construir la función \\(f\\) de manera automática usando datos. Supongamos entonces que tenemos un conjunto de datos etiquetados (sabemos la \\(y\\) correspondiente a cada \\(x\\)):\n\\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\]\nque llamamos conjunto de entrenamiento.\nUn algoritmo de aprendizaje (aprender de los datos automáticamente) es una regla que asigna a cada conjunto de entrenamiento \\({\\mathcal L}\\) una función \\(\\hat{f}\\):\n\\[{\\mathcal L} \\to \\hat{f} = f_{\\mathcal L} \\]\nUna vez que construimos la función \\(\\hat{f}\\), podemos hacer predicciones.\nEl desempeño del predictor particular \\(\\hat{f}\\) se mide igual que antes: observamos otra muestra \\({\\mathcal T}\\), que llamamos muestra de prueba,\n\\[{\\mathcal T} = \\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)})\\},\\]\ny calculamos el error de prueba. Si suponemos que \\(m\\) es suficientemente grande:\n\\[ \\widehat{Err}(\\hat{f}) = \\frac{1}{m} \\sum_{i=1}^m L(\\mathbf{y}^{(i)} , \\hat{f}(\\mathbf{x}^{(i)})) \\]\nes una buena aproximación del error de predicción \\(Err(\\hat{f})\\).\nAdicionalmente, definimos otra cantidad de menor interés, el error de entrenamiento, como\n\\[\\overline{err} = \\frac{1}{N}\\sum_{i=1}^N L(y^{(i)} , \\hat{f}(x^{(i)})).\\] que es una medida de qué tan bien se ajusta a \\(\\hat{f}\\) a los datos con los que se entrenó \\(\\hat{f}\\). Usualmente esta cantidad no es apropiada para medir el desempeño de un predictor, pues el algoritmo \\(\\hat{f}\\) incluye las “respuestas” \\(y_i\\) en su construcción, de forma que tiende a ser una estimación optimista del error de predicción.\n\nEjemplo: vecinos más cercanos\nConsideremos usar un método de \\(k\\)-vecinos más cercanos para resolver este problema. Este método es simple: si queremos hacer una predicción en las entradas \\(x\\), buscamos los puntos de entrenamiento con entradas \\(x^{(i)}\\) más cercanas a \\(x\\), que denotamos como \\(N_k(x)\\). Tomamos las \\(y\\) correspondientes a estas \\(x\\) y las usamos para hacer nuestra predicción:\n\\[f_2(x) = \\frac{1}{k}\\sum_{x^{(i)} \\in N_k(x)} y^{(i)}\\]\nPrimero obtendremos una muestra de entrenamiento:\n\nset.seed(12)\nentrena_tbl &lt;- genera_datos(n = 20, tipo = \"entrena\")\n\nEn nuestro ejemplo, en lugar de usar un número fijo de vecinos, utilizaremos 10% de los datos más cercanos al punto donde queremos predecir:\n\n# modelo\nmodelo_kvecinos &lt;- nearest_neighbor(\n    neighbors = nrow(entrena_tbl) * 0.1, \n    weight_func = \"gaussian\") |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"kknn\")\n# preprocesamiento\nreceta &lt;- recipe(y ~ x, data = entrena_tbl |&gt; select(x, y))\n# flujo\nflujo &lt;- workflow() |&gt; \n  add_recipe(receta) |&gt; \n  add_model(modelo_kvecinos)\n# Ajustamos flujo\nflujo_ajustado_vecinos &lt;- fit(flujo, entrena_tbl)\n\nHacemos predicciones y calculamos el error:\n\neval_tbl &lt;- bind_rows(prueba_tbl, entrena_tbl) \nresumen_vmc_tbl &lt;- \n  predict(flujo_ajustado_vecinos, eval_tbl) |&gt; \n  mutate(predictor = \"vecinos\") |&gt; \n  bind_cols(eval_tbl) |&gt; \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(resumen_vmc_tbl)\n\n\n\n\n\n  \n    \n    \n      predictor\n      .metric\n      entrena\n      prueba\n    \n  \n  \n    vecinos\nrmse\n36\n65\n  \n  \n  \n\n\n\n\nEl error de prueba, que es el que nos interesa hacer chico, es considerablemente grande. Si graficamos podemos ver el problema:\n\npreds_vmc &lt;- predict(flujo_ajustado_vecinos, años_x) |&gt; \n  bind_cols(años_x) |&gt; mutate(predictor = \"vecinos\")\npreds_graf_tbl &lt;- bind_rows(preds_vmc, preds_graf_tbl |&gt; \n  filter(predictor == \"_óptimo\"))\ng_1 &lt;- ggplot(entrena_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |&gt; filter(predictor != \"regla\"), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  geom_point(aes(y = y), colour = \"red\") +\n  labs(subtitle = \"Óptimo vs ajustado\")\ng_1\n\n\n\n\nDonde vemos que este método intenta interpolar los datos, capturando ruido y produciendo variaciones que lo alejan del modelo óptimo. Esto lo notamos en lo siguiente:\n\nHay una brecha grande entre el error de entrenamiento y el error predictivo.\nEsta estimación de vecinos más cercanos es muy dependiente de la muestra de entrenamiento que obtengamos, pues intenta casi interpolar los datos. Esto sugiere alta variabilidad de las predicciones dependiendo de la muestra particular de entrenamiento que utilizamos.\nDecimos que este predictor está sobreajustado.\n\n\n\nEjemplo: regresión lineal\nAhora intentaremos con un modelo lineal. En este caso, utilizamos un predictor de la forma\n\\[f(x) = \\beta_0 + \\beta_1x\\] Usamos la muestra de entrenamiento para encontrar la \\(\\beta_0\\) y \\(\\beta_1\\) que minimizar el error sobre los datos disponibles de entrenamiento, lo cual es un problema de optimización relativamente fácil. Usamos entonces \\[\\hat{f}(x) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x\\] para hacer nuestras predicciones.\n\nmodelo_lineal &lt;- linear_reg() |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"lm\")\nflujo_lineal &lt;- workflow() |&gt; \n  add_recipe(receta) |&gt; \n  add_model(modelo_lineal)\n# Ajustamos\nflujo_ajustado_lineal &lt;- fit(flujo_lineal, entrena_tbl)\n\nHacemos predicciones y calculamos el error:\n\neval_tbl &lt;- bind_rows(prueba_tbl, entrena_tbl) \nresumen_lineal_tbl &lt;- \n  predict(flujo_ajustado_lineal, eval_tbl) |&gt; \n  mutate(predictor = \"lineal\") |&gt; \n  bind_cols(eval_tbl) |&gt; \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(bind_rows(resumen_vmc_tbl, resumen_lineal_tbl))\n\n\n\n\n\n  \n    \n    \n      predictor\n      .metric\n      entrena\n      prueba\n    \n  \n  \n    vecinos\nrmse\n36\n65\n    lineal\nrmse\n49\n56\n  \n  \n  \n\n\n\n\nY el desempeño de este método es mejor que vecinos más cercanos (ver columna de prueba).\n\npreds_1 &lt;- predict(flujo_ajustado_lineal, tibble(x = 0:17)) |&gt; \n  bind_cols(tibble(x = 0:17, predictor = \"lineal\"))\npreds_graf_tbl &lt;- bind_rows(preds_1, preds_graf_tbl)\ng_1 &lt;- ggplot(datos_tbl, aes(x = x)) +\n  geom_point(aes(y = y), colour = \"red\", alpha = 0.1) +\n  geom_line(data = preds_graf_tbl |&gt; filter(predictor %in% c(\"_óptimo\", \"lineal\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Óptimo vs ajustado\")\ng_1\n\n\n\n\nEn este caso:\n\nNo hay brecha tan grande entre el error de entrenamiento y el error predictivo\nObservamos patrones claros de desajuste: el predictor lineal no captura el patrón curvo que presentan los datos: en la parte media de las \\(x\\) tiende a producir predicciones demasiado altas y lo contario ocurre en los extremos\nDecimos que esté modelo presenta subajuste."
  },
  {
    "objectID": "02-principios-supervisado.html#entendiendo-el-error-de-predicción",
    "href": "02-principios-supervisado.html#entendiendo-el-error-de-predicción",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.4 Entendiendo el error de predicción",
    "text": "2.4 Entendiendo el error de predicción\nEstos dos ejemplos de predictores tienen mal desempeño (comparado con el óptimo por distintas razones. Para entender qué pasa, consideramos los residuales de cada ajuste, para un caso de prueba:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})\\] Esta cantidad puede tener un valor positivo o negativo grande, lo que indica errores grandes. Sea \\(f^*\\) el predictor óptimo que explicamos arriba. Entonces, en primer lugar:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{(f^* (\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{reducible} + \\underbrace{(\\mathbf{y}- f^*(\\mathbf{x}))}_\\text{irreducible}.\\] donde vemos que si las dos cantidades de la derecha están cercanas a cero, entonces el residual es cercano a cero (la predicción es precisa):\n\nError irreducible: no depende de nuestro algoritmo, sino de la información que tenemos en \\(x\\) para predecir \\(y\\). Si queremos hacer esté error más chico, necesitamos incluir otras variables \\(x\\) relevantes para predecir \\(y\\).\nError reducible: qué tan lejos nuestro método está del óptimo. Podemos mejorar este error seleccionando nuestra muestra de entrenamiento y método de predicción \\(\\hat{f}\\) de manera adecuada.\n\nEn nuestros dos ejemplos anteriores, el error reducible era considerablemente grande (como podemos verificar comparando con el predictor óptimo, que sólo sufre de error irreducible). Pero la razón por la que ese error reducible es grande es diferente en cada caso.\nPara explicar la diferencia, podemos considerar \\(f_{lim},\\) el predictor que obtendríamos con nuestro método si ajustáramos nuestro método con la población completa, de manera que \\(\\hat{f_{\\mathcal{L}}}\\to f_{\\lim}\\) cuando el tamaño de la muestra de entrenamiento \\({\\mathcal{L}}\\) se hace muy grande.\nPodemos refinar nuestra descomposición y escribir:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{f^* (\\mathbf{x}) - f_{\\lim}(\\mathbf{x})}_\\text{sesgo-especificacion} +\n  \\underbrace{f_{\\lim}(\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})}_\\text{error-estimacion} +\n  \\underbrace{y - f^*(\\mathbf{x})}_\\text{irreducible}.\\]\nEl error reducible ahora se descompone en dos partes:\n\nEl sesgo de especificacion: que se debe a la incapacidad de nuestro modelo de capturar la forma del predictor óptimo, incluso conociendo toda la población. Este término no depende de la muestra de entrenamiento: depende de la capacidad de nuestro método para aprender en condiciones ideales.\nEl error de estimación: este error resulta de que tenemos información limitada de la población, y nuestro ajuste se aleja de lo que obtendríamos con información completa. Esta parte del error varía dependiendo de la muestra particular de entrenamiento que utilizamos.\n\nEn nuestros dos ejemplos, intuímos que vecinos más cercanos sufre más de error de estimación y regresión lineal de sesgo de especificación, lo cual verificamos más adelante.\nPodemos refinar aún más nuestra descomposición considerando qué pasa con distintas muestras del mismo tamaño para entender mejor el error de estimación. Si consideramos el valor esperado de nuestra predicción a lo largo de las posibles muestras \\(\\mathcal L\\) que podemos extraer, descomponemos el segundo término como:\n\\[\\hat{f_{\\mathcal{L}}}(\\mathbf{x}) - f_{\\lim}(\\mathbf{x})  = \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) -  E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) + E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) - f_{\\lim}(\\mathbf{x}) \\] donde el valor esperado es sobre todas las muestras de entrenamiento de un tamaño fijo \\(n\\) que podríamos obtener. El primer término puede llamarse variabilidad, mientras que el segundo es el sesgo que obtenemos al usar una muestra \\(n\\) finita (para algunos métodos, el segundo término puede ser igual a cero). Desde este punto de vista, podemos hacer también la descomposición:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{f^* (\\mathbf{x}) - f_{lim}(\\mathbf{x})}_\\text{sesgo-especificacion}  + \\underbrace{f_{lim}(\\mathbf{x}) - E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{sesgo-estimacion} +   \\underbrace{E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})}_\\text{variabilidad} + \\underbrace{ \\mathbf{y} - f^*(\\mathbf{x})}_\\text{irreducible}.\\]\nTenemos entonces cuatro términos:\n\nEl sesgo de especificación mide la capacidad del modelo de utilizar datos de muestras cada vez más grandes. No depende de una muestra particular ni su tamaño.\nEl sesgo de estimación mide en promedio qué tan lejos está la estimación del ideal con datos completos, y depende de la naturaleza de la muestra de entrenamiento, incluyendo su tamaño.\nLa variabilidad es el único término que depende de la muestra particular que usamos. También depende del tamaño de muestra que utilizamos.\n\nLos dos primeros términos usualmente se agrupan en un sólo término de sesgo, y obtenemos la siguiente definición usual:\n\n\n\n\n\n\nDescomposición sesgo-varianza\n\n\n\nEl error total (la diferencia entre observado y nuestra predicción) se descompone como:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{f^* (\\mathbf{x}) - E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{sesgo} +   \\underbrace{E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})}_\\text{variabilidad} + \\underbrace{y - f^*(\\mathbf{x})}_\\text{irreducible}.\\]\n\n\nQue explica qué sucede con distintas posibles muestras de entrenamiento de tamaño fijo. El sesgo en este caso significa en promedio qué tan lejos nuestro predictor está del óptimo, y la variabilidad qué tanto puede variar nuestra predicción con respecto al promedio."
  },
  {
    "objectID": "02-principios-supervisado.html#ejemplo-fuentes-de-error",
    "href": "02-principios-supervisado.html#ejemplo-fuentes-de-error",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.5 Ejemplo: fuentes de error",
    "text": "2.5 Ejemplo: fuentes de error\nVamos a ver qué sucede con nuestros dos métodos si utilizamos una muestra grande:\n\nmuestra_grande_tbl &lt;- sample_n(poblacion_tbl, 10000) |&gt; \n  mutate(tipo = \"entrena\")\nmodelo_kvecinos &lt;- nearest_neighbor(\n    neighbors = nrow(muestra_grande_tbl) * 0.10, \n    weight_func = \"gaussian\") |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"kknn\")\n# Ajustamos (no es necesario usar la población completa para este ejemplo)\nflujo_vecinos &lt;- workflow() |&gt; \n  add_recipe(receta) |&gt; \n  add_model(modelo_kvecinos)\nflujo_ajustado_vecinos_limite &lt;- fit(flujo_vecinos, muestra_grande_tbl)\nflujo_ajustado_lineal_limite &lt;- fit(flujo_lineal, muestra_grande_tbl)\n\neval_tbl &lt;- bind_rows(prueba_tbl, muestra_grande_tbl)\nresumen_vecinos_lim_tbl &lt;- \n  predict(flujo_ajustado_vecinos_limite, eval_tbl) |&gt; \n  mutate(predictor = \"vecinos_limite\") |&gt; \n  bind_cols(eval_tbl) |&gt; \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nresumen_lineal_lim_tbl &lt;- \n  predict(flujo_ajustado_lineal_limite, eval_tbl) |&gt; \n  mutate(predictor = \"lineal_limite\") |&gt; \n  bind_cols(eval_tbl) |&gt; \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(bind_rows(resumen_vmc_tbl, \n  resumen_lineal_tbl, \n  resumen_vecinos_lim_tbl, \n  resumen_lineal_lim_tbl) |&gt; arrange(predictor))\n\n\n\n\n\n  \n    \n    \n      predictor\n      .metric\n      entrena\n      prueba\n    \n  \n  \n    lineal\nrmse\n49\n56\n    lineal_limite\nrmse\n54\n54\n    vecinos\nrmse\n36\n65\n    vecinos_limite\nrmse\n49\n49\n  \n  \n  \n\n\n\n\n¿Qué patrones ves en esta tabla? Podemos también graficar para entender mejor qué está pasando:\n\npreds_1 &lt;- predict(flujo_ajustado_vecinos_limite, tibble(x = 0:17)) |&gt; \n  bind_cols(tibble(x = 0:17, predictor = \"vecinos_limite\"))\npreds_2 &lt;- predict(flujo_ajustado_lineal_limite, tibble(x = 0:17)) |&gt; \n  bind_cols(tibble(x = 0:17, predictor = \"lineal_limite\"))\npreds_graf_tbl &lt;- bind_rows(preds_1, preds_2, preds_graf_tbl) |&gt; \n  mutate(predictor = factor(predictor))\ng_1 &lt;- ggplot(datos_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |&gt; \n            filter(str_detect(predictor, \"vecinos|_óptimo\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Vecinos\") \ng_2 &lt;- ggplot(datos_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |&gt; \n            filter(str_detect(predictor, \"lineal|_óptimo\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Lineal\") \ng_1 + g_2 + plot_layout(guides = 'collect')\n\n\n\n\nEsto patrón sugiere que:\n\nNuestro método de vecinos más cercanos tiene errores bajos por sesgo, pero tiene error considerable por sobreajuste o variabilidad.\nNuestro método lineal no tiene mucha variabilidad (el estimado con una muestra grande es casi igual al de la muestra de entrenamiento), sino más bien por sesgo.\nEl error por sesgo se reduce usando métodos más flexibles o menos restringidos que puedan capturar patrones claros en los datos.\nPara reducir la variabilidad podemos usar métodos más simples o restringidos que no capturen tanto ruido.\nEl balance de complejidad correcto depende del tamaño de muestra de entrenamiento.\nEl error irreducible se puede reducir incorporando información adicional relevante a las entradas.\n\n\n\n\n\n\n\nComplejidad y error de predicción\n\n\n\nPara un tamaño de muestra de entrenamiento fijo,\n\nMétodos de predicción más flexibles o complejos tienden a sufrir más de error por variabilidad, pues dependen fuertemente de la muestra utilizada.\nMétodos de predicción más rígidos o simples tienden a sufrir más error por sesgo, pues dependen menos de la muestra utilizada."
  },
  {
    "objectID": "02-principios-supervisado.html#agregando-más-información-y-error-irreducible",
    "href": "02-principios-supervisado.html#agregando-más-información-y-error-irreducible",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.6 Agregando más información y error irreducible",
    "text": "2.6 Agregando más información y error irreducible\nPodemos ver qué sucede cuando tenemos disponibles más variables relevantes. En este caso, probaremos con dos entradas:\n\ngenera_datos_2 &lt;- function(n = 500, tipo = NULL){\n  dat_tbl &lt;- tibble(nse = runif(n, 0, 100)) |&gt;\n    mutate(estudio_años = floor(rnorm(n, 1.5 * sqrt(nse), 1))) |&gt;\n    mutate(estudio_años = pmax(0, pmin(17, estudio_años))) |&gt; \n    mutate(habilidad = rnorm(n, 100 + 0.1 * nse, 1)) |&gt; \n    mutate(z = 100 + (habilidad/100) * ( 20 * nse + 5 * (estudio_años))) |&gt; \n    mutate(ingreso = pmax(0, 0.2*(z + rnorm(n, 0, 150))))\n  obs_tbl &lt;- dat_tbl |&gt; \n    mutate(tipo = tipo, id = 1:n)\n  obs_tbl |&gt; select(id, tipo, x_1 = estudio_años, x_2 = nse,  y = ingreso)\n}\n\n\nentrena_tbl &lt;- genera_datos_2(20, tipo = \"entrena\")\nprueba_tbl &lt;- genera_datos_2(500, tipo = \"prueba\")\nreceta_2 &lt;- recipe(y ~ x_1 + x_2, data = entrena_tbl)\nflujo_lineal &lt;- workflow() |&gt; \n  add_recipe(receta_2) |&gt; \n  add_model(modelo_lineal)\n# Ajustamos\nflujo_ajustado_lineal &lt;- fit(flujo_lineal, entrena_tbl)\npredict(flujo_ajustado_lineal, bind_rows(entrena_tbl, prueba_tbl)) |&gt; \n  bind_cols(bind_rows(entrena_tbl, prueba_tbl)) |&gt;\n  group_by(tipo) |&gt; \n  rmse(truth = y, estimate = .pred) |&gt; gt() |&gt; \n  fmt_number(.estimate, decimals = 1)\n\n\n\n\n\n  \n    \n    \n      tipo\n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    entrena\nrmse\nstandard\n27.1\n    prueba\nrmse\nstandard\n31.5\n  \n  \n  \n\n\n\n\nY vemos cómo inmediatamente redujimos el error de predicción: en este caso, aunque la variabilidad aumentó un poco (tenemos más parámetros que estimar vs el modelo con una sola variable), la reducción en el sesgo y en el error irreducible es tan grande que el desempeño es muy superior. Examina el caso de vecinos más cercanos."
  },
  {
    "objectID": "02-principios-supervisado.html#acerca-de-la-estimación-del-error-de-predicción",
    "href": "02-principios-supervisado.html#acerca-de-la-estimación-del-error-de-predicción",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.7 Acerca de la estimación del error de predicción",
    "text": "2.7 Acerca de la estimación del error de predicción\nCuando usamos una muestra de prueba limitada, podemos evaluar la precisión de nuestra estimación del error de predicción usando por ejemplo el bootstrap. En nuestro ejemplo anterior podríamos hacer los siguiente:\n\nlibrary(infer)\npreds &lt;- predict(flujo_ajustado_lineal, bind_rows(prueba_tbl)) |&gt; \n  bind_cols(prueba_tbl) \npreds |&gt; \n  generate(reps = 1000, type = \"bootstrap\", variables = id) |&gt; \n  group_by(replicate, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) |&gt; \n  select(replicate, tipo, stat = .estimate) |&gt;\n  get_ci(level = 0.90) |&gt; \n  gt() |&gt; fmt_number(where(is_double), decimals = 1)\n\nWarning: The `variables` argument is only relevant for the \"permute\" generation\ntype and will be ignored.\n\n\n\n\n\n\n  \n    \n    \n      lower_ci\n      upper_ci\n    \n  \n  \n    29.9\n33.0"
  },
  {
    "objectID": "02-principios-supervisado.html#resumen",
    "href": "02-principios-supervisado.html#resumen",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.8 Resumen",
    "text": "2.8 Resumen\n\n\n\n\n\n\nTarea fundamental del análisis supervisado\n\n\n\n\nUsando datos de entrenamiento \\({\\mathcal L}\\), construimos una funcion \\(\\hat{f}\\) para predecir. Estas funciones se ajustan usualmente intentando estimar directamente el predictor óptimo \\(f^*(x)\\) (si lo conocemos teóricamente), o indirectamente intentando minimizar la pérdida sobre el conjunto de entrenamiento.\nSi observamos nuevos valores \\(\\mathbf{x}\\), nuestra predicción es \\(\\hat{y} = \\hat{f}(\\mathbf{x})\\).\nBuscamos que cuando observemos nuevos casos para predecir, nuestro error de predicción sea bajo en promedio (\\(Err\\) sea bajo).\nUsualmente estimamos \\(Err\\) mediante una muestra de prueba o validación \\({\\mathcal T}\\).\nNos interesan métodos de construir \\(\\hat{f}\\) que produzcan errores de predicción bajos.\n\n\n\n\nNótese que el error de entrenamiento se calcula sobre la muestra \\({\\mathcal L}\\) que se usó para construir \\(\\hat{f}\\), mientras que el error de predicción se estima usando una muestra independiente \\({\\mathcal T}\\).\n\\(\\hat{Err}\\) es una estimación razonable de el error de predicción \\(Err\\) (por ejemplo, \\(\\hat{Err} \\to Err\\) cuando el tamaño de la muestra de prueba crece), pero \\(\\overline{err}\\) típicamente es una estimación mala del error de predicción.\nNótese también que aunque generalmente podemos ajustar reduciendo el error de entrenamiento, lo que queremos es reducir el error de prueba: es decir, el error fuera de la muestra de entrenamiento.\n\n\n\n\n\n\n\nReduciendo el error de predicción\n\n\n\nPara reducir el error de predicción, podemos:\n\nIncluir variables relevantes que reduzcan el error irreducible\nReducir variabilidad usando métodos más estables o menos complejos\nReducir sesgo usando métodos más flexibles\nUsar métodos con la estructura adecuada para el problema\n\nGeneralmente 2 y 3 están en contraposición, a lo que muchas veces se le llama equilibrio de varianza y sesgo. Los puntos 1 y 4 generalmente mejoran los resultados reduciendo tanto sesgo como variabilidad."
  },
  {
    "objectID": "02-principios-supervisado.html#resolver-problemas-con-aprendizaje-automático",
    "href": "02-principios-supervisado.html#resolver-problemas-con-aprendizaje-automático",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.9 Resolver problemas con aprendizaje automático",
    "text": "2.9 Resolver problemas con aprendizaje automático\nEn este curso nos concentraremos en la construcción, evaluación y mejora de modelos predictivos. Para que estas ideas funcionen en problemas reales, hay más aspectos a considerar que no discutiremos con tanto detalle, pues en general están muy ligados al problema particular de predicción que nos interesa (y muchas veces son considerablemente más difíciles de la teoría y los algoritmos):\n\nPara entender exactamente cuál es el problema que queremos resolver se requiere trabajo analítico considerable, y también trabajo en entender aspectos del área o negocio donde nos interesa usar aprendizaje máquina. Muchas veces es fácil resolver un problema muy preciso, que tenemos a la mano, pero que más adelante nos damos cuenta de que no es útil.\nEstos dos puntos incluyen indentificar las métricas que queremos monitorear y mejorar, lo cual no siempre es claro. Optimizar métricas incorrectas es poco útil en el mejor de los casos, y en los peores pueden causar daños. Evitar esto requiere monitoreo constante de varios aspectos del funcionamiento de nuestros modelos y sus consecuencias.\n¿Cómo poner en producción modelos y mantenerlos? Un flujo apropiado de trabajo, que comienza con pipelines de preproceso y heurísticas simples, para después utilizar modelos de aprendizaje automático, seguido de monitoreo y entrenamiento continuo son cruciales para tener éxito con este enfoque."
  },
  {
    "objectID": "03-metodos-locales.html#controlando-complejidad",
    "href": "03-metodos-locales.html#controlando-complejidad",
    "title": "3  Métodos locales no estructurados",
    "section": "3.1 Controlando complejidad",
    "text": "3.1 Controlando complejidad\nPrimero examinamos cómo controlamos el nivel de complejidad para un método local como \\(k\\) vecinos más cercanos. La idea es que:\n\nMás complejidad: Si tomamos \\(k\\) demasiado chica, cada estimación usa pocos datos y puede ser ruidosa (incurrimos en variabilidad). Sin embargo, el predictor resultante puede ajustarse a patrones locales y globales.\nMenos complejidad: Si tomamos \\(k\\) demasiado grande, cada estimación usa potencialmente datos no relevantes muy lejanos a donde queremos predecir (incurrimos en sesgo), sin embargo cada estimación es más estable pues utiliza más datos.\n\nComenzamos con un ejemplo simple en dimensión baja:\n\nEjemplo\n\nlibrary(tidyverse)\nlibrary(gt)\nauto &lt;- read_csv(\"../datos/auto.csv\")\n# seleccionar variables y poner en sistema métrico\ndatos &lt;- auto |&gt; \n  select(name, weight, year, mpg, displacement) |&gt; \n  mutate(\n    peso_kg = weight * 0.45359237,\n    rendimiento_kpl = mpg * (1.609344 / 3.78541178), \n    año = year\n  )\n\nVamos a separa en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (75% para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba):\n\nlibrary(tidymodels)\nset.seed(121)\ndatos_split &lt;- initial_split(datos, prop = 0.75)\ndatos_entrena &lt;- training(datos_split)\ndatos_prueba &lt;- testing(datos_split)\nnrow(datos_entrena)\n\n[1] 294\n\nnrow(datos_prueba)\n\n[1] 98\n\n\nVamos a usar año y peso de los coches para predecir su rendimiento:\n\nggplot(datos_entrena, \n  aes(x = peso_kg, y = rendimiento_kpl, colour = año)) +\n  geom_point()\n\n\n\n\nProbaremos con varios valores para \\(k\\), el número de vecinos más cercanos. La función de predicción ajustada es entonces:\n\n# nótese que normalizamos entradas - esto también es importante\n# hacer cuando hacemos vecinos más cercanos, pues en otro caso\n# las variables con escalas más grandes dominan el cálculo\nvmc_1 &lt;- nearest_neighbor(neighbors = tune(), weight_func = \"gaussian\") |&gt;  \n  set_engine(\"kknn\") |&gt;  \n  set_mode(\"regression\")\nreceta_vmc &lt;- recipe(\n  rendimiento_kpl ~ peso_kg + año, datos_entrena) |&gt; \n  step_normalize(all_predictors()) \nflujo_vecinos &lt;- workflow() |&gt;  \n  add_recipe(receta_vmc) |&gt; \n  add_model(vmc_1)\n# definir parámetros que nos interesa explorar\nvecinos_params &lt;- parameters(neighbors(range = c(1, 100)))\n# definir cuáles valores de los parámetros exploramos\nvecinos_grid &lt;- grid_regular(vecinos_params, levels = 100)\nmis_metricas &lt;- metric_set(rmse)\n\nEn la siguiente gráfica mostramos cómo cambia el error de los las predicciones sobre la muestra de prueba separada de la de entrenamiento. En este caso le llamaremos muestra de validación porque más adelante veremos que puede ser conveniente dividir en entrenamiento-validación-prueba en lugar de usar sólo 2 particiones:\n\nr_split &lt;- manual_rset(list(datos_split), \"validación\")\nvecinos_eval_tbl &lt;- tune_grid(flujo_vecinos,\n                            resamples = r_split,\n                            grid = vecinos_grid,\n                            metrics = mis_metricas) \nvecinos_ajustes_tbl &lt;- vecinos_eval_tbl |&gt;\n  unnest(cols = c(.metrics)) |&gt; \n  select(id, neighbors, .metric, .estimate)\nggplot(vecinos_ajustes_tbl, aes(x = neighbors, y = .estimate)) +\n  geom_line() + geom_point() +\n  ylab(\"Error de validación\") + xlab(\"Vecinos\")\n\n\n\n\nDonde obtenemos más o menos lo que esperaríamos: modelos con muy pocos vecinos o demasiados vecinos se desempeñan relativamente mal.\nSeleccionaremos el mejor modelo según el error estimado de predicción y visualizamos primero nuestras predicciones y los datos de entrenamiento de la siguiente forma:\n\nmejor_rmse &lt;- select_best(vecinos_eval_tbl, metric = \"rmse\")\najuste_1 &lt;- finalize_workflow(flujo_vecinos, mejor_rmse) |&gt; \n  fit(datos_entrena)\ndat_graf &lt;- tibble(peso_kg = seq(900, 2200, by = 10)) |&gt; \n  crossing(tibble(año = c(70, 75, 80)))\ndat_graf &lt;- dat_graf |&gt; \n  mutate(pred_1 = predict(ajuste_1, dat_graf) |&gt; pull(.pred))\nggplot(datos_entrena, aes(x = peso_kg, group = año, colour = año)) +\n  geom_point(aes(y = rendimiento_kpl), alpha = 0.6) + \n  geom_line(data = dat_graf, aes(y = pred_1),  linewidth = 1.2)\n\n\n\n\nEl método parece funcionar razonablemente bien para este problema simple. Sin embargo, si el espacio de entradas no es de dimensión baja, entonces podemos encontrarnos con dificultades."
  },
  {
    "objectID": "03-metodos-locales.html#la-maldición-de-la-dimensionalidad",
    "href": "03-metodos-locales.html#la-maldición-de-la-dimensionalidad",
    "title": "3  Métodos locales no estructurados",
    "section": "3.2 La maldición de la dimensionalidad",
    "text": "3.2 La maldición de la dimensionalidad\nEl método de k-vecinos más cercanos funciona mejor cuando\n\nNo es necesario hacer \\(k\\) demasiado grande, de forma que terminemos tomando valores lejanos que inducen sesgo.\nNo es necesario hacer \\(k\\) demasiado chica, de forma que nuestras predicciones sean inestables.\n\n\n\n\n\n\n\nMaldición de la dimensionalidad\n\n\n\nEn dimensión alta, para la mayoría de las \\(\\mathbf{x}\\) donde queremos hacer predicciones típicamente no existen vecinos cercanos, aún para conjuntos de entrenamiento muy grandes.\n\n\nEsto implica que para tamaños típicos \\(n\\) de muestra de entrenamiento:\n\nSi tomamos \\(k\\) chica, el sesgo por especificación es chico (muestras muy grandes), pero el sesgo de estimación puede ser grande pues estamos de todas formas obligados a buscar vecinos lejos de donde queremos predecir. La variabilidad también es alta pues usamos pocos datos para cada predicción.\nSi tomamos \\(k\\) más grande, el sesgo por especificación tiende ser más grande (pues promediamos sobre regiones relativamente grandes). Perdemos la supuesta ventaja del método local, aún cuando quizá reduzcamos el sesgo de estimación.\nPara que una \\(k\\) chica tenga sesgo de estimación bajo, el tamaño \\(n\\) de la muestra de entrenamiento tiene que ser gigantesca.\n\n\nEjemplo\nConsideremos que la salida Y es determinística \\(Y = e^{-8\\sum_{j=1}^p x_j^2}\\). Vamos a usar 1-vecino más cercano para hacer predicciones, con una muestra de entrenamiento de 1000 casos. Generamos $x^{i}’s uniformes en \\([ 1,1]\\), para \\(p = 2\\), y calculamos la respuesta \\(Y\\) para cada caso:\n\nfun_exp &lt;- function(x) exp(-8 * sum(x ^ 2))\nx &lt;- map(1:1000, ~ runif(2, -1, 1))\ndat &lt;- tibble(x = x) |&gt; \n        mutate(y = map_dbl(x, fun_exp))\nggplot(dat |&gt; mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), \n       aes(x = x_1, y = x_2, colour = y)) + geom_point()\n\n\n\n\nLa mejor predicción en \\(x_0 = (0,0)\\) es \\(f((0,0)) = 1\\). El vecino más cercano al origen es\n\ndat &lt;- dat |&gt; mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |&gt; \n  arrange(dist_origen)\nmas_cercano &lt;- dat[1, ]\nmas_cercano\n\n# A tibble: 1 × 3\n  x             y dist_origen\n  &lt;list&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 &lt;dbl [2]&gt; 0.995      0.0261\n\nmas_cercano$x[[1]]\n\n[1] -0.025090354  0.007277334\n\n\nNuestra predicción es entonces \\(\\hat{f}(0)=\\) 0.994555, que es bastante cercano al valor verdadero (1).\nAhora intentamos hacer lo mismo para dimensión \\(p=8\\).\n\nx &lt;- map(1:1000, ~ runif(8, -1, 1))\ndat &lt;- tibble(x = x) |&gt; \n       mutate(y = map_dbl(x, fun_exp))\ndat &lt;- dat |&gt; mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |&gt; \n  arrange(dist_origen)\nmas_cercano &lt;- dat[1, ]\nmas_cercano\n\n# A tibble: 1 × 3\n  x             y dist_origen\n  &lt;list&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 &lt;dbl [8]&gt; 0.104       0.532\n\nmas_cercano$x[[1]]\n\n[1]  0.30027994  0.36774993 -0.06613864 -0.03673154  0.12260975  0.16718980\n[7] -0.01866598 -0.09308947\n\n\nY el resultado es un desastre. Nuestra predicción es\n\nmas_cercano$y\n\n[1] 0.1038249\n\n\nNecesitariamos una muestra de alrededor de un millón de casos para obtener resultados no tan malos (haz pruebas).\n¿Qué es lo que está pasando? La razón es que en dimensiones altas, los puntos de la muestra de entrenamiento están muy lejos unos de otros, y están cerca de la frontera, incluso para tamaños de muestra relativamente grandes como n = 1000. Cuando la dimensión crece, la situación empeora exponencialmente."
  },
  {
    "objectID": "03-metodos-locales.html#regresión-lineal-en-dimensión-alta",
    "href": "03-metodos-locales.html#regresión-lineal-en-dimensión-alta",
    "title": "3  Métodos locales no estructurados",
    "section": "3.3 Regresión lineal en dimensión alta",
    "text": "3.3 Regresión lineal en dimensión alta\nAhora intentamos algo similar con una función que es razonable aproximar con una función lineal:\n\nfun_cuad &lt;- function(x)  0.5 * (1 + x[1])^2\n\nY queremos predecir para \\(x=(0,0,\\ldots,0)\\), cuyo valor exacto es\n\nfun_cuad(0)\n\n[1] 0.5\n\n\nLos datos se generan de la siguiente forma:\n\nsimular_datos &lt;- function(p = 40){\n    x &lt;- map(1:1000,  ~ runif(p, -1, 1))\n    dat &lt;- tibble(x = x) |&gt; mutate(y = map_dbl(x, fun_cuad)) \n    dat\n}\n\nPor ejemplo para dimensión baja \\(p=1\\) (nótese que una aproximación lineal es razonable):\n\nejemplo &lt;- simular_datos(p = 1) |&gt; mutate(x = unlist(x))\nggplot(ejemplo, aes(x = x, y = y)) + geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAhora repetimos el proceso en dimensión \\(p=40\\): simulamos las entradas, y aplicamos un vecino más cercano\n\nvmc_1 &lt;- function(dat){\n    dat &lt;- dat |&gt; \n        mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |&gt; \n        arrange(dist_origen)\n        mas_cercano &lt;- dat[1, ]\n        mas_cercano$y\n}\nset.seed(834)\ndat &lt;- simular_datos(p = 40)\nvmc_1(dat)\n\n[1] 1.206478\n\n\nEste no es un resultado muy bueno. Sin embargo, regresión se desempeña considerablemente mejor:\n\nregresion_pred &lt;- function(dat){\n    p &lt;- length(dat$x[[1]])\n    dat_reg &lt;- cbind(\n        y = dat$y, \n        x = matrix(unlist(dat$x), ncol = p, byrow=T)) |&gt; \n        as.data.frame()\n    mod_lineal &lt;- lm(y ~ ., dat = dat_reg)\n    origen &lt;- data.frame(matrix(rep(0, p), 1, p))\n    names(origen) &lt;- names(dat_reg)[2:(p+1)]\n    predict(mod_lineal, newdata = origen)\n}\nregresion_pred(dat)\n\n        1 \n0.6677861 \n\n\nLa razón de este mejor desempeño de regresión es que en este caso, el modelo lineal explota la estructura aproximadamente lineal del problema (¿cuál estructura lineal? haz algunas gráficas). Nota: corre este ejemplo varias veces con semilla diferente.\nSolución: vamos a hacer varias simulaciones, para ver qué modelo se desempeña mejor.\n\nsims &lt;- map(1:200, function(i){\n    dat &lt;- simular_datos(p = 40)\n    vmc_y &lt;- vmc_1(dat)\n    reg_y &lt;- regresion_pred(dat)\n    tibble(rep = i, \n           error = c(abs(vmc_y - 0.5), abs(reg_y - 0.5)), \n            tipo = c(\"vmc\", \"regresion\"))\n}) |&gt; bind_rows()\nggplot(sims, aes(x = tipo, y = error)) + geom_boxplot() \n\n\n\n\nAsí que típicamente el error de vecinos más cercanos es más alto que el de regresión. El error esperado es para vmc es más de doble que el de regresión:\n\nsims |&gt; group_by(tipo) |&gt; \n  summarise(media_error = mean(error)) |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      tipo\n      media_error\n    \n  \n  \n    regresion\n0.1662124\n    vmc\n0.3542532\n  \n  \n  \n\n\n\n\nLo que sucede más específicamente es que en regresión lineal utilizamos todos los datos para hacer nuestra estimación en cada predicción. Si la estructura del problema es aproximadamente lineal, entonces regresión lineal explota la estructura para hacer pooling de toda la información para construir predicción con sesgo y varianza bajas. En contraste, vecinos más cercanos sufre de varianza alta.\n\n\n\n\n\n\nMétodos locales sin estructura\n\n\n\nLos métodos locales muchas veces no funcionan bien en dimensión alta. La razón es que:\n\nEl sesgo es alto, pues promediamos puntos muy lejanos al lugar donde queremos predecir (aunque tomemos pocos vecinos cercanos).\nEn el caso de que encontremos unos pocos puntos cercanos, la varianza también puede ser alta porque promediamos relativamente pocos vecinos.\n\nMétodos con más estructura global, apropiada para el problema, logran explotar información de puntos que no están tan cerca del lugar donde queremos predecir.\n\n\nMuchas veces el éxito en la predicción depende de establecer esas estructuras apropiadas ya sea mediante:\n\nEstructura en nuestros modelos (por ejemplo, efectos lineales cuando variables tienen efectos aproximadamente lineales, árboles cuando hay algunas interacciones, redes convolucionales para procesamiento de imágenes y señales, dependencia del contexto en modelos de lenguaje, etc.)\nReducción de dimensionalidad apropiada (por ejemplo, embeddings basados en otros modelos, o técnicas como componentes principales/descompocisión en valores singulares, etc.)."
  },
  {
    "objectID": "04-lineales-ingenieria.html#aprendizaje-de-coeficientes-ajuste",
    "href": "04-lineales-ingenieria.html#aprendizaje-de-coeficientes-ajuste",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.1 Aprendizaje de coeficientes (ajuste)",
    "text": "4.1 Aprendizaje de coeficientes (ajuste)\nEn el ejemplo anterior, los coeficientes fueron calculados (o estimados) usando experiencia, reglas, argumentos teóricos, o quizá otras fuentes de datos (como estudios o encuestas, conteos, etc.)\nAhora quisiéramos construir un algoritmo para aprender estos coeficientes del modelo \\[f_\\beta (x) = \\beta_0 + \\beta_1 x_1 + \\cdots \\beta_p x_p\\] a partir de una muestra de entrenamiento de datos históricos de tiendas que hemos abierto antes: \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\] El criterio de ajuste (algoritmo de aprendizaje) más usual para regresión lineal es el de mínimos cuadrados.\nConstruimos las predicciones (ajustados) para la muestra de entrenamiento: \\[ f_\\beta (x^{(i)}) = \\beta_0 + \\beta_1 x_1^{(i)}+ \\cdots + \\beta_p x_p^{(i)}\\]\nY consideramos las diferencias de los ajustados con los valores observados:\n\\[e^{(i)} = y^{(i)} - f_\\beta (x^{(i)})\\]\nLa idea entonces es minimizar la suma de los residuales al cuadrado, para intentar que la función ajustada pase lo más cercana a los puntos de entrenamiento que sea posible. La función de pérdida que utilizamos más frecuentemente es la pérdida cuadrática, dada por:\n\\[L(\\beta) = \\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\]\n\n\n\n\n\n\nMínimos cuadrados para regresión lineal\n\n\n\nBuscamos encontrar: \\[\\hat{\\beta} = \\mathrm{arg\\,min}_{\\beta} L(\\beta) = \\mathrm{arg\\,min}_{\\beta}\\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] donde \\[f_\\beta (x^{(i)}) = \\beta_0 + \\beta_1 x_1^{(i)}+ \\cdots + \\beta_p x_p^{(i)}\\]\n\n\nHay varias maneras de resolver este problema: puede hacerse analíticamente con álgebra lineal, o con algún método numérico como descenso máximo (que puede escalarse fácilmente). Típicamente la función objetivo es convexa, y la solución es única, excepto en casos degenerados que podremos evitar más adelante usando regularización.\nObservación: Como discutimos al final de la sección anterior, minimizar directamente el error de entrenamiento para encontrar los coeficientes puede resultar en en un modelo sobreajustado/con varianza alta/ruidoso. Hay cuatro grandes estrategias para mitigar este problema: restringir o estructurar la familia de funciones, penalizar la función objetivo, perturbar la muestra de entrenamiento, o cambiar el proceso de minimización perturbando la función objetivo en cada paso o deteniendo el proceso antes de llegar a un mínimo sobreajustado. El método mas común es cambiar la función objetivo, que discutiremos más adelante en la sección de regularización."
  },
  {
    "objectID": "04-lineales-ingenieria.html#ingeniería-de-entradas",
    "href": "04-lineales-ingenieria.html#ingeniería-de-entradas",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.2 Ingeniería de entradas",
    "text": "4.2 Ingeniería de entradas\nAlgunas veces, encontrar la estructura apropiada puede requerir más trabajo que simplemente escoger una familia de modelos. Por ejemplo, en el caso de precios de casa, vimos que podríamos mejorar el ajuste haciendo que el coeficiente de área habitable dependiera de la calidad de los terminados @ref(medicioncostosa).\nUsualmente tendremos que hacer varias transformaciones para obtener buen desempeño de un modelo lineal. En la siguientes secciones mostramos algunas de las más usuales."
  },
  {
    "objectID": "04-lineales-ingenieria.html#variables-categóricas",
    "href": "04-lineales-ingenieria.html#variables-categóricas",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.3 Variables categóricas",
    "text": "4.3 Variables categóricas\nEn primer lugar, podemos incluir variables categóricas creando variables numéricas 0-1 para cada categoría. Por ejemplo para la variable calidad sótano:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(83)\ncasas_split &lt;- initial_split(casas, prop = 0.75)\ncasas_entrena &lt;- training(casas_split)\ncasas_entrena |&gt; count(calidad_sotano)\n\n# A tibble: 5 × 2\n  calidad_sotano     n\n  &lt;chr&gt;          &lt;int&gt;\n1 Ex                89\n2 Fa                23\n3 Gd               457\n4 TA               500\n5 &lt;NA&gt;              26\n\n\nEl mejor nivel es Ex (excelente), luego sigue Gd (bueno), luego Fa (razonable) y finalmente TA (típico)). Hay otro nivel Po (Malo) que no aparece en estos datos.\nEn primer lugar, podemos codificar los valores faltantes, que en este caso indican casas sin sótano:\n\nreceta_na &lt;- recipe(~ calidad_sotano, casas_entrena) |&gt; \n  step_unknown(calidad_sotano, new_level = \"no_sótano\") |&gt; \n  step_relevel(calidad_sotano, ref_level = \"TA\")\ncasas_preproc &lt;- prep(receta_na) |&gt; juice()\ncasas_preproc |&gt; count(calidad_sotano)\n\n# A tibble: 5 × 2\n  calidad_sotano     n\n  &lt;fct&gt;          &lt;int&gt;\n1 TA               500\n2 Ex                89\n3 Fa                23\n4 Gd               457\n5 no_sótano         26\n\n\nAhora convertimos a codificación dummy:\n\nset.seed(7)\nreceta_dummy &lt;- \n  recipe( ~ calidad_sotano, casas_entrena) |&gt; \n  step_unknown(calidad_sotano, new_level = \"no_sótano\") |&gt; \n  step_relevel(calidad_sotano, ref_level = \"TA\") |&gt; \n  step_dummy(calidad_sotano, keep_original_cols = TRUE)\n# preparar receta\nreceta_dummy_prep &lt;- prep(receta_dummy) \n# extrae los datos de entrenamiento preprocesados\nreceta_dummy_prep |&gt; juice() |&gt; \n  sample_n(10) |&gt; gt() |&gt; \n  tab_options(table.font.size = 10)\n\n\n\n\n\n  \n    \n    \n      calidad_sotano\n      calidad_sotano_Ex\n      calidad_sotano_Fa\n      calidad_sotano_Gd\n      calidad_sotano_no_sótano\n    \n  \n  \n    TA\n0\n0\n0\n0\n    Gd\n0\n0\n1\n0\n    Ex\n1\n0\n0\n0\n    Ex\n1\n0\n0\n0\n    TA\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n  \n  \n  \n\n\n\n\nNótese que no hay columna para el nivel TA, que tomamos como referencia. Incluir esta columna sería redundante, pues tenemos una constante en el predictor. En general, cuando una variable categórica tiene \\(k\\) niveles, esta codificación produce \\(k-1\\) columnas binarias.\nVeamos qué pasa cuando preprocesamos datos de prueba (para después poder hacer predicciones):\n\nprueba_casas &lt;- testing(casas_split)\n# supongamos que un nuevo nivel aparece\nprueba_casas$calidad_sotano[1] &lt;- \"no visto antes\"\ndatos &lt;- bake(receta_dummy_prep, prueba_casas)\n\nWarning: There are new levels in a factor: no visto antes\nNew levels will be coerced to `NA` by `step_unknown()`.\nConsider using `step_novel()` before `step_unknown()`.\n\n\nWarning: There are new levels in a factor: NA\n\n\nEn este caso, podemos hacer nuestro flujo más robusto incluyendo un nuevo nivel en los factores donde pondremos casos no vistos. Modificamos nuestra receta:\n\nreceta_dummy &lt;- \n  recipe( ~ calidad_sotano, casas_entrena) |&gt; \n  step_novel(calidad_sotano, new_level = \"nuevo\") |&gt; \n  step_unknown(calidad_sotano, new_level = \"no_sótano\") |&gt; \n  step_relevel(calidad_sotano, ref_level = \"TA\") |&gt; \n  step_dummy(calidad_sotano, keep_original_cols = TRUE)\n# preparar receta\nreceta_dummy_prep &lt;- prep(receta_dummy) \n\n\nprueba_casas &lt;- testing(casas_split)\n# supongamos que un nuevo nivel aparece\nprueba_casas$calidad_sotano[1] &lt;- \"no visto antes\"\ndatos &lt;- bake(receta_dummy_prep, prueba_casas)\ndatos |&gt; head() |&gt; gt() |&gt; tab_options(table.font.size = 10)\n\n\n\n\n\n  \n    \n    \n      calidad_sotano\n      calidad_sotano_Ex\n      calidad_sotano_Fa\n      calidad_sotano_Gd\n      calidad_sotano_nuevo\n      calidad_sotano_no_sótano\n    \n  \n  \n    nuevo\n0\n0\n0\n1\n0\n    Ex\n1\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n0\n    Gd\n0\n0\n1\n0\n0\n    TA\n0\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n0\n  \n  \n  \n\n\n\n\nY podemos ignorar el nuevo nivel al hacer predicciones (que equivale a ponerlo en la categoría de referencia, que en este caso es TA), o podemos lidiar de manera ad-hoc con este nivel.\nOtro problema con el que podemos encontrarnos es variables categóricas que son muy ralas. Por ejemplo, una variable que tiene muchas categorías y algunas de ellas tienen muy pocos datos, además de que es probable que observemos nuevas categorías en el futuro. Por ejemplo, para la variable de zona:\n\ncasas_entrena |&gt; count(nombre_zona) |&gt; \n  arrange(desc(n)) |&gt; gt() |&gt; tab_options(table.font.size = 10)\n\n\n\n\n\n  \n    \n    \n      nombre_zona\n      n\n    \n  \n  \n    NAmes\n163\n    CollgCr\n113\n    OldTown\n80\n    Edwards\n74\n    Somerst\n65\n    Gilbert\n61\n    Sawyer\n59\n    NridgHt\n58\n    NWAmes\n57\n    BrkSide\n45\n    SawyerW\n42\n    Crawfor\n39\n    Mitchel\n36\n    IDOTRR\n31\n    NoRidge\n30\n    Timber\n26\n    ClearCr\n22\n    StoneBr\n20\n    SWISU\n18\n    Blmngtn\n14\n    BrDale\n14\n    MeadowV\n12\n    NPkVill\n8\n    Veenker\n7\n    Blueste\n1\n  \n  \n  \n\n\n\n\nEn este caso, tenemos muchas categorías, algunas con muy pocos datos, y es posible que observemos nuevos datos. Una técnica es agrupar los datos de baja cardinalidad en un nuevo nivel (incluyendo categorías no observadas en entrenamiento):\n\nreceta_vecindario_1 &lt;- \n  recipe( ~ nombre_zona, casas_entrena) |&gt;\n  step_other(nombre_zona, threshold = 0.01, other = \"otras\") \nreceta_vecindario &lt;- receta_vecindario_1 |&gt; \n  step_dummy(nombre_zona)\n# preparar receta\nreceta_vecindario_prep &lt;- prep(receta_vecindario_1)\nset.seed(8231)\nreceta_vecindario_prep |&gt; juice() |&gt; \n  count(nombre_zona) |&gt; arrange(desc(n)) |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      nombre_zona\n      n\n    \n  \n  \n    NAmes\n163\n    CollgCr\n113\n    OldTown\n80\n    Edwards\n74\n    Somerst\n65\n    Gilbert\n61\n    Sawyer\n59\n    NridgHt\n58\n    NWAmes\n57\n    BrkSide\n45\n    SawyerW\n42\n    Crawfor\n39\n    Mitchel\n36\n    IDOTRR\n31\n    NoRidge\n30\n    Timber\n26\n    ClearCr\n22\n    StoneBr\n20\n    SWISU\n18\n    otras\n16\n    Blmngtn\n14\n    BrDale\n14\n    MeadowV\n12\n  \n  \n  \n\n\n\n\nEn este caso, las zonas de baja frecuencia fueron agrupadas en la categoría “otras”. Si observamos un nuevo nivel al momento de predicción:\n\nprueba_casas &lt;- testing(casas_split)\n# supongamos que un nuevo nivel aparece\nprueba_casas$nombre_zona[1] &lt;- \"Xochimilco\"\ndatos &lt;- bake(prep(receta_vecindario), prueba_casas)\ndatos |&gt; head() |&gt;  gt() |&gt; tab_options(table.font.size = 10)\n\n\n\n\n\n  \n    \n    \n      nombre_zona_BrDale\n      nombre_zona_BrkSide\n      nombre_zona_ClearCr\n      nombre_zona_CollgCr\n      nombre_zona_Crawfor\n      nombre_zona_Edwards\n      nombre_zona_Gilbert\n      nombre_zona_IDOTRR\n      nombre_zona_MeadowV\n      nombre_zona_Mitchel\n      nombre_zona_NAmes\n      nombre_zona_NoRidge\n      nombre_zona_NridgHt\n      nombre_zona_NWAmes\n      nombre_zona_OldTown\n      nombre_zona_Sawyer\n      nombre_zona_SawyerW\n      nombre_zona_Somerst\n      nombre_zona_StoneBr\n      nombre_zona_SWISU\n      nombre_zona_Timber\n      nombre_zona_otras\n    \n  \n  \n    0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n    0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n    0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n    0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n    0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n    0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n  \n  \n  \n\n\n\n\nEl proceso general es (ver por ejemplo esta lista):\n\n\n\n\n\n\nVariables categóricas\n\n\n\n\nEstablecemos los niveles que puede tener cada variable, incluyendo la posibilidad de categorías nuevas al momento de predecir, y categorías para valores no disponibles (NAs) (es posible también imputar con algún método en caso necesario).\nReorganizamos factores dependiendo del problema. Por ejemplo, incluir categorías de baja frecuencia en una categoría separada, o manipulaciones ad-hoc dependiendo del problema.\nSustituimos variables categóricas con \\(K\\) niveles en \\(K-1\\) columnas indicadoras de los niveles (estableciendo) alguna categoría como referencia. Esto no es estrictamente necesario en otros métodos, o si utilizamos regularización (ver sección siguiente)."
  },
  {
    "objectID": "04-lineales-ingenieria.html#interacciones",
    "href": "04-lineales-ingenieria.html#interacciones",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.4 Interacciones",
    "text": "4.4 Interacciones\nOtra manera de expandir nuestro modelo es la utilización de interacciones, que muchas veces son clave para tener éxito con modelos lineales. Vimos ejemplos de interacciones en el ejemplo de las casas (@ref(medicioncostosa)) y en el primer ejemplo de ventas de tiendas que dependían del tráfico.\n\nEjemplo\nSi \\(x_1\\) es el área en metros cuadrados de una casa, y \\(x_2\\) una calificación numérica de su calidad, podemos considerar el modelo sin interacciones:\n\\[\\beta_0 + \\beta_1x_1 + \\beta_2 x_2\\]\nPero no tiene mucho sentido que el efecto marginal de \\(x_1\\) sea constante para cualquier nivel de calidad, y tampoco que la calidad de terminados agregue una cantidad fija al precio de la casa sin tomar en cuenta su tamaño. Podemos remediar esto creando una nueva variable que es le producto de \\(x_1\\) y \\(x_2\\):\n\\[x_3 = x_1 x_2\\]\ny agregando, nuestro predictor para precio es\n\\[ \\beta_0 +  \\beta_1x_1 + \\beta_2 x_2 + \\beta_3 x_1x_2\\]\nAhora notemos que para \\(x_2\\) fija, el modelo es\n\\[(\\beta_0 + \\beta_2x_2) + (\\beta_1 + \\beta_3x_2)x_1 = \\gamma_0 + \\gamma_1x_1\\] De modo que es lineal en \\(x_1\\). La diferencia es que cuando cambia \\(x_2\\), la recta que ajustamos es diferente.\n\nbeta &lt;- c(0, 50, 100, 20)\ncombs_tbl &lt;- crossing(x_1 = seq(2, 20, by = 1), x_2 = seq(0, 10, by = 2)) |&gt; \n  mutate(x_3 = x_1 * x_2) |&gt; \n  mutate(pred = beta[1] + beta[2]*x_1 + beta[3]*x_2 + beta[4]*x_3)\nggplot(combs_tbl, aes(x = x_1, y = pred, group = x_2, colour = x_2)) +\n  geom_line()\n\n\n\n\nY vemos que cuando la calidad es baja, el precio por metro cuadrado es más bajo que cuando la calidad es alta. Otra manera de pensar esto es que la inclusión de la interacción produce curvas marginales que rotan dependiendo del valor de otras variables.\nPregunta. ¿puedes pensar en otros casos donde las interacciones deben jugar un papel importante?\n\n\n\n\n\n\nInteracciones\n\n\n\n\nTransformamos las variables categóricas a dummies. Transformamos las variables numéricas si es necesario (normalizar, aplicar transformación no lineal, etc.)\nIncluimos interacciones de la siguiente forma:\n\n\nPara la interaccion de dos variables numéricas \\(x_1\\) y \\(x_2\\) agregamos el producto \\(x_3 = x_1x_2\\).\nPara interacción de una variable categórica \\(g\\) con una numérica \\(x\\) podemos hacer el mismo procedimiento multiplicando la variable categórica por cada una de las variable dummy que creamos a partir de \\(g\\). Esto en efecto produce una pendiente para \\(x\\) dependiendo del valor que toma \\(g\\)."
  },
  {
    "objectID": "04-lineales-ingenieria.html#ejemplo-precios-de-casas",
    "href": "04-lineales-ingenieria.html#ejemplo-precios-de-casas",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.5 Ejemplo: precios de casas",
    "text": "4.5 Ejemplo: precios de casas\nEn el ejemplo de precios de casas, por ejemplo, es claro que el efecto en ventas del tamaño de las áreas (habitable, garage, etc.) depende de la calidad de los terminados, como vimos en la introducción. En la siguiente receta de preprocesamiento:\n\nCortamos calidad general en 5 grupos: este paso no es necesario y puede dañar el desempeño, pero es consistente con el análisis que hicimos anteriormente.\nLidiamos con niveles nuevos y los ponemos en una categoría “nuevo” (para que nuestro modelo no falle al momento de predicción)\nPonemos los faltantes de calidad sotano y garage en una categoría nueva (no tienen sótano y/o garage)\nAgrupamos las zonas con pocas observaciones en una categoría de “Otros”\nQuitamos los NA’s de área garage y área sotano, que deben ser igual a 0 cuando no existen estas características.\nCreamos variables dummy de todas las variables categóricas\nIncluimos interacciones de distintas áreas con las dummy correspondientes, incluyendo zona con área habitable\nFinalmente, eliminamos para el ajuste aquellas variables que tengan varianza cercana a cero (500 /1 quiere decir que elimina cualquier variable cuyo conteo del valor más común entre el conteo de la siguiente es mayor a 500).\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(83)\ncasas_split &lt;- initial_split(casas, prop = 0.75)\ncasas_entrena &lt;- training(casas_split)\nreceta_casas &lt;- recipe(precio_miles ~ \n           nombre_zona + \n           area_hab_m2 + area_garage_m2 + area_sotano_m2 + \n           area_lote_m2 + \n           año_construccion + \n           calidad_gral + calidad_garage + calidad_sotano + \n           num_coches  + \n           aire_acondicionado + condicion_venta, \n           data = casas_entrena) |&gt; \n  step_filter(condicion_venta == \"Normal\") |&gt; \n  step_select(-condicion_venta, skip = TRUE) |&gt; \n  step_cut(calidad_gral, breaks = c(3, 5, 7, 8), \n           include_outside_range = TRUE) |&gt;\n  step_novel(nombre_zona, calidad_sotano, calidad_garage) |&gt; \n  step_unknown(calidad_sotano, calidad_garage) |&gt; \n  step_other(nombre_zona, threshold = 0.02, other = \"otras\") |&gt; \n  step_mutate(area_sotano_m2 = ifelse(is.na(area_sotano_m2), 0, area_sotano_m2)) |&gt; \n  step_mutate(area_garage_m2 = ifelse(is.na(area_garage_m2), 0, area_garage_m2)) |&gt; \n  step_dummy(nombre_zona, calidad_gral, calidad_garage, calidad_sotano, aire_acondicionado) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"nombre_zona\")) |&gt; \n  step_interact(terms = ~ area_garage_m2:starts_with(\"calidad_garage\")) |&gt; \n  step_interact(terms = ~ area_sotano_m2: starts_with(\"calidad_sotano\")) |&gt; \n  step_nzv(all_predictors(), freq_cut = 500 / 1, unique_cut = 1)\n\nEntrenamos la receta y vemos cuántos casos y columnas tenemos:\n\nreceta_casas_prep &lt;- prep(receta_casas, verbose = TRUE)\n\noper 1 step filter [training] \noper 2 step select [training] \noper 3 step cut [training] \noper 4 step novel [training] \noper 5 step unknown [training] \noper 6 step other [training] \noper 7 step mutate [training] \noper 8 step mutate [training] \noper 9 step dummy [training] \noper 10 step interact [training] \noper 11 step interact [training] \noper 12 step interact [training] \noper 13 step interact [training] \noper 14 step nzv [training] \nThe retained training set is ~ 0.46 Mb  in memory.\n\ndatos_tbl &lt;- juice(receta_casas_prep)\ndim(datos_tbl)\n\n[1] 907  62\n\n\n\ndatos_tbl |&gt;\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  head() |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      area_hab_m2\n      area_garage_m2\n      area_sotano_m2\n      area_lote_m2\n      año_construccion\n      num_coches\n      precio_miles\n      nombre_zona_CollgCr\n      nombre_zona_Crawfor\n      nombre_zona_Edwards\n      nombre_zona_Gilbert\n      nombre_zona_IDOTRR\n      nombre_zona_Mitchel\n      nombre_zona_NAmes\n      nombre_zona_NoRidge\n      nombre_zona_NridgHt\n      nombre_zona_NWAmes\n      nombre_zona_OldTown\n      nombre_zona_Sawyer\n      nombre_zona_SawyerW\n      nombre_zona_Somerst\n      nombre_zona_Timber\n      nombre_zona_otras\n      calidad_gral_X.3.5.\n      calidad_gral_X.5.7.\n      calidad_gral_X.7.8.\n      calidad_gral_X.8.max.\n      calidad_garage_Fa\n      calidad_garage_Gd\n      calidad_garage_TA\n      calidad_garage_unknown\n      calidad_sotano_Fa\n      calidad_sotano_Gd\n      calidad_sotano_TA\n      calidad_sotano_unknown\n      aire_acondicionado_Y\n      area_hab_m2_x_calidad_gral_X.3.5.\n      area_hab_m2_x_calidad_gral_X.5.7.\n      area_hab_m2_x_calidad_gral_X.7.8.\n      area_hab_m2_x_calidad_gral_X.8.max.\n      area_hab_m2_x_nombre_zona_CollgCr\n      area_hab_m2_x_nombre_zona_Crawfor\n      area_hab_m2_x_nombre_zona_Edwards\n      area_hab_m2_x_nombre_zona_Gilbert\n      area_hab_m2_x_nombre_zona_IDOTRR\n      area_hab_m2_x_nombre_zona_Mitchel\n      area_hab_m2_x_nombre_zona_NAmes\n      area_hab_m2_x_nombre_zona_NoRidge\n      area_hab_m2_x_nombre_zona_NridgHt\n      area_hab_m2_x_nombre_zona_NWAmes\n      area_hab_m2_x_nombre_zona_OldTown\n      area_hab_m2_x_nombre_zona_Sawyer\n      area_hab_m2_x_nombre_zona_SawyerW\n      area_hab_m2_x_nombre_zona_Somerst\n      area_hab_m2_x_nombre_zona_Timber\n      area_hab_m2_x_nombre_zona_otras\n      area_garage_m2_x_calidad_garage_Fa\n      area_garage_m2_x_calidad_garage_Gd\n      area_garage_m2_x_calidad_garage_TA\n      area_sotano_m2_x_calidad_sotano_Fa\n      area_sotano_m2_x_calidad_sotano_Gd\n      area_sotano_m2_x_calidad_sotano_TA\n    \n  \n  \n    137.22\n20.07\n79.99\n584.55\n1928\n1\n145.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0.00\n137.22\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n137.22\n0\n0\n20.07\n0\n0.00\n79.99\n    179.86\n46.82\n130.62\n1039.96\n2000\n2\n230.5\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0.00\n0.00\n179.86\n0\n0.00\n0\n0\n179.86\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0.00\n0\n0\n46.82\n0\n130.62\n0.00\n    81.94\n27.31\n0.00\n774.72\n1959\n1\n106.5\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n81.94\n0.00\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n81.94\n0\n0\n0\n0\n0\n0\n0\n0\n0.00\n0\n0\n27.31\n0\n0.00\n0.00\n    153.01\n20.07\n74.88\n637.13\n1915\n1\n128.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0.00\n153.01\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n153.01\n0\n0\n20.07\n0\n74.88\n0.00\n    101.45\n26.57\n50.73\n205.97\n1970\n1\n88.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n101.45\n0.00\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n101.45\n0\n0\n26.57\n0\n0.00\n50.73\n    71.35\n36.79\n71.35\n668.90\n1972\n1\n133.9\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n71.35\n0.00\n0.00\n0\n71.35\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0.00\n0\n0\n36.79\n0\n71.35\n0.00\n  \n  \n  \n\n\n\n\nFinalmente, usamos un modelo lineal con las 62 entradas que acabamos de crear:\n\nflujo_casas &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(linear_reg() |&gt; set_engine(\"lm\"))\najuste &lt;- fit(flujo_casas, casas_entrena)\n\nAunque no es de interés particular para nosotros por el momento, examinamos los coeficientes (que no son tan simples de interpretar como discutiremos más adelante):\n\najuste |&gt; broom::tidy() |&gt; \n  mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt; \n  select(term, estimate) |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n    \n  \n  \n    (Intercept)\n-858.68\n    area_hab_m2\n0.80\n    area_garage_m2\n1.02\n    area_sotano_m2\n0.88\n    area_lote_m2\n0.01\n    año_construccion\n0.39\n    num_coches\n2.11\n    nombre_zona_CollgCr\n16.70\n    nombre_zona_Crawfor\n37.11\n    nombre_zona_Edwards\n28.82\n    nombre_zona_Gilbert\n31.31\n    nombre_zona_IDOTRR\n11.81\n    nombre_zona_Mitchel\n32.01\n    nombre_zona_NAmes\n28.74\n    nombre_zona_NoRidge\n22.00\n    nombre_zona_NridgHt\n-6.51\n    nombre_zona_NWAmes\n17.67\n    nombre_zona_OldTown\n25.28\n    nombre_zona_Sawyer\n37.91\n    nombre_zona_SawyerW\n-11.95\n    nombre_zona_Somerst\n-8.99\n    nombre_zona_Timber\n3.23\n    nombre_zona_otras\n-14.27\n    calidad_gral_X.3.5.\n35.54\n    calidad_gral_X.5.7.\n13.76\n    calidad_gral_X.7.8.\n16.99\n    calidad_gral_X.8.max.\n-67.26\n    calidad_garage_Fa\n15.54\n    calidad_garage_Gd\n39.04\n    calidad_garage_TA\n18.71\n    calidad_garage_unknown\n20.61\n    calidad_sotano_Fa\n72.92\n    calidad_sotano_Gd\n54.95\n    calidad_sotano_TA\n60.56\n    calidad_sotano_unknown\n59.88\n    aire_acondicionado_Y\n15.17\n    area_hab_m2_x_calidad_gral_X.3.5.\n-0.25\n    area_hab_m2_x_calidad_gral_X.5.7.\n0.05\n    area_hab_m2_x_calidad_gral_X.7.8.\n0.19\n    area_hab_m2_x_calidad_gral_X.8.max.\n0.81\n    area_hab_m2_x_nombre_zona_CollgCr\n-0.25\n    area_hab_m2_x_nombre_zona_Crawfor\n-0.14\n    area_hab_m2_x_nombre_zona_Edwards\n-0.39\n    area_hab_m2_x_nombre_zona_Gilbert\n-0.33\n    area_hab_m2_x_nombre_zona_IDOTRR\n-0.21\n    area_hab_m2_x_nombre_zona_Mitchel\n-0.42\n    area_hab_m2_x_nombre_zona_NAmes\n-0.31\n    area_hab_m2_x_nombre_zona_NoRidge\n-0.16\n    area_hab_m2_x_nombre_zona_NridgHt\n-0.03\n    area_hab_m2_x_nombre_zona_NWAmes\n-0.26\n    area_hab_m2_x_nombre_zona_OldTown\n-0.32\n    area_hab_m2_x_nombre_zona_Sawyer\n-0.43\n    area_hab_m2_x_nombre_zona_SawyerW\n-0.07\n    area_hab_m2_x_nombre_zona_Somerst\n0.00\n    area_hab_m2_x_nombre_zona_Timber\n-0.14\n    area_hab_m2_x_nombre_zona_otras\n0.00\n    area_garage_m2_x_calidad_garage_Fa\n-0.59\n    area_garage_m2_x_calidad_garage_Gd\n-0.97\n    area_garage_m2_x_calidad_garage_TA\n-0.78\n    area_sotano_m2_x_calidad_sotano_Fa\n-0.86\n    area_sotano_m2_x_calidad_sotano_Gd\n-0.53\n    area_sotano_m2_x_calidad_sotano_TA\n-0.67\n  \n  \n  \n\n\n\n\nNótese que:\n\nEn esta tabla están los coeficientes \\(\\beta_i\\) en las covariables que creamos a partir de las variables de entrada.\nEl modelo lineal no tiene que ser lineal en las variables que recibimos originalmente en la tabla de datos.\nEn este ejemplo, convertimos algunas variables a dummy, y multiplicamos algunas variables de área por esas variables dummy.\n\nFinalmente, evaluamos el desempeño sobre las ventas normales:\n\nmetricas &lt;- metric_set(mape, mae, rmse, rsq)\ncasas_prueba_normal &lt;- testing(casas_split) |&gt; \n  filter(condicion_venta == \"Normal\")\nmetricas(casas_prueba_normal |&gt; bind_cols(predict(ajuste, casas_prueba_normal)), \n     truth = precio_miles, estimate = .pred) |&gt; \n  gt() |&gt; fmt_number(.estimate, decimals = 2)\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    mape\nstandard\n10.34\n    mae\nstandard\n17.12\n    rmse\nstandard\n23.49\n    rsq\nstandard\n0.89"
  },
  {
    "objectID": "04-lineales-ingenieria.html#no-linealidad-y-atípicos",
    "href": "04-lineales-ingenieria.html#no-linealidad-y-atípicos",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.6 No linealidad y atípicos",
    "text": "4.6 No linealidad y atípicos\nEn un primer ejemplo consideremos la variable de área de lote:\n\nlibrary(patchwork)\ng_1 &lt;- ggplot(casas_entrena, aes(x = area_lote_m2, y = precio_miles)) +\n  geom_point() + #geom_smooth(method = \"loess\", span = 0.5, se = FALSE,\n                #             method.args = list(degree = 1)) +\n  geom_smooth(method = \"lm\", se = FALSE)\ng_2 &lt;- ggplot(casas_entrena |&gt; filter(area_lote_m2 &lt; 5000), aes(x = area_lote_m2, y = precio_miles)) +\n  geom_point(data = casas_entrena, aes(colour = area_lote_m2 &gt; 5000)) + \n  #geom_smooth(method = \"loess\", span = 0.5, se = FALSE,\n  #                           method.args = list(degree = 1)) +\n  geom_smooth(method = \"lm\", se = FALSE)\ng_1 + g_2\n\n\n\n\nY notamos que hay algunos valores grandes que pueden perturbar el ajuste lineal. Esto puede producir varianza alta en las predicciones, pues el ajuste depende mucho de unos cuantos valores de entrenamiento. Una solución puede ser transformar la entrada por ejempo usando el logaritmo, que comprime la cola derecha de la distribución de la variable que tiene mucho sesgo:\n\nggplot(casas_entrena, aes(x = area_lote_m2, y = precio_miles)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", span = 0.5, se = FALSE) +\n  scale_x_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nNótese que probablemente tendremos que agregar más flexibilidad en nuestro predictor para capturar apropiadamente la información en esta variable."
  },
  {
    "objectID": "04-lineales-ingenieria.html#no-linealidad-y-splines",
    "href": "04-lineales-ingenieria.html#no-linealidad-y-splines",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.7 No linealidad y splines",
    "text": "4.7 No linealidad y splines\nEn algunos casos, la relación de una variable de entrada con la predicción es no lineal. Podemos entonces incluír entradas derivadas de la original usando transformaciones no lineales: por ejemplo, transformar entradas usando el logaritmo, o agregar el cuadrado o la raíz de las variables de entrada.\nUna de las maneras más simples y menos problemáticas de hacer esto es usando splines naturales para modelar, que son funciones cúbicas por tramos dos veces diferenciables. Los tramos están definidos por nudos que podemos definir por ejemplo igualmente espaciados en los datos.\n\nvalores_x &lt;- seq(-10, 110, 1)\nbase_splines &lt;- splines::ns(x = valores_x, \n  knots = c(33, 66), Boundary.knots = c(0, 100))\nspline_1 &lt;- base_splines %*% c(1, 1, 1)\nspline_2 &lt;- base_splines %*% c(-0.1, 2, 1)\ntibble(x = valores_x, y = spline_1, spline = 1) |&gt; \nbind_rows(tibble(x = valores_x, y = spline_2, spline = 2)) |&gt; \n  ggplot(aes(x = x, y = y, \n    group = spline, colour = factor(spline))) + \n  geom_point() + geom_line() +\n  geom_vline(xintercept = c(0, 33, 66, 100), \n    colour = \"red\")\n\n\n\n\nEstos dos son ejemplos de funciones cúbicas por tramos y dos veces diferenciables, con nudos en 0, 33, 66 y 100. Su forma particular depende de tres coeficientes, que pueden pensarse también como definidos por dónde tienen que pasar la curva en \\(y\\) para los valores \\(x = 33, 66\\) y \\(100\\). Extrapolan linealmente fuera del rango de los datos.\nLa ventaja de utilizar estos splines es que son estables en el cálculo, pues a lo más utilizan potencias cúbicas, y la complejidad puede aumentarse incrementando el número de nodos.\n\nEjemplo\nRevisamos nuestro ejemplo de rendimiento de coches:\n\nlibrary(tidyverse)\nlibrary(gt)\nauto &lt;- read_csv(\"../datos/auto.csv\")\ndatos &lt;- auto[, c('name', 'weight','year', 'mpg', 'displacement')]\ndatos &lt;- datos |&gt; mutate(\n  peso_kg = weight * 0.45359237,\n  rendimiento_kpl = mpg * (1.609344 / 3.78541178), \n  año = year)\n\nVamos a separar en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (75% para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba):\n\nlibrary(tidymodels)\nset.seed(121)\ndatos_split &lt;- initial_split(datos, prop = 0.75)\ndatos_entrena &lt;- training(datos_split)\ndatos_prueba &lt;- testing(datos_split)\n\nVamos a usar año y peso de los coches para predecir su rendimiento:\n\nggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl, colour = año)) +\n  geom_point()\n\n\n\n\nNuestra receta incluye la transformación no lineal de splines:\n\nreceta_lineal &lt;- recipe(rendimiento_kpl ~ peso_kg + año, datos_entrena) |&gt; \n  step_ns(peso_kg, deg_free = 3) |&gt; \n  step_ns(año, deg_free = 2)\nmod_lineal &lt;- linear_reg() |&gt;  \n  set_engine(\"lm\")  \nflujo &lt;- workflow() |&gt;  \n  add_recipe(receta_lineal) |&gt; \n  add_model(mod_lineal)\n\nLos datos de entrada son los siguientes:\n\njuice(prep(receta_lineal)) |&gt; head() |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      rendimiento_kpl\n      peso_kg_ns_1\n      peso_kg_ns_2\n      peso_kg_ns_3\n      año_ns_1\n      año_ns_2\n    \n  \n  \n    12.754311\n-0.1384501\n0.3919536\n-0.2338715\n0.1263158\n-0.08343893\n    13.136941\n-0.1508427\n0.4978146\n-0.2970367\n0.5652102\n0.00590925\n    12.754311\n0.3794499\n0.4646633\n-0.2232209\n0.4765544\n0.35513660\n    7.695101\n0.4471996\n0.4245669\n-0.1625354\n0.5652102\n0.00590925\n    8.757960\n0.4368600\n0.4313780\n-0.1743974\n0.5652102\n0.00590925\n    14.242314\n-0.1126420\n0.2985773\n-0.1781555\n0.5794245\n-0.12316573\n  \n  \n  \n\n\n\n\nNótese que tenemos 4 entradas en lugar de las 2 originales, pues creamos dos transformaciones no lineales de peso_kg. El modelo es lineal en estas 4 variables, pero no en las 2 originales. Ajustamos:\n\nflujo_ajustado &lt;- fit(flujo, datos_entrena)\n\nY ahora podemos graficar los resultados y vemos cómo pudimos capturar la relación no lineal entre peso y rendimiento:\n\ndat_graf &lt;- tibble(peso_kg = seq(900, 2200, by = 10)) |&gt;   \n  crossing(tibble(año = c(70, 75, 80)))\ndat_graf &lt;- dat_graf |&gt; \n  mutate(pred_1 = predict(flujo_ajustado, dat_graf) |&gt; pull(.pred))\nggplot(datos_entrena, aes(x = peso_kg, group = año, colour = año)) +\n  geom_point(aes(y = rendimiento_kpl), alpha = 0.6) + \n  geom_line(data = dat_graf, aes(y = pred_1),  size = 1.2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nLos grados de libertad también pueden afinarse utilizando un conjunto de validación como hicimos antes en vecinos más cercanos.\n\n\nEjemplo: casas\nPor ejemplo, podríamos incluir un efecto no lineal de area_lote, calidad y condición general y año de construcción:\n\nreceta_casas &lt;- recipe(precio_miles ~ \n           nombre_zona + \n           area_hab_m2 + area_garage_m2 + area_sotano_m2 + \n           area_lote_m2 + \n           año_construccion + \n           calidad_gral + calidad_garage + calidad_sotano + \n           condicion_gral + \n           num_coches  + \n           aire_acondicionado + condicion_venta, \n           data = casas_entrena) |&gt; \n  step_filter(condicion_venta == \"Normal\") |&gt; \n  step_select(-condicion_venta, skip = TRUE) |&gt; \n  \n  step_novel(nombre_zona, calidad_sotano, calidad_garage) |&gt; \n  step_unknown(calidad_sotano, calidad_garage) |&gt; \n  step_other(nombre_zona, threshold = 0.02, other = \"otras\") |&gt; \n  step_mutate(area_sotano_m2 = \n    ifelse(is.na(area_sotano_m2), 0, area_sotano_m2)) |&gt; \n  step_mutate(area_garage_m2 = \n    ifelse(is.na(area_garage_m2), 0, area_garage_m2)) |&gt;\n # step_log(area_lote_m2) |&gt; \n  step_ns(año_construccion, deg_free = 2) |&gt; \n  step_ns(calidad_gral, deg_free = 2) |&gt; \n  step_ns(condicion_gral, deg_free = 2) |&gt; \n  step_ns(area_lote_m2, deg_free = 3) |&gt; \n  step_dummy(nombre_zona,  calidad_garage, calidad_sotano, aire_acondicionado) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"nombre_zona\")) |&gt; \n  step_interact(terms = ~ area_garage_m2:starts_with(\"calidad_garage\")) |&gt; \n  step_interact(terms = ~ area_sotano_m2: starts_with(\"calidad_sotano\")) |&gt; \n  step_nzv(all_predictors(), freq_cut = 500 / 1, unique_cut = 1)\n\n\nflujo_casas &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(linear_reg() |&gt; set_engine(\"lm\"))\najuste &lt;- fit(flujo_casas, casas_entrena)\n\nFinalmente, evaluamos el desempeño sobre las ventas normales, y obtenemos una mejoría con respecto a nuestro modelo anterior:\n\nmetricas &lt;- metric_set(mape, mae, rmse, rsq)\ncasas_prueba_normal &lt;- testing(casas_split) |&gt; \n  filter(condicion_venta == \"Normal\")\nmetricas(casas_prueba_normal |&gt; \n  bind_cols(predict(ajuste, casas_prueba_normal)), \n     truth = precio_miles, estimate = .pred) |&gt; \n  gt() |&gt; fmt_number(.estimate, decimals = 2)\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    mape\nstandard\n8.47\n    mae\nstandard\n14.45\n    rmse\nstandard\n19.77\n    rsq\nstandard\n0.92\n  \n  \n  \n\n\n\n\nFinalmente, examinamos la respuesta contra la predicción:\n\ncasas_prueba_normal |&gt; \n  bind_cols(predict(ajuste, casas_prueba_normal)) |&gt; \n  ggplot(aes(x = .pred, y = precio_miles)) + geom_abline() + \n    geom_point(colour = \"red\") + coord_obs_pred()"
  },
  {
    "objectID": "05-regularizacion-1.html#ejemplo-datos-simulados-y-varianza",
    "href": "05-regularizacion-1.html#ejemplo-datos-simulados-y-varianza",
    "title": "5  Regularización y variabilidad",
    "section": "5.1 Ejemplo: datos simulados y varianza",
    "text": "5.1 Ejemplo: datos simulados y varianza\nConsideremos un problema donde tenemos unas 100 entradas con 120 casos. Supondremos que la función verdadera es\n\\[f(x) = \\sum_{j=1}^{100} \\beta_j x_j\\]\n\nlibrary(tidyverse)\nlibrary(gt)\nset.seed(28015)\nbeta_vec &lt;- rnorm(100, 0, 0.2)\np &lt;- length(beta_vec)\nbeta &lt;- tibble(term = str_c('V', 1:p), valor = beta_vec)\nhead(beta)\n\n# A tibble: 6 × 2\n  term     valor\n  &lt;chr&gt;    &lt;dbl&gt;\n1 V1    -0.121  \n2 V2     0.0374 \n3 V3    -0.129  \n4 V4     0.240  \n5 V5    -0.00962\n6 V6    -0.0443 \n\n\nSimulamos datos:\n\nsim_datos &lt;- function(n, beta){\n  p &lt;- nrow(beta)\n  mat_x &lt;- matrix(rnorm(n * p, 0, 1), n, p) + rnorm(n, 0, 5) \n  colnames(mat_x) &lt;- beta |&gt; pull(term)\n  beta_vec &lt;- beta |&gt; pull(valor)\n  f_x &lt;- mat_x %*% beta_vec \n  y &lt;- as.numeric(f_x) + rnorm(n, 0, 1)\n  datos &lt;- as_tibble(mat_x) \n  datos |&gt; mutate(y = y)\n}\ndatos &lt;- sim_datos(n = 4000, beta = beta)\n\nSeparamos datos de entrenamiento y prueba y definimos y ajustamos un predictor lineal:\n\nlibrary(tidymodels)\nset.seed(994)\nn_entrena &lt;- nrow(datos) * 0.03\nseparacion &lt;- initial_split(datos, 0.03)\ndat_ent &lt;- training(separacion)\nmodelo &lt;-  linear_reg() |&gt; set_engine(\"lm\")\nreceta &lt;- recipe(y ~ ., dat_ent)\nflujo &lt;- workflow() |&gt; \n  add_model(modelo) |&gt; \n  add_recipe(receta)\nflujo_ajustado &lt;- fit(flujo, dat_ent)\nmod_1  &lt;- flujo_ajustado |&gt; extract_fit_engine()\n\nExtraemos los coeficientes y graficamos ajustados contra verdaderos:\n\ncoefs_1 &lt;- tidy(mod_1) |&gt; \n  left_join(beta, by = \"term\")\nggplot(coefs_1 |&gt; filter(term != \"(Intercept)\"), \n       aes(x = valor, y = estimate)) +\n  geom_point() +\n  xlab('Coeficientes verdaderos') + \n  ylab('Coeficientes estimados') +\n  geom_abline() \n\n\n\n\nY notamos que las estimaciones no son buenas. Podemos hacer otra simulación para confirmar que el problema es que las estimaciones son muy variables.\nCon otra muestra de entrenamiento, vemos que las estimaciones tienen varianza alta.\n\ndatos_ent_2 &lt;- sim_datos(n = 120, beta = beta)\nmod_2 &lt;- fit(flujo, datos_ent_2) |&gt; extract_fit_engine()\ncoefs_2 &lt;- tidy(mod_2)\nqplot(coefs_1$estimate, coefs_2$estimate) + xlab('Coeficientes mod 1') + \n  ylab('Coeficientes mod 2') +\n  geom_abline(intercept=0, slope =1)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nEn la práctica, nosotros tenemos una sola muestra de entrenamiento. Así que, con una muestra de tamaño\n\\(n=120\\) como en este ejemplo, obtendremos típicamente resultados no muy buenos. Estos coeficientes ruidosos afectan nuestras predicciones de manera negativa, aún cuando el modelo ajustado parece reproducir razonablemente bien la variable respuesta:\n\nlibrary(patchwork)\ndat_pr &lt;- testing(separacion)\npreds_entrena &lt;- predict(flujo_ajustado, dat_ent) |&gt; \n  bind_cols(dat_ent |&gt; select(y))\npreds_prueba &lt;- predict(flujo_ajustado, dat_pr) |&gt; \n  bind_cols(dat_pr |&gt; select(y))\ng_1 &lt;- ggplot(preds_entrena, aes(x = .pred, y = y)) +\n  geom_abline(colour = \"red\") +\n  geom_point() + \n  xlab(\"Predicción\") + ylab(\"y\") +\n  labs(subtitle = \"Muestra de entrenamiento\")\ng_2 &lt;- ggplot(preds_prueba, aes(x = .pred, y = y)) + \n  geom_abline(colour = \"red\") +\n  geom_point() + \n  xlab(\"Predicción\") + ylab(\"y\") +\n  labs(subtitle = \"Muestra de prueba\")\ng_1 + g_2"
  },
  {
    "objectID": "05-regularizacion-1.html#ejemplo-controlando-la-varianza",
    "href": "05-regularizacion-1.html#ejemplo-controlando-la-varianza",
    "title": "5  Regularización y variabilidad",
    "section": "5.2 Ejemplo: controlando la varianza",
    "text": "5.2 Ejemplo: controlando la varianza\nComo el problema es la variabilidad de los coeficientes (en este ejemplo sabemos que no hay sesgo pues conocemos el modelo verdadero), podemos atacar este problema poniendo restricciones a los coeficientes, de manera que caigan en rangos más aceptables.\nUna manera de hacer esto es restringir el rango de los coeficientes cambiando la función que minimizamos para ajustar el modelo lineal. Recordamos que la cantidad que queremos minimizar es\n\\[D(\\beta) = D(a_0, \\beta_1, \\ldots, \\beta_p) = \\sum_{i=1}^N (y^{(i)} - f_\\beta (x^{(i)}))^2 = \\sum_{i=1}^N (y^{(i)} - \\beta_0 - \\beta_1 x_1^{(i)}-\\beta_2x_2^{(i)} - \\cdots - \\beta_px_p^{(i)})^2\\]\ndonde la suma es sobre los datos de entrenamiento. Queremos encontrar \\(a =(\\beta_0, \\beta_1, \\ldots, \\beta_p)\\) para resolver\n\\[\\min_\\beta D(\\beta)\\]\nEn el ejemplo que estamos considerando, vemos que existe mucha variación en los coeficientes obtenidos de muestra de entrenamiento a muestra de entrenamiento, y que algunos de ellos toman valores muy grandes positivos o negativos. Podemos entonces intentar resolver mejor el problema penalizado\n\\[\\min_\\beta D(\\beta) + \\lambda \\sum_{j=1}^p \\beta_j^2\\]\nSi escogemos un valor relativamente grande de \\(\\lambda &gt; 0\\), entonces terminaremos con una solución donde los coeficientes\nno pueden alejarse mucho de 0, y esto previene parte del sobreajuste que observamos en nuestro primer ajuste. Otra manera de decir esto es: intentamos minimizar cuadrados, pero no permitimos que los coeficientes se alejen demasiado de cero, o ponemos un costo a soluciones que intentan “mover” mucho los coeficientes para ajustar mejor al conjunto de entrenamiento.\n\nNormalmente normalizamos las variables de entrada \\(x\\) para que tenga sentido normalizar todos los coeficientes con una misma \\(\\lambda\\).\nTambién es posible poner restricciones sobre el tamaño de \\(\\sum_j \\beta_j^2\\), lo cual es equivalente al problema de penalización.\nUsualmente no penalizamos la constante \\(\\beta_0\\), de forma que si \\(\\lambda\\) es muy grande, nuestro modelo ajustado predice simplemente la media de los datos de entrenamiento.\nEste tipo de penalización se llama muchas veces \\(L_2\\), o penalización ridge.\n\nEn este caso obtenemos:\n\nmodelo_reg &lt;-  linear_reg(mixture = 0, penalty = 0.1) |&gt;\n  set_engine(\"glmnet\", lambda.min.ratio = 0)\nflujo_reg &lt;- workflow() |&gt; \n  add_model(modelo_reg) |&gt; \n  add_recipe(receta)\nflujo_reg &lt;- fit(flujo_reg, dat_ent)\nmod_reg  &lt;- flujo_reg |&gt; extract_fit_parsnip()\n\nLos coeficientes del modelo penalizado son:\n\ncoefs_penalizado &lt;- tidy(mod_reg) \n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-7\n\ncoefs_penalizado\n\n# A tibble: 101 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  -0.236      0.1\n 2 V1           -0.0774     0.1\n 3 V2            0.0340     0.1\n 4 V3           -0.0405     0.1\n 5 V4            0.151      0.1\n 6 V5            0.150      0.1\n 7 V6            0.114      0.1\n 8 V7           -0.207      0.1\n 9 V8            0.0148     0.1\n10 V9            0.191      0.1\n# ℹ 91 more rows\n\n\nNótese que efectivamente la suma de cuadrados de los coeficientes penalizados es considerablemente más chica que las del modelo no penalizado:\n\nsum(coefs_penalizado$estimate[-1]^2)\n\n[1] 1.957447\n\n\n\nsum(coefs_1$estimate[-1]^2)\n\n[1] 12.59754\n\n\nLos nuevos coeficientes estimados tienen menor variación, y están más cercanos a los valores reales:\n\nqplot(coefs_1$valor[-1], coefs_penalizado$estimate[-1]) + \n  xlab('Coeficientes') + \n  ylab('Coeficientes estimados') +\n  geom_abline()\n\n\n\n\n\npreds_prueba_2 &lt;- predict(mod_reg, dat_pr) |&gt; \n  bind_cols(dat_pr |&gt; select(y))\npreds_prueba_ambas &lt;- bind_rows(\n          preds_prueba |&gt; mutate(tipo = \"sin penalizar\"),\n          preds_prueba_2 |&gt; mutate(tipo = \"penalizado\"))\nggplot(preds_prueba_ambas, aes(x = y, y = .pred)) +\n  geom_abline(colour = \"red\") +\n  geom_point(alpha = 0.3) + \n  xlab(\"Predicción\") + ylab(\"y\") +\n  facet_wrap(~ tipo, nrow = 1) + \n  labs(subtitle = \"Muestra de prueba\") \n\n\n\n\n\nmetricas &lt;- metric_set(mae, rmse)\nres_1 &lt;- metricas(preds_prueba, truth = y, estimate = .pred) |&gt; \n  mutate(tipo = \"no penalizado\")\nres_2 &lt;- metricas(preds_prueba_2, truth = y, estimate = .pred) |&gt; \n  mutate(tipo = \"penalizado\")\nbind_rows(res_1, res_2) |&gt;\n  arrange(.metric) |&gt; \n  gt() |&gt; fmt_number(.estimate, decimals = 2)\n\n\n\n\n\n  \n    \n    \n      .metric\n      .estimator\n      .estimate\n      tipo\n    \n  \n  \n    mae\nstandard\n2.34\nno penalizado\n    mae\nstandard\n1.27\npenalizado\n    rmse\nstandard\n2.95\nno penalizado\n    rmse\nstandard\n1.60\npenalizado\n  \n  \n  \n\n\n\n\nY vemos que los errores de predicción se reducen considerablemente.\n\nObsérvese que esta mejora en varianza tiene un costo: un aumento en el sesgo (observa en los extremos de las predicciones regularizadas).\nSin embargo, lo que nos importa principalmente es reducir el error de predicción, y eso lo logramos escogiendo un balance sesgo-varianza apropiado para los datos y el problema.\n\n\n\n\n\n\n\nRegularización L2\n\n\n\nCuando agregamos el término de penalización tipo ridge al error de entrenamiento como objetivo a minimizar en el ajuste, los coeficientes de la solución penalizada están encogidos con respecto a los no penalizados.\nRegularizar reduce la varianza de los coeficientes a lo largo de distintas muestras de entrenamiiento, lo que reduce la posibilidad de sobreajuste.\nUtilizamos regularización para reducir el error de predicción cuando el problema es variabilidad grande de los coeficientes (coeficientes ruidosos) en modelos relativamente grandes o con pocos datos de entrenamiento.\n\n\nEn general, a métodos donde restringimos el espacio de modelos o penalizamos ajustes complejos en la función de pérdida que nos interesa se llaman métodos con regularización. Un ejemplos es todos los modelos donde en lugar de considerar la función de perdida \\(L\\) solamente, consideramos minimizar\n\\[L(f) + \\Omega(f),\\] donde \\(f\\) es una medida de la complejidad, como puede ser: que la función \\(f\\) tiene oscilaciones grandes o pendientes grandes, tiene un número grande de discontinuidades, etc."
  },
  {
    "objectID": "05-regularizacion-1.html#ejemplo-2-penalización-y-estimaciones-ruidosas",
    "href": "05-regularizacion-1.html#ejemplo-2-penalización-y-estimaciones-ruidosas",
    "title": "5  Regularización y variabilidad",
    "section": "5.3 Ejemplo 2: penalización y estimaciones ruidosas",
    "text": "5.3 Ejemplo 2: penalización y estimaciones ruidosas\nConsideremos los siguientes datos clásicos de Radiación Solar, Temperatura, Velocidad del Viento y Ozono para distintos días en Nueva York (Chambers et al. (1983)):\n\nair_data &lt;- airquality |&gt; \n    mutate(Wind_cat = cut(Wind, quantile(Wind, c(0, 0.33, 0.66, 1)), \n                          include.lowest = T)) |&gt; \n    filter(!is.na(Ozone) & !is.na(Solar.R))\nair &lt;- air_data\nggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + \n  geom_point() +\n  facet_wrap(~Wind_cat, ncol = 3) + \n  scale_colour_gradientn(colours = rainbow(2, rev = TRUE))\n\n\n\n\nLa gráfica muestra algunas interacciones y relaciones no lineales. Formulamos un modelo lineal como sigue:\n\nreceta_ozono &lt;- recipe(Ozone ~ Temp + Wind + Solar.R,\n                       data = air) |&gt; \n  step_ns(Temp, Wind, Solar.R, deg_free = 2) |&gt; \n  step_interact(terms = ~ starts_with(\"Temp_ns\"):starts_with(\"Wind_ns\")) |&gt; \n  step_interact(terms = ~ starts_with(\"Temp_ns\"):starts_with(\"Solar.R_ns\"))\najuste_ozono &lt;- workflow() |&gt; \n  add_recipe(receta_ozono) |&gt; \n  add_model(linear_reg() |&gt; set_engine(\"lm\")) |&gt; \n  fit(air)\n\nY el ajuste se ve como sigue:\n\npred_grid &lt;- expand_grid(Wind = c(5,10,15), \n                         Temp = seq(60, 90, 10), \n                         Solar.R = seq(20, 300, by = 10)) |&gt; \n    mutate(Wind_cat = cut(Wind, \n           quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), \n           include.lowest = T))\npred_grid &lt;- pred_grid |&gt; \n  bind_cols(predict(ajuste_ozono, pred_grid))\ng_lineal &lt;- ggplot(air, aes(x = Solar.R, colour = Temp)) + \n    geom_point(aes(y = Ozone)) +\n    facet_wrap( ~ Wind_cat) + \n    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +\n    geom_line(data = pred_grid, \n      aes(y = .pred, group = interaction(Temp, Wind_cat)), size = 1) +\n    labs(subtitle = \"Curvas de modelo lineal, para viento = 5, 10, 15\") \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ng_lineal\n\n\n\n\nNótese que algunos aspectos de este modelo parecen ser muy ruidosos: por ejemplo, el comportamiento de las curvas para el primer pánel (donde hay pocos datos de temperatura baja), el hecho de que en algunos casos parece haber curvaturas decrecientes e incluso predicciones negativas. No deberíamos dar mucho crédito a las predicciones de este modelo, y tiene peligro de producir predicciones desastrosas.\nSin embargo, si usamos algo de regularización:\n\najuste_ozono &lt;- workflow() |&gt; \n  add_recipe(receta_ozono) |&gt; \n  add_model(linear_reg(mixture = 0, penalty = 3.0) |&gt; \n              set_engine(\"glmnet\", lambda.min.ratio = 0)) |&gt; \n  fit(air)\n# nota: normalmente no es necesario usar lambda.min.ratio\n\nY el ajuste se ve como sigue:\n\npred_grid &lt;- expand_grid(Wind = c(5,10,15), \n                         Temp = seq(60, 90, 10), \n                         Solar.R = seq(10, 320, by = 10)) |&gt; \n    mutate(Wind_cat = \n           cut(Wind, quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), \n               include.lowest = T))\npred_grid &lt;- pred_grid |&gt; \n  bind_cols(predict(ajuste_ozono, pred_grid))\ng_lineal &lt;- ggplot(air, aes(x = Solar.R, colour = Temp)) + \n    geom_point(aes(y = Ozone)) +\n    facet_wrap( ~ Wind_cat) + \n    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +\n    geom_line(data = pred_grid, aes(y = .pred, group = interaction(Temp, Wind_cat)), size = 1) +\n    labs(subtitle = \"Curvas de modelo lineal, para viento = 5, 10, 15\") \ng_lineal\n\n\n\n\nEste ajuste se ve mucho más razonable."
  },
  {
    "objectID": "05-regularizacion-1.html#regresión-ridge-escogiendo-el-parámetro-de-complejidad",
    "href": "05-regularizacion-1.html#regresión-ridge-escogiendo-el-parámetro-de-complejidad",
    "title": "5  Regularización y variabilidad",
    "section": "5.4 Regresión ridge: escogiendo el parámetro de complejidad",
    "text": "5.4 Regresión ridge: escogiendo el parámetro de complejidad\nComo vimos antes, no es posible seleccionar el parámetro \\(\\lambda\\) usando la muestra de entrenamiento (¿con qué \\(\\lambda\\) cómo se obtiene el menor error cuadrático medio sobre la muestra de entrenamiento). Usaremos un conjunto de validación relativamente grande\n\nset.seed(191)\nsource(\"../R/casas_traducir_geo.R\")\n\nRows: 1460 Columns: 81\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (43): MSZoning, Street, Alley, LotShape, LandContour, Utilities, LotConf...\ndbl (38): Id, MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, Ye...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 27 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Neighborhood\ndbl (2): lat, long\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# esta proporción es para ejemplificar\ncasas_split &lt;- initial_split(casas, prop = 0.25) \ncasas_entrena &lt;- training(casas_split)\nreceta_casas &lt;- \n  recipe(precio_miles ~ calidad_gral +\n           area_hab_m2 + area_garage_m2 + area_sotano_m2 + \n           area_2o_piso_m2 + \n           año_construccion + año_venta + condicion_venta +\n           nombre_zona + \n           condicion_gral + \n           condicion_exteriores + \n           tipo_sotano + calidad_sotano +\n           baños_completos +  num_coches +\n           aire_acondicionado + \n           tipo_edificio + estilo, \n         data = casas_entrena) |&gt; \n  step_filter(condicion_venta == \"Normal\") |&gt; \n  step_select(-condicion_venta, skip = TRUE) |&gt; \n  step_cut(calidad_gral, breaks = c(3, 5, 7, 8), \n           include_outside_range = TRUE) |&gt; \n  step_cut(condicion_gral, breaks = c(3, 5, 7, 8), \n           include_outside_range = TRUE) |&gt; \n  step_mutate(sin_piso_2 = as.numeric(area_2o_piso_m2 == 0)) |&gt;\n  step_novel(tipo_sotano, calidad_sotano) |&gt; \n  step_novel(condicion_exteriores, tipo_edificio, estilo) |&gt; \n  step_unknown(tipo_sotano, calidad_sotano, new_level = \"sin_sotano\") |&gt; \n  step_unknown(condicion_exteriores) |&gt; \n  step_other(nombre_zona, threshold = 0.05) |&gt; \n  step_dummy(calidad_gral, condicion_gral, \n             nombre_zona, aire_acondicionado,\n             calidad_sotano, tipo_sotano, condicion_exteriores, \n             tipo_edificio, estilo) |&gt; \n  step_interact(terms = ~ c(area_hab_m2, area_garage_m2, \n    area_sotano_m2, area_2o_piso_m2):starts_with(\"calidad_gral\")) |&gt; \n  step_interact(terms = ~ area_sotano_m2:starts_with(\"calidad_sotano\")) |&gt; \n  step_interact(terms = ~ c(area_hab_m2, area_garage_m2, \n    area_sotano_m2, area_2o_piso_m2):starts_with(\"condicion_gral\")) |&gt; \n  step_interact(terms = ~ c(area_hab_m2, area_garage_m2, \n    area_sotano_m2, area_2o_piso_m2):starts_with(\"nombre_zona\")) |&gt; \n  step_zv(all_predictors())\n\nPara ver el número de entradas de este modelo:\n\nprep(receta_casas) |&gt; juice() |&gt; dim()\n\n[1] 289 108\n\n\n\nmodelo_penalizado &lt;- linear_reg(mixture = 0.0, penalty = tune()) |&gt; \n  set_engine(\"glmnet\", lambda.min.ratio = 0)\nflujo_casas &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(modelo_penalizado)\n\nConstruimos manualmente el conjunto de validación:\n\n# creamos un objeto con datos de entrenamiento y de prueba\nval_split &lt;- manual_rset(casas_split |&gt; list(), \"validación\")\nlambda_params &lt;- parameters(penalty(range = c(-3, 3), \n                                    trans = log10_trans()))\nlambda_grid &lt;- grid_regular(lambda_params, levels = 20)\nlambda_grid\n\n# A tibble: 20 × 1\n      penalty\n        &lt;dbl&gt;\n 1    0.001  \n 2    0.00207\n 3    0.00428\n 4    0.00886\n 5    0.0183 \n 6    0.0379 \n 7    0.0785 \n 8    0.162  \n 9    0.336  \n10    0.695  \n11    1.44   \n12    2.98   \n13    6.16   \n14   12.7    \n15   26.4    \n16   54.6    \n17  113.     \n18  234.     \n19  483.     \n20 1000      \n\n\n\nmis_metricas &lt;- metric_set(rmse)\neval_tbl &lt;- tune_grid(flujo_casas,\n                      resamples = val_split,\n                      grid = lambda_grid,\n                      metrics = mis_metricas) \nridge_ajustes_tbl &lt;- eval_tbl |&gt;\n  unnest(cols = c(.metrics)) |&gt; \n  select(id, penalty, .metric, .estimate)\nridge_ajustes_tbl |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      id\n      penalty\n      .metric\n      .estimate\n    \n  \n  \n    validación\n1.000000e-03\nrmse\n35.36721\n    validación\n2.069138e-03\nrmse\n35.36721\n    validación\n4.281332e-03\nrmse\n35.36721\n    validación\n8.858668e-03\nrmse\n35.36721\n    validación\n1.832981e-02\nrmse\n35.36721\n    validación\n3.792690e-02\nrmse\n35.36721\n    validación\n7.847600e-02\nrmse\n34.88798\n    validación\n1.623777e-01\nrmse\n33.64199\n    validación\n3.359818e-01\nrmse\n32.41837\n    validación\n6.951928e-01\nrmse\n31.22570\n    validación\n1.438450e+00\nrmse\n30.26859\n    validación\n2.976351e+00\nrmse\n29.62744\n    validación\n6.158482e+00\nrmse\n29.45135\n    validación\n1.274275e+01\nrmse\n29.65517\n    validación\n2.636651e+01\nrmse\n30.23900\n    validación\n5.455595e+01\nrmse\n31.52315\n    validación\n1.128838e+02\nrmse\n34.17121\n    validación\n2.335721e+02\nrmse\n38.82727\n    validación\n4.832930e+02\nrmse\n45.67700\n    validación\n1.000000e+03\nrmse\n54.01752\n  \n  \n  \n\n\n\n\n\nggplot(ridge_ajustes_tbl, \n    aes(x = penalty, y = .estimate, colour = .metric)) + \n  geom_point() + geom_line() + scale_x_log10() \n\n\n\n\nY vemos que con una penalización alrededor de \\(\\lambda = 1\\) podemos obtener mejor desempeño que con el modelo no regularizado.\nPregunta: en qué partes de la gráfica es relativamente grande la varianza? ¿en qué parte es relativamente grande el sesgo?"
  },
  {
    "objectID": "05-regularizacion-1.html#regresión-lasso",
    "href": "05-regularizacion-1.html#regresión-lasso",
    "title": "5  Regularización y variabilidad",
    "section": "5.5 Regresión lasso",
    "text": "5.5 Regresión lasso\nSe puede controlar la varianza de mínimos cuadrados de otras maneras. Cuando la varianza proviene también de la inclusión de variables que no necesariamente están relacionadas con la respuesta, podemos usar métodos de selección de variables, como en stepwise regression, por ejemplo.\nOtra manera interesante de lograr mejor desempeño predictivo con modelos más parsimoniosos resulta de usar un término de penalización distinto al de ridge. En ridge, el problema que resolvemos es minimizar el objetivo\n\\[D(\\beta) + \\lambda \\sum_{j=1}^p \\beta_j^2\\]\nEn regresión lasso, usamos una penalización de tipo \\(L_1\\):\n\\[D(a) + \\lambda \\sum_{j=1}^p |\\beta_j|\\] En un principio, no parece ser muy diferente a ridge. Veremos sin embargo que usar esta penalización también se puede ver como un proceso de selección de variables."
  },
  {
    "objectID": "05-regularizacion-1.html#lasso-vs-ridge",
    "href": "05-regularizacion-1.html#lasso-vs-ridge",
    "title": "5  Regularización y variabilidad",
    "section": "5.6 Lasso vs Ridge",
    "text": "5.6 Lasso vs Ridge\nConsideramos cómo predecir el porcentaje de grasa corporal a partir de distintas mediciones de dimensiones corporales:\n\ndat_grasa &lt;- read_csv(file = '../datos/bodyfat.csv') \nset.seed(183)\ngrasa_particion &lt;- initial_split(dat_grasa, 0.7)\ngrasa_ent &lt;- training(grasa_particion)\ngrasa_pr &lt;- testing(grasa_particion)\n\n\n# nota: con glmnet no es necesario normalizar, pero aquí lo hacemos\n# para ver los coeficientes en términos de las variables estandarizadas:\ngrasa_receta &lt;- recipe(grasacorp ~ ., grasa_ent) |&gt; \n  update_role(cadera, cuello, muñeca, \n              tobillo, rodilla, new_role = \"ninguno\") |&gt; \n  step_normalize(all_predictors()) |&gt; \n  prep()\nmodelo_2 &lt;- linear_reg(mixture = 0, penalty = 0) |&gt; \n  set_engine(\"glmnet\", lambda.min.ratio = 1e-20) \nflujo_2 &lt;- workflow() |&gt; \n  add_model(modelo_2) |&gt; \n  add_recipe(grasa_receta)\nflujo_2 &lt;- flujo_2 |&gt; fit(grasa_ent) \nmodelo_2 &lt;- extract_fit_parsnip(flujo_2)\ncoefs &lt;- modelo_2 |&gt; pluck(\"fit\") |&gt;tidy() |&gt; \n  filter(term != \"(Intercept)\")\ng_l2 &lt;- ggplot(coefs, aes(x = lambda, y = estimate, colour = term)) +\n  geom_line(size = 1.4) + scale_x_log10() +\n  scale_colour_manual(values = cbb_palette) +\n  labs(subtitle = \"Regularizacion L2\")\ng_l2\n\n\n\n\nEstas gráfica se llama traza de los coeficientes, y nos muestra cómo cambian los coefi´cientes conforme cambiamos la regularización. Nótese que cuando la regularización es chica, obtenemos algunos resultados contra-intuitivos como que el coeficiente de peso es negativo para predecir el nivel de grasa corporal. Cuando regularizamos más, este coeficiente es positivo. La razón de esto tiene qué ver con la correlación fuerte de las variables de entrada, por ejemplo:\n\ncor(grasa_ent |&gt; select(peso, abdomen, biceps, muslo)) |&gt; \n  round(2)\n\n        peso abdomen biceps muslo\npeso    1.00    0.91   0.81  0.88\nabdomen 0.91    1.00   0.71  0.81\nbiceps  0.81    0.71   1.00  0.75\nmuslo   0.88    0.81   0.75  1.00\n\n\nAhora probemos con regularización lasso:\n\n## mixture = 1 es regresión lasso\nmodelo_1 &lt;-  linear_reg(mixture = 1, penalty = 0) |&gt; \n  set_engine(\"glmnet\", lambda.min.ratio = 0) \nflujo_1 &lt;- workflow() |&gt; \n  add_model(modelo_1) |&gt; \n  add_recipe(grasa_receta)\n\n\nflujo_1 &lt;- flujo_1 |&gt; fit(grasa_ent) \nmodelo_1 &lt;- extract_fit_parsnip(flujo_1)\ncoefs &lt;- modelo_1 |&gt; pluck(\"fit\") |&gt; tidy() |&gt; \n  filter(term != \"(Intercept)\")\nggplot(coefs, \n    aes(x = lambda, y = estimate, colour = term)) +\n  geom_line(size = 1.4) + scale_x_log10() +\n  scale_colour_manual(values = cbb_palette) +\n  labs(subtitle = \"Regularizacion L1\")\n\n\n\n\nY nótese que conforme aumentamos la penalización, algunas variables salen del modelo (sus coeficientes son cero). Por ejemplo, para un valor de \\(lambda\\) intermedio, obtenemos un modelo simple de la forma:\n\ncoefs |&gt; filter(step == 21) |&gt; \n  select(term, estimate, lambda)\n\n# A tibble: 3 × 3\n  term     estimate lambda\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 edad        0.160  0.434\n2 estatura   -0.452  0.434\n3 abdomen     6.67   0.434\n\n\nY nótese que este modelo solo incluye 3 variables.La traza confirma que la regularización lasso, además de encoger coeficientes, saca variables del modelo conforme el valor de regularización aumenta.\nLa razón de esta diferencia cualitativa entre cómo funciona lasso y ridge se puede entender considerando que los problemas de penalización mostrados arriba puede escribirse en forma de problemas de restricción. Por ejemplo, lasso se puede reescribir como el problema de resolver\n\\[\\min_a D(\\beta)\\] sujeto a \\[\\sum_{i=1}^p |\\beta_j| &lt; t\\] En la gráfica siguiente (tomada de Hastie, Tibshirani, y Friedman (2017)), lasso está a la izquierda y ridge está a la derecha, las curvas rojas son curvas de nivel de la suma de cuadrados \\(D(a)\\), y \\(\\hat{\\beta}\\) es el estimador usual de mínimos cuadrados de los coeficientes (sin penalizar). En azul está la restricción:\n\n\n\nRidge vs Lasso\n\n\n\n\n\n\n\n\nRegularización para modelos lineales\n\n\n\n\nEn regresión ridge, los coeficientes se encogen gradualmente desde la solución no restringida hasta el origen. Ridge es un método de encogimiento de coeficientes. Regresión ridge es especialmente útil cuando tenemos varias variables de entrada fuertemente correlacionadas. Regresión ridge intenta encoger juntos coeficientes de variables correlacionadas para reducir varianza en las predicciones.\nEn regresión lasso, los coeficientes se encogen gradualmente, pero también se excluyen variables del modelo. Por eso lasso es un método de encogimiento y selección de variables. Lasso encoge igualmente coeficientes para reducir varianza, pero también comparte similitudes con regresión de mejor subconjunto, en donde para cada número de variables \\(l\\) buscamos escoger las \\(l\\) variables que den el mejor modelo. Sin embargo, el enfoque de lasso es más escalable y puede calcularse de manera más simple.\n\n\n\nNota: es posible también utilizar una penalización que mezcla ridge y lasso:\n\\[\\lambda \\left (\\alpha \\sum_j |a_j| + (1-\\alpha)\\sum_j a_j^2 \\right )\\]\ny \\(\\alpha\\) es un parámetro que podemos afinar:\n\n# elastic net = ridge + lasso\n# mixture es alpha y penalty es lambda\nmodelo_enet &lt;- linear_reg(mixture = 0.5, penalty = 0.05)\n# y si queremos afinar:\nmodelo_enet &lt;- linear_reg(mixture = tune(), penalty = tune())"
  },
  {
    "objectID": "05-regularizacion-1.html#regularización-con-descenso-en-gradiente",
    "href": "05-regularizacion-1.html#regularización-con-descenso-en-gradiente",
    "title": "5  Regularización y variabilidad",
    "section": "5.7 Regularización con descenso en gradiente",
    "text": "5.7 Regularización con descenso en gradiente\nOtra forma de hacer regularización que se utiliza comunmente se basa en el método de minimización que usamos para obtener nuestra función \\(\\hat{f}\\) para hacer predicciones. La idea es utilizar un método iterativo que comience con una \\(f_0\\) simple, y luego iterar a una nueva \\(f_1\\) que se adapta mejor a los datos pero no es muy diferente a \\(f_0\\). En lugar de seguir iterando hasta llegar a un mínimo de \\(L(f),\\) evaluamos con una muestra de prueba para encontrar un lugar apropiado para detenernos (early stopping). También podemos modificar \\(L(f)\\) en cada paso para evitar atorarnos en un mínimo sobreajustado.\nUna manera de hacer esto es usando el método de descenso estocástico, (ver apéndices Apéndice A y Apéndice B) que consiste en:\n\nEn cada iteración \\(i\\) construimos una función de pérdida \\(L^{(i)}(f)\\) basada solamente en una parte de los datos (batch).\nEn cada iteración \\(i\\) sólo nos movemos en una dirección de descenso para los parámetros, sin intentar buscar un mínimo local o global. La dirección de descenso está dada por \\(-\\nabla L^{(i)}\\).\n\nAunque este método es más útil en casos como redes neuronales o métodos basados en árboles, podemos comenzar por un ejemplo en regresión lineal para entender su efecto regularizador:\n\nlibrary(keras)\nx_grasa &lt;- grasa_receta |&gt; juice() |&gt; \n  select(abdomen, edad, antebrazo, biceps, estatura, muslo, pecho, peso) |&gt; \n  as.matrix()\nvars_nombres &lt;- colnames(x_grasa)\ny_grasa &lt;- grasa_receta |&gt; juice() |&gt; pull(grasacorp)\n## keras tiene distintos algos de optimización\nmodelo_reg &lt;- keras_model_sequential() |&gt; \n  layer_dense(units = 1, \n    kernel_initializer = initializer_constant(0))\nmodelo_reg |&gt; compile(\n  loss = \"mse\",\n  optimizer = optimizer_sgd(learning_rate = 0.01)\n)\n# esto es más eficiente hacerlo con callbacks en general:\npesos_tbl &lt;- map_dfr(1:400, function(epoca){\n    modelo_reg |&gt; fit(\n      x = x_grasa, y = y_grasa,\n      epochs = 1, \n      verbose = FALSE)\n    pesos_tbl &lt;- get_weights(modelo_reg)[[1]] |&gt; t() |&gt; \n      as_tibble() \n    names(pesos_tbl) &lt;- vars_nombres\n    pesos_tbl |&gt; mutate(epoca = epoca)\n  }\n)\n\n\nlibrary(patchwork)\ng_dest &lt;- pesos_tbl |&gt; pivot_longer(cols  = -contains(c(\"grasacorp\", \"epoca\"))) |&gt; \n  ggplot(aes(x = epoca, y = value, colour = name)) + \n  geom_line(linewidth = 1.1) +   scale_colour_manual(values = cbb_palette) +\n  scale_x_continuous(trans  = compose_trans(\"log10\", \"reverse\")) +\n  labs(subtitle = \"Descenso estocástico\")\n\ng_dest + g_l2\n\n\n\n\nY vemos que si inicializamos el proceso de minimización con valores chicos, pararnos en una época (iteración completa sobre los datos) nos permite tener un efecto similar al de utilizar regularización tipo L2.\n\n\n\n\n\n\nDescenso estocástico\n\n\n\nEl método de descenso estocástico (usualmente por minilotes) nos permite resolver problemas de optimización, y muchas veces actúa también como regularizador (al cambiar en cada paso la función de pérdida, y utilizando early stopping). Tiene las ventajas adicionales de que:\n\nAl cambiar la función de pérdida en cada paso, también es posible escapar de puntos estacionarios subóptimos (si el problema no tiene varios puntos estacionarios, es decir, donde el gradiente es cero).\nEs eficiente en el sentido de que no es necesario utilizar todo los datos para hacer un paso suficientemente bueno, y es escalable a grandes conjuntos de datos.\n\nEs crucial escoger un tamaño de paso adecuado para cada problema. Generalmente se considera un parámetro que debe ser afinado, de manera similar al parámetro de regularización L2 que vimos arriba.\n\n\n\n\n\n\nChambers, J. M., W. S. Cleveland, B. Kleiner, y P. A. Tukey. 1983. Graphical Methods for Data Analysis. Chapman & Hall statistics series. Wadsworth International Group. https://books.google.com.mx/books?id=I-tQAAAAMAAJ.\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/."
  },
  {
    "objectID": "06-redes-neuronales-1.html#introducción-a-redes-neuronales",
    "href": "06-redes-neuronales-1.html#introducción-a-redes-neuronales",
    "title": "6  Redes neuronales (intro)",
    "section": "6.1 Introducción a redes neuronales",
    "text": "6.1 Introducción a redes neuronales\nEn partes anteriores, vimos cómo hacer más flexibles los métodos de regresión: la idea es construir entradas derivadas a partir de las variables originales, e incluirlas en el modelo de regresión. Este enfoque es bueno cuando tenemos relativamente pocas variables originales de entrada, y tenemos una idea de qué variables derivadas es buena idea incluir (por ejemplo, splines para una variable como edad, interacciones para variables importantes, etc). Sin embargo, si hay una gran cantidad de entradas, esta técnica puede ser prohibitiva en términos de cálculo y trabajo manual.\nPor ejemplo, si tenemos unas 100 entradas numéricas, al crear todas las interacciones \\(x_i x_j\\) y los cuadrados \\(x_i^2\\) terminamos con unas 5150 variables. Para el problema de dígitos (256 entradas o pixeles) terminaríamos con unas 32 mil entradas adicionales. Aún cuando es posible regularizar, en estos casos suena más conveniente construir entradas derivadas a partir de los datos.\nPara hacer esto, consideramos entradas \\(X_1, . . . , X_p\\), y supongamos que tenemos un problema regresión donde queremos predecir \\(Y\\). Aunque hay muchas maneras de construir entradas derivadas, una manera simple sería construir \\(m\\) nuevas entradas mediante:\n\\[a_k = h \\left ( \\theta_{k,0} + \\sum_{j=1}^p \\theta_{k,j}x_j\n\\right)\\]\npara \\(k=1,\\ldots, m\\), donde \\(h\\) es una función no lineal (logística o relu entre otras), y las \\(\\theta\\) son parámetros que seleccionaremos más tarde. La idea es hacer combinaciones lineales de variables transformadas.\nModelamos ahora la respuesta usando las entradas derivadas en lugar de las originales en una regresión lineal:\n\\(a_1, . . . , a_m\\): \\[f(x) =  \\beta_0 + \\sum_{j=1}^m \\beta_ja_j\\]\nPodemos representar este esquema con una red dirigida (\\(m=3\\) variables derivadas):\n\n\n\n\n\n\nLa función logística\nUna de las transformaciones \\(h\\) más comunes para construir entradas derivadas es la función logística:\n\n\n\nLa función logística está dada por \\[h(x)=\\frac{e^x}{1+e^x}\\]\n\n\n\nh &lt;- function(x){exp(x)/(1+exp(x)) }\nggplot(tibble(x = seq(-6, 6, 0.01)), aes(x = x)) + stat_function(fun = h)\n\n\n\n\nObservaciones:\n\n¿Por qué usar \\(h\\) para las entradas derivadas \\(a_k\\)? En primer lugar, nótese que si no transformamos con alguna función no lineal \\(h\\), el modelo final \\(p_1\\) para la probabilidad condicional es el mismo que el de regresión logística (combinaciones lineales de combinaciones lineales son combinaciones lineales). Sin embargo, al transformar con \\(h\\), las \\(x_j\\) contribuyen de manera no lineal a las entradas derivadas.\nLas variables \\(a_k\\) que se pueden obtener son similares (para una variable de entrada) a los splines que vimos en la parte anterior.\nEs posible demostrar que si se crean suficientes entradas derivadas (\\(m\\) es suficientemente grande), entonces la función \\(f(x)\\) puede aproximar cualquier función continua. La función \\(h\\) (que se llama función de activación no es especial: funciones continuas con forma similar a la sigmoide (logística) pueden usarse también (por ejemplo, arcotangente, o lineal rectificada). La idea es que cualquier función se puede aproximar mediante superposición de funciones tipo sigmoide (ver por ejemplo Cybenko 1989, Approximation by Superpositions of a Sigmoidal Function).\n\n\n\n¿Cómo construyen entradas las redes neuronales?\nComencemos por un ejemplo simple de clasificación binaria con una sola entrada \\(x\\). Supondremos que el modelo verdadero está dado por:\n\nh &lt;- function(x){\n    1/(1 + exp(-x)) # es lo mismo que exp(x)/(1 + exp(x))\n}\nx &lt;- seq(-2, 2, 0.1)\nf &lt;- atan(2 - 2 * x^2)\nset.seed(2805721)\nx_1 &lt;- runif(10, -2, 2)\ny &lt;- rnorm(10, atan(2 - 2 * x_1^2), 0.2)\ndatos &lt;- tibble(x_1, y)\ndat_f &lt;- tibble(x, f)\ng &lt;- ggplot(dat_f) + geom_line(aes(x, f))\ng\n\n\n\ng + geom_point(data = datos, aes(x = x_1, y = y), colour = 'red')\n\n\n\n\ndonde adicionalmente graficamos 30 datos simulados. Recordamos que queremos ajustar la curva roja, que da la probabilidad condicional de clase. Podríamos ajustar un modelo de regresión logística expandiendo manualmente el espacio de entradas agregando \\(x^2\\), y obtendríamos un ajuste razonable. Pero la idea aquí es que podemos crear entradas derivadas de forma automática.\nSupongamos entonces que pensamos crear dos entradas \\(a_1\\) y \\(a_2\\), funciones de \\(x_1\\), y luego predecir \\(g.1\\), la clase, en función de estas dos entradas. Por ejemplo, podríamos tomar:\n\n\n\n\n\ndonde hacemos una regresión para predecir \\(y\\) mediante \\[f(a) = \\beta_0 + \\beta_1a_1+\\beta_2 a_2,\\] \\(a_1\\) y \\(a_2\\) están dadas por \\[a_1(x)=h(\\beta_{1,0} + \\beta_{1,1} x_1),\\] \\[a_2(x)=h(\\beta_{2,0} + \\beta_{2,1} x_1).\\]\nPor ejemplo, podríamos tomar\n\na_1 &lt;- h( 1 + 2 * x)  # 2(x+1/2)\na_2 &lt;- h(-1 + 2 * x)  # 2(x-1/2) # una es una versión desplazada de otra.\n\nLas funciones \\(a_1\\) y \\(a_2\\) dependen de \\(x\\) de la siguiente forma:\n\ndat_a &lt;- tibble(x = x, a_1 = a_1, a_2 = a_2)\ndat_a_2 &lt;- dat_a |&gt; gather(variable, valor, a_1:a_2)\nggplot(dat_a_2, aes(x=x, y=valor, colour=variable, group=variable)) + geom_line()\n\n\n\n\nSi las escalamos y sumamos, obtenemos\n\ndat_a &lt;- tibble(x=x, a_1 = a_1, a_2 =  a_2, \n  suma = -1.5 +  6 * a_1 -  6 * a_2)\ndat_a_2 &lt;- dat_a |&gt; \n  pivot_longer(a_1:suma, names_to = \"variable\", values_to = \"valor\")\nggplot(dat_a_2, aes(x = x, y = valor, colour = variable, group = variable)) + geom_line()\n\n\n\n\ny finalmente obtenemos:\n\ndat_2 &lt;- tibble(x, f = (-1.5 + 6 * a_1 - 6 * a_2))\nggplot(dat_2, aes(x=x, y = f)) + geom_line()+\ngeom_line(data=dat_f, aes(x=x,y=f), col='red') +\n   geom_point(data = datos, aes(x = x_1, y = y))\n\n\n\n\nque da un ajuste razonable. Este es un ejemplo de cómo la mezcla de dos funciones logísticas puede replicar esta función con forma de chipote. Otras funciones más complejas se pueden obtener incluyendo más \\(a_j\\)’s que son versiones escaladas y desplazadas de la función logística. El mecanismo para combinar estas \\(a_j\\)’s es similar al de los splines que vimos en la sección anterior.\n\n\n¿Cómo ajustar los parámetros?\nPara encontrar los mejores parámetros, minimizamos la devianza sobre los parámetros \\(\\beta_0,\\beta_1,\\beta_2\\) y \\(\\beta_{1,0},\\beta_{1,1},\\beta_{2,0},\\beta_{2,1}\\).\nVeremos más adelante que conviene hacer esto usando descenso o en gradiente o descenso en gradiente estocástico, pero por el momento usamos la función optim de R para minimizar la devianza. En primer lugar, creamos una función que para todas las entradas calcula los valores de salida. En esta función hacemos feed-forward de las entradas a través de la red para calcular la salida.\n\n## esta función calcula los valores de cada nodo en toda la red,\n## para cada entrada\nfeed_fow &lt;- function(beta, x){\n  a_1 &lt;- h(beta[1] + beta[2] * x) # calcula variable 1 de capa oculta\n  a_2 &lt;- h(beta[3] + beta[4] * x) # calcula variable 2 de capa oculta\n  f &lt;- (beta[5] + beta[6] * a_1 + beta[7] * a_2) # calcula capa de salida\n  f\n}\n\nNótese que simplemente seguimos el diagrama mostrado arriba para hacer los cálculos, combinando linealmente las entradas en cada capa.\nAhora definimos una función para calcular la devianza. Conviene crear una función que crea funciones, para obtener una función que sólo se evalúa en los parámetros para cada conjunto de datos de entrenamiento fijos:\n\nperdida_cuad_fun &lt;- function(x, y){\n    # esta función es una fábrica de funciones\n   perdida_cuad &lt;- function(beta){\n      f &lt;- feed_fow(beta, x)\n      mean((y - f)^2)\n   }\n  perdida_cuad\n}\n\nPor ejemplo:\n\nperdida_cuad &lt;- perdida_cuad_fun(x_1, y) # crea función\n## ahora dev toma solamente los 7 parámetros beta:\nperdida_cuad(c(0,0,0,0,0,0,0))\n\n[1] 1.034101\n\n\nFinalmente, intentamos resolver el problema de minimización de la pérdida cuadrática de los datos de entrenamiento. Para esto usaremos la función optim de R:\n\nset.seed(5)\nsalida &lt;- optim(rnorm(7), perdida_cuad, method = 'BFGS') # inicializar al azar punto inicial\nbeta &lt;- salida$par\nbeta\n\n[1] -9.097577  5.315080 -6.973249 -6.762489  1.093208 -3.309966 -2.488820\n\n\nY ahora podemos graficar con el vector \\(\\beta\\) encontrado:\n\n## hacer feed forward con beta encontrados\np_2 &lt;- feed_fow(beta, x)\ndat_2 &lt;- data.frame(x, p_2 = p_2)\nggplot(dat_2, aes(x = x, y = p_2)) + geom_line()+\ngeom_line(data = dat_f, aes(x = x, y = f), col='red') +\n   geom_point(data = datos, aes(x = x_1, y = y))\n\n\n\n\nLos coeficientes estimados, que en este caso muchas veces se llaman pesos, son:\n\nbeta |&gt; round(2)\n\n[1] -9.10  5.32 -6.97 -6.76  1.09 -3.31 -2.49\n\n\nque parecen ser muy grandes. Igualmente, de la figura vemos que el ajuste no parece ser muy estable (esto se puede confirmar corriendo con distintos conjuntos de entrenamiento). Podemos entonces regularizar ligeramente la devianza para resolver este problema. En primer lugar, definimos la devianza regularizada (ridge), donde penalizamos todos los coeficientes que multiplican a una variable, pero no los intercepts:\n\nperdida_cuad_fun_r &lt;- function(x, y, lambda){\n    # esta función es una fábrica de funciones\n   perdida_reg &lt;- function(beta){\n         f &lt;- feed_fow(beta, x)\n         # en esta regularizacion quitamos sesgos, pero puede hacerse también con sesgos.\n         mean((y - f)^2) + lambda * sum(beta[c(2,4,6:7)]^2) \n   }\n  perdida_reg\n}\n\n\nperdida_r &lt;- perdida_cuad_fun_r(x_1, y, 0.001) # crea función dev\nset.seed(5)\nsalida &lt;- optim(rnorm(7, 0, 1), perdida_r, method = 'BFGS') # inicializar al azar punto inicial\nbeta &lt;- salida$par\nperdida_cuad(beta) / nrow(datos)\n\n[1] 0.001831033\n\nf_2 &lt;- feed_fow(beta, x)\ndat_2 &lt;- data.frame(x, f_2 = f_2)\nbeta\n\n[1] -2.072798  2.136928 -4.317303 -4.221802  1.458438 -3.053558 -2.952405\n\nggplot(dat_2, aes(x = x, y = f_2)) + geom_line() +\ngeom_line(data = dat_f, aes(x = x, y = f), col='red') +\n   geom_point(data = datos, aes(x = x_1, y = y))\n\n\n\n\ny obtenemos un ajuste más estable. Podemos usar también keras. El modelo, con una capa intermedia de dos unidades, y regularización ridge para los coeficientes, y optimización por descenso en gradiente se define como:\n\nlibrary(keras)\n# para reproducibilidad:\ntensorflow::set_random_seed(13) \n# construir modelo\nejemplo_mod &lt;- keras_model_sequential()\nejemplo_mod |&gt; \n   layer_dense(units = 2, \n    activation = \"sigmoid\", kernel_regularizer = regularizer_l2(0.001)) |&gt; \n  layer_dense(units = 1, \n    activation = \"linear\", kernel_regularizer = regularizer_l2(0.001))\n\n\nx_mat &lt;- as.matrix(datos$x_1, ncol = 1)\ny &lt;- datos$y\n# usamos devianza como medida de error y descenso en gradiente:\nejemplo_mod |&gt; compile(loss = \"mse\", \n  optimizer = optimizer_sgd(learning_rate = 0.2),\n  metrics = \"mse\")\n# nota: esta learning rate (lr) es demasiado alta para problemas típicos\nhistoria &lt;- ejemplo_mod |&gt; \n  fit(x_mat, y, \n      batch_size = nrow(x_mat), epochs = 500, verbose = 1)\n\nDespués de verificar convergencia (chécalo examinando la variable historia), graficamos para ver que obtuvimos resultados similares:\n\ndat_3 &lt;- tibble(x = x, f_2 = predict(ejemplo_mod, as.matrix(x, ncol = 1))[,1])\nggplot(dat_3, aes(x = x, y = f_2)) + geom_line()+\ngeom_line(data = dat_f, aes(x = x, y = f), col='red') +\n   geom_point(data = datos, aes(x = x_1, y = y))\n\n\n\n\nLos coeficientes obtenidos se muestran abajo. Nótese: la primera componente de la lista son los coeficientes de la unidad 1 y 2 para \\(x\\). La segunda son los sesgos u ordenadas al origen, la tercera los coeficientes de la respuesta para las unidades 1 y 2, y el cuarto es el sesgo u ordenada al origen de la unidad de salida:\n\nget_weights(ejemplo_mod)\n\n[[1]]\n         [,1]      [,2]\n[1,] -2.45195 -2.848296\n\n[[2]]\n[1] -2.567334  2.578543\n\n[[3]]\n          [,1]\n[1,] -3.460155\n[2,]  3.080144\n\n[[4]]\n[1] -1.392124\n\n\nEjercicio: compara los coeficientes que obtuviste en este ejemplo con los anteriores."
  },
  {
    "objectID": "06-redes-neuronales-1.html#interacciones-en-redes-neuronales",
    "href": "06-redes-neuronales-1.html#interacciones-en-redes-neuronales",
    "title": "6  Redes neuronales (intro)",
    "section": "6.2 Interacciones en redes neuronales",
    "text": "6.2 Interacciones en redes neuronales\nEs posible capturar interacciones con redes neuronales. Consideremos el siguiente ejemplo simple:\n\nf &lt;- function(x1, x2){\n  2 + 0.1* x1 + 0.1 * x2 + 10 * (x1 - 0.5) * (x2 - 0.5)\n}\ndat &lt;- expand.grid(x1 = seq(0, 1, 0.05), x2 = seq(0, 1, 0.05))\ndat &lt;- dat |&gt; mutate(f = f(x1, x2))\nggplot(dat, aes(x=x1, y=x2)) + geom_tile(aes(fill=f))\n\n\n\n\nEsta función puede entenderse como un o exclusivo: la respuesta es alta sólo cuando \\(x_1\\) y \\(x_2\\) tienen valores opuestos (\\(x_1\\) grande pero \\(x_2\\) chica y viceversa).\nNo es posible modelar correctamente esta función mediante el modelo lineal (sin interacciones). Pero podemos incluir la interacción en el modelo lineal o intentar usar una red neuronal. Primero simulamos unos datos y probamos el modelo logístico con y sin interacciones:\n\nset.seed(322)\nn &lt;- 2000\ndat_ent &lt;- tibble(x1 = rbeta(n, 1, 1), x2 = rbeta(n, 1, 1)) |&gt;\n  mutate(f = f(x1, x2)) |&gt;\n  mutate(y = f + rnorm(n, 0, 0.1))\nmod_1 &lt;- lm(y ~ x1 + x2, data = dat_ent)\nmod_1\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dat_ent)\n\nCoefficients:\n(Intercept)           x1           x2  \n     1.8936       0.2046       0.2097  \n\n\nEl resultado del modelo lineal no es bueno:\n\ntibble(y_hat = fitted(mod_1), y = dat_ent$y) |&gt; \n  ggplot(aes(x = y_hat, y = y)) + geom_point(color = \"red\") +\n  geom_abline() +\n  coord_obs_pred()\n\n\n\n\nSin embargo, agregando una interacción lo mejoramos considerablemente (examina la raíz del error cuadrático medio, por ejemplo):\n\nmod_2 &lt;- lm(y ~ x1 + x2 + x1:x2, data = dat_ent)\nmod_2\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x1:x2, data = dat_ent)\n\nCoefficients:\n(Intercept)           x1           x2        x1:x2  \n      4.499       -4.895       -4.885        9.964  \n\ntibble(y_hat = fitted(mod_2), y = dat_ent$y) |&gt; \n  ggplot(aes(x = y_hat, y = y)) + geom_point(color = \"red\") +\n  geom_abline() +\n  coord_obs_pred()\n\n\n\n\nObservese la gran diferencia de error entre los dos modelos (en este caso, el sobreajuste no es un problema).\nAhora consideramos qué red neuronal puede ser apropiada.\n\ntensorflow::set_random_seed(421) \nmod_inter &lt;- keras_model_sequential()\nmod_inter |&gt; \n  layer_dense(units = 4, activation = \"sigmoid\",\n              name = \"capa_intermedia\", input_shape = c(2)) |&gt;\n  layer_dense(units = 1, name = \"capa_final\",\n              activation = \"linear\") \n\n\nmod_inter |&gt; compile(loss = \"mse\", \n  optimizer = optimizer_sgd(learning_rate = 0.3, momentum = 0.5))\nhistoria &lt;- mod_inter |&gt; \n  fit(dat_ent |&gt; select(x1, x2) |&gt; as.matrix(), dat_ent$y,\n      batch_size = 20,\n      epochs = 100, verbose = 1)\n\nVerificamos que esta red captura la interacción:\n\npreds &lt;- predict(mod_inter,\n  dat |&gt; select(x1, x2) |&gt; as.matrix())\ndat &lt;- dat |&gt; mutate(f_red = preds)\nggplot(dat, aes(x = x1, y = x2)) + \n  geom_tile(aes(fill = f_red))\n\n\n\n\n\npreds_ent &lt;- predict(mod_inter, dat_ent |&gt; select(x1, x2) |&gt; as.matrix())\ntibble(pred = preds_ent[,1], f = dat_ent$y) |&gt; \n  ggplot(aes(x = pred, y = f)) +\n  geom_point() +\n  geom_abline(colour = \"red\") +\n  coord_obs_pred()\n\n\n\n\nAunque podemos extraer los cálculos de la red ajustada, vamos a hacer el cálculo de la red a mano. La función feed forward es:\n\nbeta &lt;- get_weights(mod_inter)\nfeed_fow &lt;- function(beta, x){\n  a &lt;- h(t(beta[[1]]) %*% x + as.matrix(beta[[2]], 2, 1)) \n  f &lt;- t(beta[[3]]) %*% a + as.matrix(beta[[4]], 1, 1)\n  f\n}\n\nObservación: ¿cómo funciona esta red? Consideremos la capa intermedia (3 unidades) para cuatro casos: \\((0,0), (0,1), (1,0), (1,1)\\):\n\nmat_entrada &lt;- tibble(x_1 = c(0,0,1,1), x_2 = c(0,1,0,1)) |&gt; as.matrix()\ncapa_1 &lt;- keras_model(inputs = mod_inter$input,\n    outputs = get_layer(mod_inter, \"capa_intermedia\")$output)\npred_mat &lt;- predict(capa_1, mat_entrada) |&gt; round(2)\nrownames(pred_mat) &lt;- c(\"apagadas\", \"segunda\", \"primera\", \"ambas\")\npred_mat\n\n         [,1] [,2] [,3] [,4]\napagadas 0.15 0.00 0.64 0.12\nsegunda  0.59 0.05 0.09 0.01\nprimera  0.01 0.05 0.07 0.60\nambas    0.05 0.56 0.00 0.06\n\n\nLos pesos de la última capa son:\n\nbeta[3:4]\n\n[[1]]\n          [,1]\n[1,] -5.422077\n[2,]  4.781271\n[3,]  5.289690\n[4,] -5.083926\n\n[[2]]\n[1] 2.370607\n\n\nEjercicio: interpreta la red en términos de qué unidades están encendidas (valor cercano a 1) o apagadas (valor cercano a 0). ¿Puedes ajustar este modelo con dos tres unidades intermedias? Haz varias pruebas: ¿qué dificultades encuentras?"
  },
  {
    "objectID": "81-apendice-descenso.html#cálculo-del-gradiente",
    "href": "81-apendice-descenso.html#cálculo-del-gradiente",
    "title": "Apéndice A — Apéndice 1: descenso en gradiente",
    "section": "A.1 Cálculo del gradiente",
    "text": "A.1 Cálculo del gradiente\nVamos a escribir ahora el algoritmo de descenso en gradiente para regresión lineal. Igual que en los ejemplos anteriores, tenemos que precalcular el gradiente. Una vez que esto esté terminado, escribir la iteración es fácil.\nRecordamos que queremos minimizar (dividiendo entre dos para simplificar más adelante) \\[L(\\beta) = \\frac{1}{2N}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\]\nLa derivada de la suma es la suma de las derivadas, así nos concentramos en derivar uno de los términos\n\\[  u^{(i)}=\\frac{1}{2}(y^{(i)} - f_\\beta(x^{(i)}))^2 \\] Usamos la regla de la cadena para obtener \\[ \\frac{1}{2}\\frac{\\partial}{\\partial \\beta_j} (y^{(i)} - f_\\beta(x^{(i)}))^2 =\n-(y^{(i)} - f_\\beta(x^{(i)})) \\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j}\\]\nAhora recordamos que \\[f_{\\beta} (x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\\]\nY vemos que tenemos dos casos. Si \\(j=0\\),\n\\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_0} = 1\\] y si \\(j=1,2,\\ldots, p\\) entonces\n\\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j} = x_j^{(i)}\\]\nEntonces, si ponemos \\(u^{(i)}=\\frac{1}{2}(y^{(i)} - f_\\beta(x^{(i)}))^2\\):\n\\[\\frac{\\partial u^{(i)}}{\\partial \\beta_0} = -(y^{(i)} - f_\\beta(x^{(i)}))\\] y\n\\[\\frac{\\partial u^{(i)}}{\\partial \\beta_j} = - x_j^{(i)}(y^{(i)} - f_\\beta(x^{(i)}))\\]\nY sumando todos los términos (uno para cada caso de entrenamiento):\n\n\n\n\n\n\nGradiente para regresión lineal\n\n\n\nSea \\(e^{(i)} = y_{(i)} - f_{\\beta} (x^{(i)})\\). Entonces \\[\\begin{equation}\n  \\frac{\\partial L(\\beta)}{\\partial \\beta_0} = - \\frac{1}{N}\\sum_{i=1}^N e^{(i)}\n  (\\#eq:grad1)\n\\end{equation}\\] \\[\\begin{equation}\n  \\frac{\\partial L(\\beta)}{\\partial \\beta_j} = - \\frac{1}{N}\\sum_{i=1}^N x_j^{(i)}e^{(i)}\n  (\\#eq:grad2)\n\\end{equation}\\] para \\(j=1,2,\\ldots, p\\).\n\n\nNótese que cada punto de entrenamiento contribuye al cálculo del gradiente - la contribución es la dirección de descenso de error para ese punto particular de entrenamiento. Nos movemos entonces en una dirección promedio, para intentar hacer el error total lo más chico posible."
  },
  {
    "objectID": "81-apendice-descenso.html#implementación",
    "href": "81-apendice-descenso.html#implementación",
    "title": "Apéndice A — Apéndice 1: descenso en gradiente",
    "section": "A.2 Implementación",
    "text": "A.2 Implementación\nEn este punto, podemos intentar una implementación simple basada en el código anterior para hacer descenso en gradiente para nuestro problema de regresión (es un buen ejercicio). En lugar de eso, mostraremos cómo usar librerías ahora estándar para hacer esto. En particular usamos keras (con tensorflow), que tienen la ventaja:\n\nEn tensorflow y keras no es necesario calcular las derivadas a mano. Utiliza diferenciación automática, que no es diferenciación numérica ni simbólica: se basa en la regla de la cadena y la codificación explícita de las derivadas de funciones elementales.\n\n\nlibrary(tidymodels)\nlibrary(keras)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(68821)\n# dividir muestra\ncasas_split &lt;- initial_split(casas |&gt;\n                      select(precio_m2_miles, area_hab_m2, calidad_gral, num_coches), \n                             prop = 0.75)\n# obtener muestra de entrenamiento\ncasas_entrena &lt;- training(casas_split)\ncasas_receta &lt;- recipe(precio_m2_miles ~ ., casas_entrena) \n\n\n# definición de estructura del modelo (regresión lineal)\nx_ent &lt;- casas_receta |&gt; prep() |&gt; juice()  |&gt; select(-precio_m2_miles) |&gt; as.matrix()\ny_ent &lt;- casas_receta |&gt; prep() |&gt; juice() |&gt; pull(precio_m2_miles)\nn_entrena &lt;- nrow(x_ent)\ncrear_modelo &lt;- function(lr = 0.01){\n    modelo_casas &lt;- \n        keras_model_sequential() |&gt;\n        layer_dense(units = 1,        #una sola respuesta,\n            activation = \"linear\",    # combinar variables linealmente\n            kernel_initializer = initializer_constant(0), #inicializamos coeficientes en 0\n            bias_initializer = initializer_constant(0))   #inicializamos ordenada en 0\n    # compilar seleccionando cantidad a minimizar, optimizador y métricas\n    modelo_casas |&gt; compile(\n        loss = \"mean_squared_error\",  # pérdida cuadrática\n        optimizer = optimizer_sgd(learning_rate = lr), # descenso en gradiente\n        metrics = list(\"mean_squared_error\"))\n    modelo_casas\n}\n# tasa de aprendizaje es lr, tenemos que poner una tasa chica (prueba)\nmodelo_casas &lt;- crear_modelo(lr = 0.00001)\n# Ahora iteramos\n# Primero probamos con un número bajo de iteraciones\nhistoria &lt;- modelo_casas |&gt; fit(\n  x_ent,    # x entradas\n  y_ent,    # y salida o target\n  batch_size = nrow(x_ent), # para descenso en gradiente\n  epochs = 20, # número de iteraciones\n  verbose = 0\n)\n\n\nplot(historia, metrics = \"mean_squared_error\", smooth = FALSE) +\n  geom_line()\n\n\n\nhistoria$metrics$mean_squared_error |&gt; round(4)\n\n [1] 1.7903 0.7636 0.4549 0.3621 0.3342 0.3258 0.3232 0.3224 0.3222 0.3221\n[11] 0.3220 0.3220 0.3220 0.3219 0.3219 0.3219 0.3218 0.3218 0.3218 0.3217\n\n\nProbamos con más corridas para checar convergencia:\n\n# Agregamos iteraciones: esta historia comienza en los últimos valores de\n# la corrida anterior\nhistoria &lt;- modelo_casas |&gt; fit(\n  as.matrix(x_ent), # x entradas\n  y_ent,            # y salida o target\n  batch_size = nrow(x_ent), # para descenso en gradiente\n  epochs = 1000, # número de iteraciones\n  verbose = 0\n)\n\n\nplot(historia, metrics = \"mean_squared_error\", smooth = FALSE) \n\n\n\n\nEl modelo parece todavía ir mejorando. Veamos de todas formas los coeficientes estimados hasta ahora:\n\nkeras::get_weights(modelo_casas)\n\n[[1]]\n            [,1]\n[1,] 0.007331367\n[2,] 0.016437240\n[3,] 0.004633876\n\n[[2]]\n[1] 0.003028648\n\n\nLa implementación oficial de R es lm, que en general tiene buen desempeño para datos que caben en memoria:\n\nlm(precio_m2_miles ~ area_hab_m2 + calidad_gral + num_coches, \n   data = casas_entrena) |&gt; \n  coef()\n\n (Intercept)  area_hab_m2 calidad_gral   num_coches \n  0.66869194  -0.00449751   0.16807663   0.13115749 \n\n\nDe modo que todavía requerimos más iteraciones para alcanzar convergencia. ¿Por qué la convergencia es tan lenta? En parte, la razón es que las escalas de las variables de entrada son muy diferentes, de modo que es difícil ajustar una tasa de aprendizaje constante que funcione bien. Podemos remediar esto poniendo todas las entradas en la misma escala (normalizando)"
  },
  {
    "objectID": "81-apendice-descenso.html#normalización-de-entradas",
    "href": "81-apendice-descenso.html#normalización-de-entradas",
    "title": "Apéndice A — Apéndice 1: descenso en gradiente",
    "section": "A.3 Normalización de entradas",
    "text": "A.3 Normalización de entradas\nLa convergencia de descenso en gradiente (y también el desempeño numérico para otros algoritmos) puede dificultarse cuando las variables tienen escalas muy diferentes. Esto produce curvaturas altas en la función que queremos minimizar.\nEn este ejemplo simple, una variable tiene desviación estándar 10 y otra 1:\n\nx1 &lt;- rnorm(100, 0, 5) \nx2 &lt;- rnorm(100, 0, 1) +  0.1*x1\ny &lt;- 0*x1 + 0*x2 + rnorm(100, 0, 0.1) \ndat &lt;- tibble(x1, x2,  y)\nrss &lt;- function(beta)  mean((as.matrix(dat[, 1:2]) %*% beta - y)^2) \ngrid_beta &lt;- expand.grid(beta1 = seq(-1, 1, length.out = 50), \n                         beta2 = seq(-1, 1, length.out = 50))\nrss_1 &lt;- apply(grid_beta, 1, rss) \ndat_x &lt;- data.frame(grid_beta, rss_1)\nggplot(dat_x, aes(x = beta1, y = beta2, z = rss_1)) + \n    geom_contour(binwidth = 0.5) +\n    coord_equal() \n\n\n\n\nEn algunas direcciones el gradiente es muy grande, y en otras chico. Esto implica que la convergencia puede ser muy lenta en algunas direcciones, puede diverger en otras, y que hay que ajustar el paso \\(\\eta &gt; 0\\) con cuidado, dependiendo de dónde comiencen las iteraciones.\nPor ejemplo, con un tamaño de paso relativamente chico, damos unos saltos grandes al principio y luego avanzamos muy lentamente:\n\ngrad_calc &lt;- function(x_ent, y_ent){\n  # calculamos directamente el gradiente\n  salida_grad &lt;- function(beta){\n    n &lt;- length(y_ent)\n    f_beta &lt;- as.matrix(cbind(1, x_ent)) %*% beta\n    e &lt;- y_ent - f_beta\n    grad_out &lt;- - as.numeric(t(cbind(1, x_ent)) %*% e) / n\n    names(grad_out) &lt;- c('Intercept', colnames(x_ent))\n    grad_out\n  }\n  salida_grad\n}\ngrad_sin_norm &lt;- grad_calc(dat[, 1:2, drop = FALSE], dat$y)\niteraciones &lt;- descenso(10, c(0, -0.25, -0.75), 0.02, grad_sin_norm)\nggplot(dat_x) + \n    geom_contour(aes(x = beta1, y = beta2, z = rss_1), binwidth = 0.5) +\n    coord_equal() +\n  geom_path(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red') +\n  geom_point(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red')\n\n\n\n\nSi incrementamos el tamaño de paso observamos también convergencia lenta. En este caso particular, subir más el tamaño de paso puede producir divergencia:\n\niteraciones &lt;- descenso(10, c(0, -0.25, -0.75), 0.07, grad_sin_norm)\nggplot(dat_x) + \n    geom_contour(aes(x = beta1, y = beta2, z = rss_1), binwidth = 0.5) +\n    coord_equal() +\n  geom_path(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red') +\n  geom_point(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red')\n\n\n\n\nUna normalización usual es con la media y desviación estándar, donde hacemos, para cada variable de entrada \\(j=1,2,\\ldots, p\\) \\[ x_j^{(i)} = \\frac{ x_j^{(i)} - \\bar{x}_j}{s_j}\\] donde \\[\\bar{x}_j = \\frac{1}{N} \\sum_{i=1}^N x_j^{(i)}\\] \\[s_j = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N (x_j^{(i)}- \\bar{x}_j )^2}\\] es decir, centramos y normalizamos por columna. Otra opción común es restar el mínimo y dividir entre la diferencia del máximo y el mínimo, de modo que las variables resultantes toman valores en \\([0,1]\\).\nEntonces escalamos antes de ajustar:\n\nx1_s = (x1 - mean(x1))/sd(x1)\nx2_s = (x2 - mean(x2))/sd(x2)\ndat &lt;- tibble(x1_s = x1_s, x2_s = x2_s,  y = y)\nrss &lt;- function(beta)  mean((as.matrix(dat[, 1:2]) %*% beta - y)^2) \ngrid_beta &lt;- expand.grid(beta1 = seq(-1, 1, length.out = 50), \n                         beta2 = seq(-1, 1, length.out = 50))\nrss_1 &lt;- apply(grid_beta, 1, rss) \ndat_x &lt;- data.frame(grid_beta, rss_1)\nggplot(dat_x, aes(x = beta1, y = beta2, z = rss_1)) + \n    geom_contour(binwidth = 0.5) +\n    coord_equal() \n\n\n\n\nNótese que los coeficientes ajustados serán diferentes a los del caso no normalizado.\nSi normalizamos, obtenemos convergencia más rápida\n\ngrad_sin_norm &lt;- grad_calc(dat[, 1:2, drop = FALSE], dat$y)\niteraciones &lt;- descenso(10, c(0, -0.25, -0.75), 0.5, grad_sin_norm)\nggplot(dat_x) + \n    geom_contour(aes(x = beta1, y = beta2, z = rss_1), binwidth = 0.5) +\n    coord_equal() +\n  geom_path(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red') +\n  geom_point(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red')\n\n\n\n\n\n\n\nCuando normalizamos antes de ajustar el modelo, las predicciones deben hacerse con entradas normalizadas. La normalización se hace con los mismos valores que se usaron en el entrenamiento (y no recalculando medias y desviaciones estándar con el conjunto de prueba). En cuanto a la forma funcional del predictor (f), el problema con entradas normalizadas es equivalente al de las entradas no normalizadas. Asegúrate de esto escribiendo cómo correponden los coeficientes de cada modelo normalizado con los coeficientes del modelo no normalizado.\n\n\n\nSupongamos que el modelo en las variables originales es \\[{f}_\\beta (X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p,\\] Consideramos el modelo con variables estandarizadas \\[{g}_{\\beta^s} (X) = \\beta_0^s + \\beta_1^s Z_1 + \\beta_2^s Z_2 + \\cdots + \\beta_p^s Z_p,\\]\nSustituyendo \\(Z_j = (X_j - \\mu_j)/s_j,\\)\n\\[{g}_{\\beta^s} (X) = (\\beta_0^s - \\sum_{j=1}^p \\beta_j^s \\mu_j/s_j) + \\frac{\\beta_1^s}{s_j} X_1 + \\frac{\\beta_2^s}{s_2} X_2 + \\cdots + \\frac{\\beta_p^s}{s_p} X_p,\\] Y vemos que tiene la misma forma funcional de \\(f_\\beta(X)\\). Si la solución de mínimos cuadrados es única, entonces una vez que ajustemos tenemos que tener \\(\\hat{f}_\\beta(X) = \\hat{g}_{\\beta^s} (X)\\), lo que implica que \\[\\hat{\\beta}_0 = \\hat{\\beta}_0^s -  \\sum_{j=1}^p \\hat{\\beta}_j^s\\mu_j/s_j\\] y \\[\\hat{\\beta}_j = \\hat{\\beta}_j^s/s_j.\\]\nNótese que para pasar del problema estandarizado al no estandarizado simplemente se requiere escalar los coeficientes por la \\(s_j\\) correspondiente.\n\nEjemplo\nRepetimos nuestro modelo, pero normalizando las entradas:\n\n# usamos recipes para este ejemplo, no necesitas usarlo\ncasas_receta &lt;- recipe(precio_m2_miles ~ ., casas_entrena) |&gt;\n  step_normalize(all_predictors()) \ncasas_receta |&gt; summary()\n\n# A tibble: 4 × 4\n  variable        type      role      source  \n  &lt;chr&gt;           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 area_hab_m2     &lt;chr [2]&gt; predictor original\n2 calidad_gral    &lt;chr [2]&gt; predictor original\n3 num_coches      &lt;chr [2]&gt; predictor original\n4 precio_m2_miles &lt;chr [2]&gt; outcome   original\n\n\n\nmodelo_lineal &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\ncasas_flujo &lt;- workflow() |&gt;\n  add_recipe(casas_receta) |&gt; \n  add_model(modelo_lineal)\n\n\nlibrary(keras)\n# definición de estructura del modelo (regresión lineal)\nx_ent_s &lt;-  prep(casas_receta) |&gt; juice() |&gt; select(-precio_m2_miles) |&gt; \n  as.matrix()\najustar_casas &lt;- function(modelo, x, y, n_epochs = 100){\n  ajuste &lt;- modelo |&gt; fit(\n    as.matrix(x), y,\n    batch_size = nrow(x_ent), # para descenso en gradiente\n    epochs = n_epochs, # número de iteraciones\n    verbose = 0) |&gt; as_tibble()\n  ajuste\n}\nmodelo_casas_ns &lt;- crear_modelo(0.00001)\nmodelo_casas_s &lt;- crear_modelo(0.2)\nhistoria_s &lt;- ajustar_casas(modelo_casas_s, x_ent_s, y_ent) |&gt;\n  mutate(tipo = \"Estandarizar\")\nhistoria_ns &lt;- ajustar_casas(modelo_casas_ns, x_ent, y_ent) |&gt; \n  mutate(tipo = \"Sin estandarizar\")\nhistoria &lt;- bind_rows(historia_ns, historia_s) |&gt;  filter(metric == \"mean_squared_error\")\nggplot(historia, aes(x = epoch, y = value, colour = tipo)) +\n     geom_line() + geom_point() +scale_x_log10() + scale_y_log10()\n\n\n\n\nObservamos que el modelo con datos estandarizados convergió:\n\nkeras::get_weights(modelo_casas_s)\n\n[[1]]\n            [,1]\n[1,] -0.22045916\n[2,]  0.23149671\n[3,]  0.09673478\n\n[[2]]\n[1] 1.295027\n\ncoef(lm.fit(cbind(1,x_ent_s), y_ent))\n\n              area_hab_m2 calidad_gral   num_coches \n  1.29502679  -0.22045919   0.23149675   0.09673476 \n\n\nMientras que el modelo no estandarizado todavía requiere iteraciones:\n\nkeras::get_weights(modelo_casas_ns)\n\n[[1]]\n             [,1]\n[1,] 0.0079817399\n[2,] 0.0019486138\n[3,] 0.0005532746\n\n[[2]]\n[1] 0.0003491883\n\ncoef(lm.fit(cbind(1, x_ent), y_ent))\n\n              area_hab_m2 calidad_gral   num_coches \n  0.66869194  -0.00449751   0.16807663   0.13115749"
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html#algoritmo-de-descenso-estocástico",
    "href": "82-apendice-descenso-estocastico.html#algoritmo-de-descenso-estocástico",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "B.1 Algoritmo de descenso estocástico",
    "text": "B.1 Algoritmo de descenso estocástico\n\n\n\n\n\n\nDescenso estocástico\n\n\n\nSeparamos al azar los datos de entrenamiento en \\(n\\) minilotes de tamaño \\(m\\).\n\nPara épocas \\(e =1,2,\\ldots, n_e\\)\n\nCalcular el gradiente sobre el minilote y hacer actualización, sucesivamente para cada uno de los minilotes \\(k=1,2,\\ldots, n/m\\): \\[\\beta_{i+1} = \\beta_{i} - \\eta\\frac{1}{m}\\sum_{j=1}^m \\nabla D^{(k)}_j (\\beta_i)\\] donde \\(D^{(k)}_j (\\beta_i)\\) es la devianza para el \\(j\\)-ésimo caso del minilote \\(k\\).\n\nRepetir para la siguiente época (opcional: reordenar antes al azar los minilotes, para evitar ciclos)."
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html#por-qué-usar-descenso-estocástico-por-minilotes",
    "href": "82-apendice-descenso-estocastico.html#por-qué-usar-descenso-estocástico-por-minilotes",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "B.2 ¿Por qué usar descenso estocástico por minilotes?",
    "text": "B.2 ¿Por qué usar descenso estocástico por minilotes?\nLas propiedades importantes de descenso estocástico son:\n\nMuchas veces no es necesario usar todos los datos para encontrar una buena dirección de descenso. Podemos ver la dirección de descenso en gradiente como un valor esperado sobre la muestra de entrenamiento (pues la pérdida es un promedio sobre el conjunto de entrenamiento). Una submuestra (minilote) puede ser suficiente para estimar ese valor esperado, con costo menor de cómputo. Adicionalmente, quizá no es tan buena idea intentar estimar el gradiente con la mejor precisión pues es solamente una dirección de descenso local (así que quizá no da la mejor decisión de a dónde moverse en cada punto). Es mejor hacer iteraciones más rápidas con direcciones estimadas.\nDesde este punto de vista, calcular el gradiente completo para descenso en gradiente es computacionalmente ineficiente. Si el conjunto de entrenamiento es masivo, descenso en gradiente puede no ser factible.\nDesde el punto de vista de sobreajuste, el uso de distintos datos para cada paso evita mínimos sobreajustados o de bajo desempeño que están presentes en la pérdida calculada con todos los datos.\n¿Cuál es el mejor tamaño de minilote? Por un lado, minilotes más grandes nos dan mejores eficiencias en paralelización (multiplicación de matrices), especialmente en GPUs. Por otro lado, con minilotes más grandes puede ser que hagamos trabajo de más, por las razones expuestas en los incisos anteriores, y tengamos menos iteraciones en el mismo tiempo. El mejor punto está entre minilotes demasiado chicos (donde no aprovechamos paralelismo) o demasiado grande (donde hacemos demasiado trabajo por iteración).\n\n4.Una propiedad importante de descenso estocástico en minilotes es que su convergencia no depende del tamaño del conjunto de entrenamiento, es decir, el tiempo de iteración para descenso estocástico no crece con el número de casos totales. Podemos tener obtener buenos ajustes incluso con tamaños muy grandes de conjuntos de entrenamiento (por ejemplo, antes de procesar todos los datos de entrenamiento). Descenso estocástico escala bien en este sentido: el factor limitante es el tamaño de minilote y el número de iteraciones.\n\nEs importante permutar al azar los datos antes de hacer los minibatches, pues órdenes “naturales” en los datos pueden afectar la convergencia. Se ha observado también que permutar los minibatches en cada iteración típicamente acelera la convergencia (si se pueden tener los datos en memoria).\n\n\nEjemplo\nEn el ejemplo anterior nota que las direcciones de descenso de descenso estocástico son muy razonables (punto 1). Nota también que obtenemos una buena aproximación a la solución con menos cómputo (punto 2 - mismo número de iteraciones, pero cada iteración con un minilote).\n\nggplot(filter(dat_dev, iteracion &gt;= 1), \n       aes(x=iteracion, y=dev_ent, colour=algoritmo)) + geom_line() +\n  facet_wrap(~tipo, ncol=1)"
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html#escogiendo-la-tasa-de-aprendizaje",
    "href": "82-apendice-descenso-estocastico.html#escogiendo-la-tasa-de-aprendizaje",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "B.3 Escogiendo la tasa de aprendizaje",
    "text": "B.3 Escogiendo la tasa de aprendizaje\nPara escoger la tasa, monitoreamos las curvas de error de entrenamiento y de validación. Si la tasa es muy grande, habrá oscilaciones grandes y muchas veces incrementos grandes en la función objectivo (error de entrenamiento). Algunas oscilaciones suaves no tienen problema -es la naturaleza estocástica del algoritmo. Si la tasa es muy baja, el aprendizaje es lento y podemos quedarnos en un valor demasiado alto.\nConviene monitorear las primeras iteraciones y escoger una tasa más alta que la mejor que tengamos acutalmente, pero no tan alta que cause inestabilidad. Una gráfica como la siguiente es útil. En este ejemplo, incluso podríamos detenernos antes para evitar el sobreajuste de la última parte de las iteraciones:\n\nggplot(filter(dat_dev, algoritmo=='descenso_estocastico'), \n       aes(x=iteracion, y=dev_ent, colour=tipo)) + geom_line() \n\n\n\n\nPor ejemplo: tasa demasiado alta:\n\niter_estocastico &lt;- descenso_estocastico(20, z_0, 0.5, minilotes) |&gt;\n  as_tibble() \ndev_ent &lt;- perdida_calc(x = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_ent$y)\ndev_valid &lt;- perdida_calc(x = as.matrix(dat_valid[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_valid$y)\ndat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) |&gt;\n   mutate(entrena = apply(iter_estocastico, 1, dev_ent), \n  validacion = apply(iter_estocastico, 1, dev_valid)) |&gt;\n  gather(tipo, devianza, entrena:validacion)\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\nggplot(dat_dev, \n       aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() \n\n\n\n\nTasa demasiado chica ( o hacer más iteraciones):\n\niter_estocastico &lt;- descenso_estocastico(20, z_0, 0.001, minilotes) |&gt;\n  as_tibble() \ndev_ent &lt;- perdida_calc(x = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_ent$y)\ndev_valid &lt;- perdida_calc(x = as.matrix(dat_valid[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_valid$y)\ndat_dev &lt;- tibble(iteracion = 1:nrow(iter_estocastico)) |&gt;\n   mutate(entrena = apply(iter_estocastico, 1, dev_ent), \n  validacion = apply(iter_estocastico, 1, dev_valid)) |&gt;\n  gather(tipo, devianza, entrena:validacion)\nggplot(dat_dev, \n       aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() \n\n\n\n\n\nPara redes neuronales, es importante explorar distintas tasas de aprendizaje, aún cuando no parezca haber oscilaciones grandes o convergencia muy lenta. En algunos casos, si la tasa es demasiado grande, puede ser que el algoritmo llegue a lugares con gradientes cercanos a cero (por ejemplo, por activaciones demasiado grandes) y tenga dificultad para moverse."
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html#mejoras-al-algoritmo-de-descenso-estocástico.",
    "href": "82-apendice-descenso-estocastico.html#mejoras-al-algoritmo-de-descenso-estocástico.",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "B.4 Mejoras al algoritmo de descenso estocástico.",
    "text": "B.4 Mejoras al algoritmo de descenso estocástico.\n\nB.4.1 Decaimiento de tasa de aprendizaje\nHay muchos algoritmos derivados de descenso estocástico. La primera mejora consiste en reducir gradualmente la tasa de aprendizaje para aprender rápido al principio, pero filtrar el ruido de la estimación de minilotes más adelante en las iteraciones y permitir que el algoritmo se asiente en un mínimo.\n\ndescenso_estocastico &lt;- function(n_epocas, z_0, eta, minilotes, decaimiento = 0.0){\n  #minilotes es una lista\n  m &lt;- length(minilotes)\n  z &lt;- matrix(0, m*n_epocas, length(z_0))\n  z[1, ] &lt;- z_0\n  for(i in 1:(m*n_epocas-1)){\n    k &lt;- i %% m + 1\n    if(i %% m == 0){\n      #comenzar nueva época y reordenar minilotes al azar\n      minilotes &lt;- minilotes[sample(1:m, m)]\n    }\n    h_deriv &lt;- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y)\n    z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ])\n    eta &lt;- eta*(1/(1+decaimiento*i))\n  }\n  colnames(z) &lt;- names(z_0)\n  z\n}\n\nY ahora vemos qué pasa con decaimiento:\n\niter_estocastico &lt;- descenso_estocastico(10, z_0, 0.1, \n                                         minilotes, decaimiento = 1e-3) |&gt;\n  as_tibble() |&gt; rename(beta_0 = Intercept, beta_1 = x_1, beta_2 = x_2, beta_3 = x_3)\ndev_ent &lt;- perdida_calc(x = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_ent$y)\ndev_valid &lt;- perdida_calc(x = as.matrix(dat_valid[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_valid$y)\ndat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) |&gt;\n   mutate(entrena = apply(iter_estocastico, 1, dev_ent), \n  validacion = apply(iter_estocastico, 1, dev_valid)) |&gt;\n  gather(tipo, devianza, entrena:validacion)\nggplot(filter(dat_dev, iteracion&gt;1), \n       aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() \n\n\n\n\nPara los primeros dos parámetros, las iteraciones se ven:\n\nggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() +\n  geom_point() +\n  geom_path(data = iter_estocastico, colour ='red', alpha=0.2) +\n  geom_point(data = iter_estocastico, colour ='red', alpha=0.2)\n\n\n\n\n\n\n\n\n\n\nTasa de aprendizaje\n\n\n\nLa tasa de aprendizaje es uno de los parámetros en redes neuronales más importantes de afinar. Generalmente se empieza con una tasa de aprendizaje con un valor bajo (0.01, o 0.1), pero es necesario experimentar.\n\nUn valor muy alto puede provocar oscilaciones muy fuertes en la pérdida\nUn valor alto también puede provocar que el algoritmo se detenga en lugar con función pérdida alta (sobreajusta rápidamente).\nUn valor demasiado bajo produce convergencia lenta.\n\n\n\n\n\nB.4.2 Momento\nTambién es posible utilizar una idea adicional que acelera la convergencia. La idea es que muchas veces la aleatoriedad del algoritmo puede producir iteraciones en direcciones que no son tan buenas (pues la estimación del gradiente es mala). Esto es parte del algoritmo. Sin embargo, si en varias iteraciones hemos observado movimientos en direcciones consistentes, quizá deberíamos movernos en esas direcciones consistentes, y reducir el peso de la dirección del minilote (que nos puede llevar en una dirección mala). El resultado es un suavizamiento de las curvas de aprendizaje.\nEsto es similar al movimiento de una canica en una superficie: la dirección de su movimiento está dada en parte por la dirección de descenso (el gradiente) y en parte la velocidad actual de la canica. La canica se mueve en un promedio de estas dos direcciones\n\n\n\n\n\n\nDescenso estocástico con momento\n\n\n\nSeparamos al azar los datos de entrenamiento en \\(n\\) minilotes de tamaño \\(m\\).\n\nPara épocas \\(e =1,2,\\ldots, n_e\\)\n\nCalcular el gradiente sobre el minilote y hacer actualización, sucesivamente para cada uno de los minilotes \\(k=1,2,\\ldots, n/m\\): \\[\\beta_{i+1} = \\beta_{i} + v,\\] \\[v= \\alpha v - \\eta\\frac{1}{m}\\sum_{j=1}^m \\nabla D^{(k)}_j\\] donde \\(D^{(k)}_j (\\beta_i)\\) es la devianza para el \\(j\\)-ésimo caso del minilote \\(k\\). A \\(v\\) se llama la velocidad\n\nRepetir para la siguiente época\n\n\n\n\ndescenso_estocastico &lt;- function(n_epocas, z_0, eta, minilotes, \n                                 momento = 0.0, decaimiento = 0.0){\n  #minilotes es una lista\n  m &lt;- length(minilotes)\n  z &lt;- matrix(0, m*n_epocas, length(z_0))\n  z[1, ] &lt;- z_0\n  v &lt;- 0\n  for(i in 1:(m*n_epocas-1)){\n    k &lt;- i %% m + 1\n    if(i %% m == 0){\n      #comenzar nueva época y reordenar minilotes al azar\n      minilotes &lt;- minilotes[sample(1:m, m)]\n      v &lt;- 0\n    }\n    h_deriv &lt;- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y)\n    z[i+1, ] &lt;- z[i, ] + v\n    v &lt;- momento*v - eta * h_deriv(z[i, ])\n    eta &lt;- eta*(1/(1+decaimiento*i))\n  }\n  colnames(z) &lt;- names(z_0)\n  z\n}\n\nY ahora vemos que usando momento el algoritmo es más parecido a descenso en gradiente usual (pues tenemos cierta memoria de direcciones anteriores de descenso):\n\nset.seed(232)\niter_estocastico &lt;- descenso_estocastico(10, z_0, 0.005, minilotes, momento = 0.9, decaimiento = 0.00001) |&gt;\n  as_tibble() |&gt; rename(beta_0 = Intercept, beta_1 = x_1, beta_2 = x_2, beta_3 = x_3)\ndev_ent &lt;- perdida_calc(x = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_ent$y)\ndev_valid &lt;- perdida_calc(x = as.matrix(dat_valid[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_valid$y)\ndat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) |&gt;\n   mutate(entrena = apply(iter_estocastico, 1, dev_ent), \n  validacion = apply(iter_estocastico, 1, dev_valid)) |&gt;\n  gather(tipo, devianza, entrena:validacion)\nggplot(filter(dat_dev, iteracion &gt; 1), \n       aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() + geom_point()\n\n\n\n\n\nggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() +\n  geom_point() +\n  geom_path(data = iter_estocastico, colour ='red', alpha=0.5) +\n  geom_point(data = iter_estocastico, colour ='red', alpha=0.5)\n\n\n\n\nNótese cómo llegamos más rápido a una buena solución (comparado con el ejemplo sin momento). Adicionalmente, error de entrenamiento y validación lucen más suaves, producto de promediar velocidades a lo largo de iteraciones.\nValores típicos para momento son 0,0.5,0.9 o 0.99.\n\n\nB.4.3 Otras variaciones\nOtras variaciones incluyen usar una tasa adaptativa de aprendizaje por cada parámetro (algoritmos adagrad, rmsprop, adam y adamax), o actualizaciones de momento un poco diferentes (Nesterov).\nLos más comunes son descenso estocástico, descenso estocástico con momento (a veces con la modificación de Nesterov), rmsprop y adam (Capítulo 8 del Deep Learning Book, (Goodfellow, Bengio, y Courville 2016))."
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html#ajuste-de-redes-con-descenso-estocástico",
    "href": "82-apendice-descenso-estocastico.html#ajuste-de-redes-con-descenso-estocástico",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "B.5 Ajuste de redes con descenso estocástico",
    "text": "B.5 Ajuste de redes con descenso estocástico\n\nlibrary(keras)\n\n\nset.seed(21321)\nx_ent &lt;- as.matrix(dat_ent[,c('x_1','x_2','x_3')])\nx_valid &lt;-  as.matrix(dat_valid[,c('x_1','x_2','x_3')])\ny_ent &lt;- dat_ent$y\ny_valid &lt;- dat_valid$y\n\nEmpezamos con regresión (sin capas ocultas), que se escribe y ajusta como sigue:\n\nmodelo &lt;- keras_model_sequential() \nmodelo |&gt;\n  layer_dense(units = 1, \n              activation = \"linear\",\n              input_shape = c(3))\n\nmodelo |&gt; compile(loss = 'mse',\n                   optimizer = optimizer_sgd(learning_rate = 0.1, momentum = 0,\n                                             decay = 0))\n\nhistory &lt;- modelo |&gt; \n  fit(x_ent, y_ent, \n      epochs = 50, batch_size = 10, \n      verbose = 0,\n      validation_data = list(x_valid, y_valid))\n\nPodemos ver el progreso del algoritmo por época\n\naprendizaje &lt;- as_tibble(history)\nggplot(aprendizaje, \n       aes(x=epoch, y=value, colour=data, group=data)) +\n  facet_wrap(~metric, ncol = 1) + geom_line() + geom_point(size = 0.5)\n\n\n\n\nVer los pesos:\n\nget_weights(modelo)\n\n[[1]]\n            [,1]\n[1,] -2.66598511\n[2,] -0.04755141\n[3,]  0.11968917\n\n[[2]]\n[1] -0.4760832\n\n\nY verificamos que concuerda con la salida de lm:\n\nmod_lineal &lt;- lm(y ~ x_1 + x_2+ x_3, data = dat_ent) \ncoef(mod_lineal)\n\n(Intercept)         x_1         x_2         x_3 \n-0.36904266 -2.46877687 -0.07368414  0.06632769 \n\n\n\n\n\n\nGoodfellow, Ian, Yoshua Bengio, y Aaron Courville. 2016. Deep Learning. MIT Press."
  },
  {
    "objectID": "99-referencias.html",
    "href": "99-referencias.html",
    "title": "Referencias",
    "section": "",
    "text": "Bishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning (Information Science and Statistics). Secaucus, NJ, USA:\nSpringer-Verlag New York, Inc.\n\n\nChambers, J. M., W. S. Cleveland, B. Kleiner, and P. A. Tukey. 1983.\nGraphical Methods for Data Analysis. Chapman & Hall\nStatistics Series. Wadsworth International Group. https://books.google.com.mx/books?id=I-tQAAAAMAAJ.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The\nElements of Statistical Learning. Springer Series in Statistics.\nSpringer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2014. An Introduction to Statistical Learning: With Applications in\nr. Springer Publishing Company, Incorporated. http://www-bcf.usc.edu/~gareth/ISL/.\n\n\nKuhn, M., and J. Silge. 2022. Tidy Modeling with r. O’Reilly\nMedia. https://books.google.com.mx/books?id=9cJ6EAAAQBAJ."
  }
]